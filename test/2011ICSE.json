{
	"DataRevision": 4,
	"Event": "ICSE2011",
	"Name": "ICSE 2011",
	"TwitterKeywords": "icse2011",
	"FlickrKeywords": "icse2011",
	"HawaiiGroup": "ICSEHawaii2011",
	"VenueInfo" : {
		"Name": "Hilton Hawaiian Village",
		"GPS": {"Latitude": 21.283726, "Longitude": -157.836714}
	},
	"HotelInfo": {
		"Telephone": "+118089494321"
	},
	"InfoPage": {
		"xaml": "foo",
		"Elements": [
			{
				"XamlName": "HotelMap",
				"Type": "Map",
				"GPS": {"Latitude" :21.283726, "Longitude": -157.836714},
				"MapLabel": "Hilton Hawaiian Village"
			},
			{
				"XamlName": "HotelCall",
				"Type": "Call",
				"LocationName": "Hilton Hawaiian Village",
				"TelephoneNumber": "+118089494321"
			},
			{
				"XamlName": "LicenseButton",
				"Type": "WebLink",
				"URL": "http://research.microsoft.com/en-US/projects/confapp/terms.aspx"
			},
			{
				"XamlName": "PrivacyButton",
				"Type": "WebLink",
				"URL": "http://privacy.microsoft.com/en-us/default.msp"
			}
		]
	},
	"SessionPriorities" : [
                    "Plenary", "Award", "Technical/Research Track", "Panel",
                    "Software Engineering in Practice Track", "New Ideas and Emerging Results Track", "Student Competition", 
                    "Demonstrations Track", "Posters", "Impact Project Focus Area", "Other"],
	"Items": [
{
  "Title": "1.x-Way Architecture-Implementation Mapping",
  "Type": "Paper",
  "Key": "1x-way-architecture-implementation-mapping",
  "Authors": ["Yongjie Zheng"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "A new architecture-implementation mapping approach, 1.x-way mapping, is presented to address architecture-implementation conformance. It targets maintaining conformance of structure and behavior, providing a solution to architecture changes, and protecting architecture-prescribed code from being manually changed. Technologies developed in this work include deep separation of generated and non-generated code, an architecture change model, architecture-based code regeneration, and architecture change notification."
},
{
  "Title": "aComment: Mining Annotations from Comments and Code to Detect Interrupt Related Concurrency Bugs",
  "Type": "Technical/Research Track",
  "Key": "acomment-mining-annotations-comments-and-code-detect-interrupt-related-concurrency-bugs",
  "Authors": ["Lin Tan", "Yuanyuan Zhou", "Yoann Padioleau"],
  "Affiliations": ["University of Waterloo, Canada", "UC San Diego, USA", "Facebook Inc., USA"],
  "Abstract": "Concurrency bugs in an operating system (OS) are detrimental as they can cause the OS to fail and affect all applications running on top of the OS. Detecting OS concurrency bugs is challenging due to the complexity of the OS synchronization, particularly with the presence of the OS speciﬁc interrupt context. Existing dynamic concurrency bug detection techniques are designed for user-level applications and cannot be applied to operating systems.\n\nTo detect OS concurrency bugs, we proposed a new type of annotations – interrupt related annotations – and generated 96,821 such annotations for the Linux kernel with little manual effort. These annotations have been used to automatically detect 9 real OS concurrency bugs (7 of which were previously unknown). Two of the key techniques that make the above contributions possible are: (1) using a hybrid approach to extract annotations from both code and comments written in natural language to achieve better coverage and accuracy in annotation extraction and bug detection; and (2) automatically propagating annotations to caller functions to improve annotating and bug detection. These two techniques are general and can be applied to non-OS code, code written in other programming languages such as Java, and for extracting other types of speciﬁcations."
},
{
  "Title": "Always-available Static and Dynamic Feedback",
  "Type": "Technical/Research Track",
  "Key": "always-available-static-and-dynamic-feedback",
  "Authors": ["Michael Bayne", "Richard Cook", "Michael D. Ernst"],
  "Affiliations": ["University of Washington, USA"],
  "Abstract": "Developers who write code in a statically typed language are denied the ability to obtain dynamic feedback by executing their code during periods when it fails to type-check. They are further confined to the static typing discipline during times in the development process where it does not yield the highest productivity. If they opt instead to use a dynamic language, they forgo the many benefits of static typing. We present a novel approach to giving developers the benefits of both static and dynamic typing, throughout the development process, and without the burden of manually separating their program into statically- and dynamically-typed parts.\n\nOur approach relaxes the static type system and provides a semantics for many type-incorrect programs. We implemented our approach in a publicly available tool, DuctileJ, for the Java language. In case studies, DuctileJ conferred benefits both during prototyping and during the evolution of existing code."
},
{
  "Title": "The American Law Institute’s Principles on Software Contracts and their Ramifications for Software Engineering Research",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "american-law-institutes-principles-software-contracts-and-their-ramifications-software-engi",
  "Authors": ["James Williams", "Jens H. Weber"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "The American Law Institute has recently published principles of software contracts that may have profound impact on changing the software industry. One of the principles implies a nondisclaimable liability of software vendors for any hidden material defects. In this paper, we describe the new principle, first from a legal and then from a software engineering point of view. We point out potential ramifications and research directions for the software engineering community."
},
{
  "Title": "Angelic Debugging",
  "Type": "Technical/Research Track",
  "Key": "angelic-debugging",
  "Authors": ["Satish Chandra", "Emina Torlak", "Shaon Barman", "Rastislav Bodik"],
  "Affiliations": ["IBM Research, USA", "UC Berkeley, USA"],
  "Abstract": "Software ships with known bugs because it is expensive to pinpoint and ﬁx the bug exposed by a failing test. To reduce the cost of bug identiﬁcation, we locate expressions that are likely causes of bugs and thus candidates for repair. Our symbolic method approximates an ideal approach to ﬁxing bugs mechanically, which is to search the space of all edits to the program for one that repairs the failing test without breaking any passing test. We approximate the expensive ideal of exploring syntactic edits by instead computing the set of values whose substitution for the expression corrects the execution. We observe that an expression is a repair candidate if it can be replaced with a value that ﬁxes a failing test and in each passing test, its value can be changed to another value without breaking the test. The latter condition makes the expression ﬂexible in that it permits multiple values. The key observation is that the repair of a ﬂexible expression is less likely to break a passing test. The method is called angelic debugging because the values are computed by angelically nondeterministic statements. We implemented the method on top of the Java PathFinder model checker. Our experiments with this technique show promise of its applicability in speeding up program debugging."
},
{
  "Title": "Architecture Evaluation without an Architecture: Experience with the Smart Grid",
  "Type": "Software Engineering in Practice Track",
  "Key": "architecture-evaluation-without-architecture-experience-smart-grid",
  "Authors": ["Rick Kazman", "Len Bass", "James Ivers", "Gabriel A. Moreno"],
  "Affiliations": ["University of Hawaii, USA", "SEI/CMU, USA"],
  "Abstract": "This paper describes an analysis of some of the challenges facing one portion of the Smart Grid in the United States—residential Demand Response (DR) systems. The purposes of this paper are twofold: 1) to discover risks to residential DR systems and 2) to illustrate an architecture-based analysis approach to uncovering risks that span a collection of technical and social concerns. The results presented here are specific to residential DR but the approach is general and it could be applied to other systems within the Smart Grid and other critical infrastructure domains. Our architecture-based analysis is different from most other approaches to analyzing complex systems in that it addresses multiple quality attributes simultaneously (e.g., performance, reliability, security, modifiability, usability, etc.) and it considers the architecture of a complex system from a socio-technical perspective where the actions of the people in the system are as important, from an analysis perspective, as the physical and computational elements of the system. This analysis can be done early in a system’s lifetime, before substantial resources have been committed to its construction or procurement, and so it provides extremely cost-effective risk analysis."
},
{
  "Title": "Aspect Recommendation for Evolving Software",
  "Type": "Technical/Research Track",
  "Key": "aspect-recommendation-evolving-software",
  "Authors": ["Tung T. Nguyen", "Hung V. Nguyen", "Hoan A. Nguyen", "Tien N. Nguyen"],
  "Affiliations": ["Iowa State University, USA"],
  "Abstract": "Cross-cutting concerns are unavoidable and create diﬃculties in the development and maintenance of large-scale systems. In this paper, we present a novel approach that identiﬁes certain groups of code units that potentially share some cross-cutting concerns and recommends them for creating and updating aspects. Those code units, called concern peers, are detected based on their similar interactions (similar calling relations in similar contexts, either internally or externally). The recommendation is applicable to both the aspectization of non-aspect-oriented programs (i.e. for aspect creation), and the evolution of aspect-oriented programs (i.e. for aspect updating). The empirical evaluation on several real-world software systems shows that our approach is scalable and provides useful recommendations."
},
{
  "Title": "Assessing Programming Language Impact on Development and Maintenance: A Study on C and C++",
  "Type": "Technical/Research Track",
  "Key": "assessing-programming-language-impact-development-and-maintenance-study-c-and-c",
  "Authors": ["Pamela Bhattacharya", "Iulian Neamtiu"],
  "Affiliations": ["UC Riverside, USA"],
  "Abstract": "Billions of dollars are spent every year for building and maintaining software. To reduce these costs we must identify the key factors that lead to better software and more productive development. One such key factor, and the focus of our paper, is the choice of programming language. Existing studies that analyze the impact of choice of programming language suffer from several deficiencies with respect to methodology and the applications they consider. For example, they consider applications built by different teams in different languages, hence fail to control for developer competence, or they consider small-sized, infrequently-used, short-lived projects. We propose a novel methodology which controls for development process and developer competence, and quantifies how the choice of programming language impacts software quality and developer productivity. We conduct a study and statistical analysis on a set of long-lived, widely-used, open source projects---Firefox, Blender, VLC, and MySQL. The key novelties of our study are: (1) we only consider projects which have considerable portions of development in two languages, C and C++, and (2) a majority of developers in these projects contribute to both C and C++ code bases. We found that using C++ instead of C results in improved software quality and reduced maintenance effort, and that code bases are shifting from C to C++. Our methodology lays a solid foundation for future studies on comparative advantages of particular programming languages"
},
{
  "Title": "AutoBlackTest: A Tool for Automatic Black-Box Testing",
  "Type": "Demonstrations Track",
  "Key": "autoblacktest-tool-automatic-black-box-testing",
  "Authors": ["Leonardo Mariani", "Mauro Pezzè", "Oliviero Riganelli", "Mauro Santoro"],
  "Affiliations": ["University of Milano Bicocca, Italy", "University of Lugano, Switzerland"],
  "Abstract": "In this paper we present AutoBlackTest, a tool for the automatic generation of test cases for interactive applications. AutoBlackTest interacts with the application though its GUI, and uses reinforcement learning techniques to understand the interaction modalities and to generate relevant testing scenarios. Early results show that the tool has the potential of automatically discovering bugs and generating useful system and regression test suites."
},
{
  "Title": "Automated Cross-Browser Compatibility Testing",
  "Type": "Technical/Research Track",
  "Key": "automated-cross-browser-compatibility-testing",
  "Authors": ["Ali Mesbah", "Mukul R. Prasad"],
  "Affiliations": ["University of British Columbia, Canada", "Fujitsu Laboratories of America, USA"],
  "Abstract": "With the advent of Web 2.0 applications and new browsers, the cross-browser compatibility issue is becoming increasingly important. Although the problem is widely recognized among web developers, no systematic approach to tackle it exists today. None of the current tools, which provide screenshots or emulation environments, speciﬁes any notion of cross-browser compatibility, much less check it automatically. In this paper, we pose the problem of cross-browser compatibility testing of modern web applications as a ‘functional consistency’ check of web application behavior across diﬀerent web browsers and present an automated solution for it. Our approach consists of (1) automatically analyzing the given web application under diﬀerent browser environments and capturing the behavior as a ﬁnite-state machine; (2) formally comparing the generated models for equivalence on a pairwise-basis and exposing any observed discrepancies. We validate our approach on several open-source and industrial case studies to demonstrate its eﬀectiveness and real-world relevance."
},
{
  "Title": "Automated Security Hardening for Evolving UML Models",
  "Type": "Demonstrations Track",
  "Key": "automated-security-hardening-evolving-uml-models",
  "Authors": ["Jan Jürjens"],
  "Affiliations": ["TU Dortmund and Fraunhofer ISST, Germany"],
  "Abstract": "Developing security-critical software correctly and securely is diﬃcult. To address this problem, there has been a signiﬁcant amount of work over the last 10 years on providing model-based development approaches based on the Uniﬁed Modeling Language which aim to raise the trustworthiness of security-critical systems, some of them including tools allowing the user to check whether a UML model satisﬁes the relevant security requirements. However, when the requirements are not satisﬁed by a given model, it can be challenging for the user to determine which changes to do to the model so that it will indeed satisfy the security requirements. Also, the fact that software continues to evolve on an ongoing basis, even after the implementation has been shipped to the customer, increases the challenge since in principle, the software has to be re-veriﬁed after each modiﬁcation, requiring signiﬁcant eﬀorts. We present work on automated tool-support that exploits recent work on secure software evolution in the Secure Change project in order to support the security hardening of evolving UML models (within the context of the UML security extension UMLsec)."
},
{
  "Title": "Automated Usability Evaluation of Parallel Programming Constructs",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "automated-usability-evaluation-parallel-programming-constructs",
  "Authors": ["Victor Pankratius"],
  "Affiliations": ["Karlsruhe Institute of Technology, Germany"],
  "Abstract": "Multicore computers are ubiquitous, and proposals to extend existing languages with parallel constructs mushroom. While everyone claims to make parallel programming easier and less error-prone, empirical language usability evaluations are rarely done in-the-ﬁeld with many users and real programs. Key obstacles are costs and a lack of appropriate environments to gather enough data for representative conclusions. This paper discusses the idea of automating the usability evaluation of parallel language constructs by gathering subjective and objective data directly in every software engineer’s IDE. The paper presents an Eclipse prototype suite that can aggregate such data from potentially hundreds of thousands of programmers. Mismatch detection in subjective and objective feedback as well as construct usage mining can improve language design at an early stage, thus reducing the risk of developing and maintaining inappropriate constructs. New research directions arising from this idea are outlined for software repository mining, debugging, and software economics."
},
{
  "Title": "Automatically Detecting and Describing High Level Actions within Methods",
  "Type": "Technical/Research Track",
  "Key": "automatically-detecting-and-describing-high-level-actions-within-methods",
  "Authors": ["Giriprasad Sridhara", "Lori Pollock", "K. Vijay-Shanker"],
  "Affiliations": ["University of Delaware, USA"],
  "Abstract": "One approach to easing program comprehension is to reduce the amount of code that a developer has to read. Describing the high level abstract algorithmic actions associated with code fragments using succint natural language phrases enables a newcomer to focus on fewer and more abstract concepts when trying to understand a given method. Unfortunately, such descriptions are typically missing because it is tedious to create manually.\n\nWe present an automatic technique for identifying code fragments that implement high level abstractions of actions and expressing these abstractions in a concise yet precise manner. Our studies of 1000 Java programs indicate that our heuristics for identifying code fragments implementing high level actions are widely applicable. Judgements of our generated descriptions by 15 experienced Java programmers strongly suggest that indeed they view the fragments that we identify as representing high level actions and our synthesized descriptions accurately express the abstraction."
},
{
  "Title": "Better Testing Through Oracle Selection",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "better-testing-through-oracle-selection",
  "Authors": ["Matt Staats", "Michael W. Whalen", "Mats P.E. Heimdahl"],
  "Affiliations": ["University of Minnesota, USA"],
  "Abstract": "In software testing, the test oracle determines if the application under test has performed an execution correctly. In current testing practice and research, signiﬁcant effort and thought is placed on selecting test inputs, with the selection of test oracles largely neglected. Here, we argue that improvements to the testing process can be made by considering the problem of oracle selection. In particular, we argue that selecting the test oracle and test inputs together to complement one another may yield improvements testing effectiveness. We illustrate this using an example and present selected results from an ongoing study demonstrating the relationship between test suite selection, oracle selection, and fault ﬁnding."
},
{
  "Title": "Blending Freeform and Managed Information in Tables",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "blending-freeform-and-managed-information-tables",
  "Authors": ["Nicolas Mangano", "Harold Ossher", "Ian Simmonds", "Matthew Callery", "Michael Desmond", "Sophia Krasikov"],
  "Affiliations": ["UC Irvine, USA", "IBM Research, USA"],
  "Abstract": "Tables are an important tool used by business analysts engaged in early requirements activities (in fact it is safe to say that tables appeal to many other types of user, in a variety of activities and domains). Business analysts typically use the tables provided by office tools. These tables offer great flexibility, but no underlying model, and hence no consistency management, multiple views or other advantages familiar to the users of modeling tools. Modeling tools, however, are usually too rigid for business analysts. In this paper we present a flexible modeling approach to tables, which combines the advantages of both office and modeling tools. Freeform information can co-exist with information managed by an underlying model, and an incremental formalization approach allows each item of information to transition fluidly between freeform and managed. As the model evolves, it is used to guide the user in the process of formalizing any remaining freeform information. The model therefore helps users without restricting them. Early feedback is described, and the approach is analyzed briefly in terms of cognitive dimensions."
},
{
  "Title": "BQL: Capturing and Reusing Debugging Knowledge",
  "Type": "Demonstrations Track",
  "Key": "bql-capturing-and-reusing-debugging-knowledge",
  "Authors": ["Zhongxian Gu", "Earl T. Barr", "Zhendong Su"],
  "Affiliations": ["UC Davis, USA"],
  "Abstract": "When fixing a bug, a programmer tends to search for similar bugs that have been resolved in the past. A fix for a similar bug may help him fix his bug or at least understand his bug. We designed and implemented the Bug Query Language (BQL) and its accompanying tools to help users search for similar bugs to aid debugging. This paper demonstrates the main features of the BQL infrastructure. We populated BQL with bugs collected from open-source projects and show that BQL could have helped users to fix real-world bugs."
},
{
  "Title": "Bringing Domain-Specific Languages to Digital Forensics",
  "Type": "Software Engineering in Practice Track",
  "Key": "bringing-domain-specific-languages-digital-forensics",
  "Authors": ["Jeroen van den Bos", "Tijs van der Storm"],
  "Affiliations": ["Netherlands Forensic Institute, Netherlands", "Centrum Wiskunde en Informatica, Netherlands"],
  "Abstract": "Digital forensics investigations often consist of analyzing large quantities of data. The software tools used for analyzing such data are constantly evolved to cope with a multiplicity of versions and variants of data formats. This process of customization is time consuming and error prone.\n\nTo improve this situation we present DERRIC, a domain-specific language (DSL) for declaratively specifying data structures. This way, the specification of structure is separated from data processing. The resulting architecture encourages customization and facilitates reuse. It enables faster development through a division of labour between investigators and software engineers.\n\nWe have performed an initial evaluation of DERRIC by constructing a data recovery tool. This so-called carver has been automatically derived from a declarative description of the structure of JPEG files. We compare it to existing carvers, and show it to be in the same league both with respect to recovered evidence, and runtime performance."
},
{
  "Title": "Build System Maintenance",
  "Type": "Paper",
  "Key": "build-system-maintenance",
  "Authors": ["Shane McIntosh"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "The build system, i.e., the infrastructure that converts source code into deliverables, plays a critical role in the development of a software project. For example, developers rely upon the build system to test and run their source code changes. Without a working build system, development progress grinds to a halt, as the source code is rendered useless. Based on experiences reported by developers, we conjecture that build maintenance for large software systems is considerable, yet this maintenance is not well understood. A firm understanding of build maintenance is essential for project managers to allocate personnel and resources to build maintenance tasks effectively, and reduce the build maintenance overhead on regular development tasks, such as fixing defects and adding new features. In our work, we empirically study build maintenance in one proprietary and nine open source projects of different sizes and domain. Our case studies thus far show that: (1) similar to Lehman's first law of software evolution, build system specifications tend to grow unless effort is invested into restructuring them, (2) the build system accounts for up to 31% of the code files in a project, and (3) up to 27% of development tasks that change the source code also require build maintenance. Currently, we are working on identifying concrete measures that projects can take to reduce the build maintenance overhead."
},
{
  "Title": "Building and Using Pluggable Type-Checkers",
  "Type": "Software Engineering in Practice Track",
  "Key": "building-and-using-pluggable-type-checkers",
  "Authors": ["Werner Dietl", "Stephanie Dietzel", "Michael D. Ernst", "Kıvanç Muşlu", "Todd W. Schiller"],
  "Affiliations": ["University of Washington, USA"],
  "Abstract": "This paper describes practical experience building and using pluggable type-checkers. A pluggable type-checker reﬁnes (strengthens) the built-in type system of a programming language. This permits programmers to detect and prevent, at compile time, defects that would otherwise have been manifested as run-time errors. The prevented defects may be generally applicable to all programs, such as null pointer dereferences. Or, an application-speciﬁc pluggable type system may be designed for a single application. We built a series of pluggable type checkers using the Checker Framework, and evaluated them on 2 million lines of code, ﬁnding hundreds of bugs in the process. We also observed 28 ﬁrst-year computer science students use a checker to eliminate null pointer errors in their course projects. Along with describing the checkers and characterizing the bugs we found, we report the insights we had throughout the process. Overall, we found that the type checkers were easy to write, easy for novices to productively use, and effective in ﬁnding real bugs and verifying program properties, even for widely tested and used open source projects."
},
{
  "Title": "Building Domain Specific Software Architectures from Software Architectural Design Patterns",
  "Type": "Paper",
  "Key": "building-domain-specific-software-architectures-software-architectural-design-patterns",
  "Authors": ["Julie S. Fant"],
  "Affiliations": ["George Mason University, USA"],
  "Abstract": "Software design patterns are best practice solutions to common software problems. However, applying design patterns in practice can be difficult since design pattern descriptions are general and can be applied at multiple levels of abstraction. In order to address the aforementioned issue, this research focuses on creating a systematic approach to designing domain specific distributed, real-time and embedded (DRE) software from software architectural design patterns. To address variability across a DRE domain, software product line concepts are used to categorize and organize the features and design patterns. The software architectures produced are also validated through design time simulation. This research is applied and validated using the space flight software (FSW) domain."
},
{
  "Title": "Camouflage: Automated Anonymization of Field Data",
  "Type": "Technical/Research Track",
  "Key": "camouflage-automated-anonymization-field-data",
  "Authors": ["James Clause", "Alessandro Orso"],
  "Affiliations": ["University of Delaware, USA", "Georgia Institute of Technology, USA"],
  "Abstract": "Privacy and security concerns have adversely affected the usefulness of many types of techniques that leverage information gathered from deployed applications. To address this issue, we present an approach for automatically anonymizing failure-inducing inputs that builds on a previously developed technique. Given an input I that causes a failure f, our approach generates an anonymized input I' that is different from I but still causes f. I' can thus be sent to developers to enable them to debug f without having to know I. We implemented our approach in a prototype tool, camouflage, and performed an extensive empirical evaluation where we applied camouflage to a large set of failure-inducing inputs for several real applications. The results of the evaluation are promising, as they show that camouflage is both practical and effective at generating anonymized inputs; for the inputs that we considered, I and I' shared no sensitive information. The results also show that our approach can outperform the general technique it extends."
},
{
  "Title": "Capturing Tacit Architectural Knowledge Using the Repertory Grid Technique",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "capturing-tacit-architectural-knowledge-using-repertory-grid-technique",
  "Authors": ["Dan Tofan", "Matthias Galster", "Paris Avgeriou"],
  "Affiliations": ["University of Groningen, Netherlands"],
  "Abstract": "Background: Knowledge about the architecture of a software-intensive system tends to vaporize easily. This leads to increased maintenance costs. Objective: We explore a new idea: utilizing the repertory grid technique to capture tacit architectural knowledge. Particularly, we investigate the elicitation of design decision alternatives and their characteristics. Method: To study the applicability of this idea, we performed an exploratory study. Seven independent subjects applied the repertory grid technique to document a design decision they had to take in previous projects. Then, we interviewed each subject to understand their perception about the technique. Results: We identified advantages and disadvantages of using the technique. The main advantage is the reasoning support it provides; the main disadvantage is the additional effort it requires. Also, applying the technique depends on the context of the project. Conclusion: Using the repertory grid technique is a promising approach for fighting architectural knowledge vaporization."
},
{
  "Title": "A Case Study of Measuring Process Risk for Early Insights into Software Safety",
  "Type": "Software Engineering in Practice Track",
  "Key": "case-study-measuring-process-risk-early-insights-software-safety",
  "Authors": ["Lucas Layman", "Victor R. Basili", "Marvin V. Zelkowitz", "Karen L. Fisher"],
  "Affiliations": ["Fraunhofer CESE, USA", "University of Maryland, USA", "NASA Goddard Spaceflight Center, USA"],
  "Abstract": "In this case study, we examine software safety risk in three flight hardware systems in NASA’s Constellation spaceflight program. We applied our Technical and Process Risk Measurement (TPRM) methodology to the Constellation hazard analysis process to quantify the technical and process risks involving software safety in the early design phase of these projects. We analyzed 154 hazard reports and collected metrics to measure the prevalence of software in hazards and the specificity of descriptions of software causes of hazardous conditions. We found that 49-70% of 154 hazardous conditions could be caused by software or software was involved in the prevention of the hazardous condition. We also found that 12-17% of the 2013 hazard causes involved software, and that 23-29% of all causes had a software control. The application of the TRPM methodology identified process risks in the application of the hazard analysis process itself that may lead to software safety risk."
},
{
  "Title": "A Case Study on Refactoring in Haskell Programs",
  "Type": "Paper",
  "Key": "case-study-refactoring-haskell-programs",
  "Authors": ["Da Young Lee"],
  "Affiliations": ["North Carolina State University, USA"],
  "Abstract": "Programmers use refactoring to improve the design of existing code without changing external behavior. Current research does not empirically answer the question, “Why and how do programmers refactor functional programs?” In order to answer the question, I conducted a case study on three open source projects in Haskell. I investigated changed portions of code in 55 successive versions of a given project to classify how programmers refactor. I found a total of 143 refactorings classiﬁed by 12 refactoring types. I also found 5 new refactoring types and propose two new refactoring tools that would be useful for developers."
},
{
  "Title": "Characterizing the Differences Between Pre- and Post- Release Versions of Software",
  "Type": "Software Engineering in Practice Track",
  "Key": "characterizing-differences-between-pre-and-post-release-versions-software",
  "Authors": ["Paul Luo Li", "Ryan Kivett", "Zhiyuan Zhan", "Sung-eok Jeon", "Nachiappan Nagappan", "Brendan Murphy", "Andrew J. Ko"],
  "Affiliations": ["Microsoft Inc., USA", "Microsoft Research, USA", "Microsoft Research, UK", "University of Washington, USA"],
  "Abstract": "Many software producers utilize beta programs to predict postrelease quality and to ensure that their products meet quality expectations of users. Prior work indicates that software producers need to adjust predictions to account for usage environments and usage scenarios differences between beta populations and postrelease populations. However, little is known about how usage characteristics relate to field quality and how usage characteristics differ between beta and post-release. In this study, we examine application crash, application hang, system crash, and usage information from millions of Windows® users to 1) examine the effects of usage characteristics differences on field quality (e.g. which usage characteristics impact quality), 2) examine usage characteristics differences between beta and post-release (e.g. do impactful usage characteristics differ), and 3) report experiences adjusting field quality predictions for Windows. Among the 18 usage characteristics that we examined, the five most important were: the number of application executed, whether the machines was pre-installed by the original equipment manufacturer, two sub-populations (two language/geographic locales), and whether Windows was 64-bit (not 32-bit). We found each of these usage characteristics to differ between beta and post-release, and by adjusting for the differences, accuracy of field quality predictions for Windows improved by ~59%."
},
{
  "Title": "Characterizing Process Variation",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "characterizing-process-variation",
  "Authors": ["Borislava I. Simidchieva", "Leon J. Osterweil"],
  "Affiliations": ["University of Massachusetts Amherst, USA"],
  "Abstract": "A process model, namely a formal definition of the coordination of agents performing activities using resources and artifacts, can aid understanding of the real-world process it models. Moreover, analysis of the model can suggest improvements to the real-world process. Complex real-world processes, however, exhibit considerable amounts of variation that can be difficult or impossible to represent with a single process model. Such processes can often be modeled better, within the restrictions of a given modeling notation, by a family of models. This paper presents an approach to the formal characterization of some of these process families. A variety of needs for process variation are identified, and suggestions are made about how to meet some of these needs using different approaches. Some mappings of different needs for variability to approaches for meeting them are presented as case studies."
},
{
  "Title": "Coalescing Executions for Fast Uncertainty Analysis",
  "Type": "Technical/Research Track",
  "Key": "coalescing-executions-fast-uncertainty-analysis",
  "Authors": ["William Sumner", "Tao Bao", "Xiangyu Zhang", "Sunil Prabhakar"],
  "Affiliations": ["Purdue University, USA"],
  "Abstract": "Uncertain data processing is critical in a wide range of applications such as scientiﬁc computation handling data with inevitable errors and ﬁnancial decision making relying on human provided parameters. While increasingly studied in the area of databases, uncertain data processing is often carried out by software, and thus software based solutions are attractive. In particular, Monte Carlo (MC) methods execute software with many samples from the uncertain inputs and observe the statistical behavior of the output. In this paper, we propose a technique to improve the cost-eﬀectiveness of MC methods. Assuming only part of the input is uncertain, the certain part of the input always leads to the same execution across multiple sample runs. We remove such redundancy by coalescing multiple sample runs in a single run. In the coalesced run, the program operates on a vector of values if uncertainty is present and a single value otherwise. We handle cases where control ﬂow and pointers are uncertain. Our results show that we can speed up the execution time of 30 sample runs by an average factor of 2.3 without precision lost or by up to 3.4 with negligible precision lost."
},
{
  "Title": "Code Coverage Analysis in Practice for Large Systems",
  "Type": "Software Engineering in Practice Track",
  "Key": "code-coverage-analysis-practice-large-systems",
  "Authors": ["Yoram Adler", "Noam Behar", "Orna Raz", "Onn Shehory", "Nadav Steindler", "Shmuel Ur", "Aviad Zlotnick"],
  "Affiliations": ["IBM, Israel", "Microsoft, Israel", "Shmuel Ur Innovation, Israel"],
  "Abstract": "Large systems generate immense quantities of code coverage data. A user faced with the task of analyzing this data, for example, to decide on test areas to improve, faces a 'needle in a haystack' problem.\n\nIn earlier studies we introduced substring hole analysis, a technique for presenting large quantities of coverage data in a succinct way. Here we demonstrate the successful use of substring hole analysis on large scale data from industrial software systems. For this end we augment substring hole analysis by introducing a work flow and tool support for practical code coverage analysis.\n\nWe conduct real data experiments indicating that augmented substring hole analysis enables code coverage analysis where it was previously impractical, correctly identifies functionality that is missing from existing tests, and can increase the probability of finding bugs. These facilitate cost-effective code coverage analysis."
},
{
  "Title": "The Code Orb -- Supporting Contextualized Coding via At-a-Glance Views",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "code-orb-supporting-contextualized-coding-glance-views",
  "Authors": ["Nicolas Lopez", "André van der Hoek"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "While code is typically presented as a flat file to a developer who must change it, this flat file exists within a context that can drastically influence how a developer approaches changing it. While the developer clearly must be careful changing any code, they probably should be yet more careful in changing code that recently saw major changes, is barely covered by test cases, and was the source of a number of bugs. Contextualized coding refers to the ability of the developer to effectively use such contextual information while they work on some changes. In this paper, we introduce the Code Orb, a contextualized coding tool that builds upon existing mining and analysis techniques to warn developers on a line-by-line basis of the volatility of the code they are working on. The key insight underneath the Code Orb is that it is neither desired nor possible to always present a code’s context in its entirety; instead, it is necessary to provide an abstracted view of the context that informs the developer of which parts of the code they need to pay more attention to. This paper discusses the principles of and rationale behind contextualized coding, introduces the Code Orb, and illustrates its function with example code and context drawn from the Mylyn project."
},
{
  "Title": "CodeTopics: Which Topic am I Coding Now?",
  "Type": "Demonstrations Track",
  "Key": "codetopics-which-topic-am-i-coding-now",
  "Authors": ["Malcom Gethers", "Trevor Savage", "Massimiliano Di Penta", "Rocco Oliveto", "Denys Poshyvanyk", "Andrea De Lucia"],
  "Affiliations": ["College of William and Mary, USA", "Carnegie Mellon University, USA", "University of Sannio, Italy", "University of Molise, Italy"],
  "Abstract": "Recent studies indicated that showing the similarity between the source code being developed and related high-level artifacts (HLAs), such as requirements, helps developers improve the quality of source code identiﬁers. In this paper, we present CodeTopics, an Eclipse plug-in that in addition to showing the similarity between source code and HLAs also highlights to what extent the code under development covers topics described in HLAs. Such views complement information derived by showing only the similarity between source code and HLAs helping (i) developers to identify functionality that are not implemented yet or (ii) newcomers to comprehend source code artifacts by showing them the topics that these artifacts relate to."
},
{
  "Title": "A Combination Approach for Enhancing Automated Traceability",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "combination-approach-enhancing-automated-traceability",
  "Authors": ["Xiaofan Chen", "John Hosking", "John Grundy"],
  "Affiliations": ["University of Auckland, New Zealand", "Swinburne University of Technology at Melbourne, Australia"],
  "Abstract": "Tracking a variety of traceability links between artifacts assists software developers in comprehension, efficient development, and effective management of a system. Traceability systems to date based on various Information Retrieval (IR) techniques have been faced with a major open research challenge: how to extract these links with both high precision and high recall. In this paper we describe an experimental approach that combines Regular Expression, Key Phrases, and Clustering with IR techniques to enhance the performance of IR for traceability link recovery between documents and source code. Our preliminary experimental results show that our combination technique improves the performance of IR, increases the precision of retrieved links, and recovers more true links than IR alone."
},
{
  "Title": "A Comparison of Model-based and Judgment-based Release Planning in Incremental Software Projects",
  "Type": "Software Engineering in Practice Track",
  "Key": "comparison-model-based-and-judgment-based-release-planning-incremental-software-projects",
  "Authors": ["Hans Christian Benestad", "Jo E. Hannay"],
  "Affiliations": ["Simula Research Laboratory, Norway"],
  "Abstract": "Numerous factors are involved when deciding when to implement which features in incremental software development. To facilitate a rational and eﬃcient planning process, release planning models make such factors explicit and compute release plan alternatives according to optimization principles. However, experience suggests that industrial use of such models is limited. To investigate the feasibility of model and tool support, we compared input factors assumed by release planning models with factors considered by expert planners. The former factors were cataloged by systematically surveying release planning models, while the latter were elicited through repertory grid interviews in three software organizations. The ﬁndings indicate a substantial overlap between the two approaches. However, a detailed analysis reveals that models focus on only select parts of a possibly larger space of relevant planning factors. Three concrete areas of mismatch were identiﬁed: (1) continuously evolving requirements and speciﬁcations, (2) continuously changing prioritization criteria, and (3) authority-based decision processes. With these results in mind, models, tools and guidelines can be adjusted to address better real-life development processes."
},
{
  "Title": "Configuring Global Software Teams: A Multi-Company Analysis of Project Productivity, Quality, and Profits",
  "Type": "Technical/Research Track",
  "Key": "configuring-global-software-teams-multi-company-analysis-project-productivity-quality-and-pr",
  "Authors": ["Narayan Ramasubbu", "Marcelo Cataldo", "Rajesh Krishna Balan", "James D. Herbsleb"],
  "Affiliations": ["Singapore Management University, Singapore", "Carnegie Mellon University, USA"],
  "Abstract": "In this paper, we examined the impact of project-level configurational choices of globally distributed software teams on project productivity, quality, and profits. Our analysis used data from 362 projects of four different firms. These projects spanned a wide range of programming languages, application domain, process choices, and development sites spread over 15 countries and 5 continents. Our analysis revealed fundamental tradeoffs in choosing configurational choices that are optimized for productivity, quality, and/or profits. In particular, achieving higher levels of productivity and quality require diametrically opposed configurational choices. In addition, creating imbalances in the expertise and personnel distribution of project teams significantly helps increase profit margins. However, a profitoriented imbalance could also significantly affect productivity and/or quality outcomes. Analyzing these complex tradeoffs, we provide actionable managerial insights that can help software firms and their clients choose configurations that achieve desired project outcomes in globally distributed software development."
},
{
  "Title": "Covana: Precise Identification of Problems in Pex",
  "Type": "Demonstrations Track",
  "Key": "covana-precise-identification-problems-pex",
  "Authors": ["Xusheng Xiao", "Tao Xie", "Nikolai Tillmann", "Jonathan de Halleux"],
  "Affiliations": ["North Carolina State University, USA", "Microsoft Research, USA"],
  "Abstract": "Achieving high structural coverage is an important goal of software testing. Instead of manual producing tests that achieve high structural coverage, testers or developers can employ tools built based on automated test-generation approaches, such as Pex, to automatically generate such tests. Although these tools can easily generate tests that achieve high structural coverage for simple programs, when applied on complex programs in practice, these tools face various problems, such as the problems of dealing with method calls to external libraries or generating method-call sequences to produce desirable object states. Since these tools are currently not powerful enough to deal with these various problems in testing complex programs, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. In this demo, we present Covana, a tool that precisely identiﬁes and reports problems that prevent Pex from achieving high structural coverage. Covana identiﬁes problems by determining whether branch statements containing not-covered branches have data dependencies on problem candidates."
},
{
  "Title": "Coverage Guided Systematic Concurrency Testing",
  "Type": "Technical/Research Track",
  "Key": "coverage-guided-systematic-concurrency-testing",
  "Authors": ["Chao Wang", "Mahmoud Said", "Aarti Gupta"],
  "Affiliations": ["NEC Laboratories America, USA", "Western Michigan University, USA"],
  "Abstract": "Concurrent programs are notoriously difficult to test, and because of the often large number of thread schedules, testing all possible interleavings is practically infeasible. We propose a coverage-guided systematic testing framework where we use dynamically learned ordering constraints over shared object accesses to select only high-risk interleavings for test execution. An interleaving is of high-risk if it has not be covered by the ordering constraints, meaning that it has concurrency scenarios that have not been tested. Our method consists of two components. First, we utilize dynamic information collected from good test runs to learn ordering constraints over the memory-accessing and synchronization statements. Second, during systematic testing, we use the learned ordering constraints to guide the selection of interleavings for future testing. Our experiments show that the new method can increase the coverage of important concurrency scenarios with a reasonable cost and detect most of the concurrency bugs in practice."
},
{
  "Title": "CREWW - Collaborative Requirements Engineering with Wii-Remotes",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "creww-collaborative-requirements-engineering-wii-remotes",
  "Authors": ["Felix Bott", "Stephan Diehl", "Rainer Lutz"],
  "Affiliations": ["University of Trier, Germany"],
  "Abstract": "In this paper, we present CREWW, a tool for co-located, collaborative CRC modeling and use case analysis. In CRC sessions role play is used to involve all stakeholders when determining whether the current software model completely and consistently captures the modeled use case. In this activity it quickly becomes difficult to keep track of which class is currently active or along which path the current state was reached. CREWW was designed to alleviate these and other weaknesses of the traditional approach."
},
{
  "Title": "Data Analytics for Game Development",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "data-analytics-game-development",
  "Authors": ["Kenneth Hullett", "Nachiappan Nagappan", "Eric Schuh", "John Hopson"],
  "Affiliations": ["UC Santa Cruz, USA", "Microsoft Research, USA", "Microsoft Game Studios, USA", "Bungie Studios, USA"],
  "Abstract": "The software engineering community has had seminal papers on data analysis for software productivity, quality, reliability, performance etc. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. Little work has been done on the emerging digital game industry. In this paper we explore how data can drive game design and production decisions in game development. We define a mixture of qualitative and quantitative data sources, broken down into three broad categories: internal testing, external testing, and subjective evaluations. We present preliminary results of a case study of how data collected from users of a released game can inform subsequent development."
},
{
  "Title": "Dealing with Noise in Defect Prediction",
  "Type": "Technical/Research Track",
  "Key": "dealing-noise-defect-prediction",
  "Authors": ["Sunghun Kim", "Hongyu Zhang", "Rongxin Wu", "Liang Gong"],
  "Affiliations": ["Hong Kong University of Science and Technology, China", "Tsinghua University, China"],
  "Abstract": "Many software defect prediction models have been built using historical defect data obtained by mining software repositories (MSR). Recent studies have discovered that data so collected contain noises because current defect collection practices are based on optional bug fix keywords or bug report links in change logs. Automatically collected defect data based on the change logs could include noises. This paper proposes approaches to deal with the noise in defect data. First, we measure the impact of noise on defect prediction models and provide guidelines for acceptable noise level. We measure noise resistant ability of two well-known defect prediction algorithms and find that in general, for large defect datasets, adding FP (false positive) or FN (false negative) noises alone does not lead to substantial performance differences. However, the prediction performance decreases significantly when the dataset contains 20%-35% of both FP and FN noises. Second, we propose a noise detection and elimination algorithm to address this problem. Our empirical study shows that our algorithm can identify noisy instances with reasonable accuracy. In addition, after eliminating the noises using our algorithm, defect prediction accuracy is improved."
},
{
  "Title": "A Decision Support System for the Classification of Software Coding Faults: A Research Abstract",
  "Type": "Paper",
  "Key": "decision-support-system-classification-software-coding-faults-research-abstract",
  "Authors": ["Billy Kidwell"],
  "Affiliations": ["University of Kentucky, USA"],
  "Abstract": "A decision support system for fault classification is presented. The fault classification scheme is developed to provide guidance in process improvement and fault-based testing. The research integrates results in fault classification, source code analysis, and fault-based testing research. Initial results indicate that existing change type and fault classification schemes are insufficient for this purpose. Development of sufficient schemes and their evaluation are discussed."
},
{
  "Title": "A Declarative Approach to Enable Flexible and Dynamic Service Compositions",
  "Type": "Paper",
  "Key": "declarative-approach-enable-flexible-and-dynamic-service-compositions",
  "Authors": ["Leandro Sales Pinto"],
  "Affiliations": ["Politecnico di Milano, Italy"],
  "Abstract": "Service Orchestration, Speciﬁcation language exception-safe and self-adaptative orchestrations, while currently available orchestration engines do not support the desired dynamism. This is a challenge that needs to be addressed and it is our belief, and starting point of this thesis, that it is intrinsically related with the imperative programming style adopted by almost all available languages and with the approach adopted by engines, which limit themselves to executing orchestrations step-by-step. This obliges service architect to precisely specify every detail in the ﬂow among services, explicitly coding all the diﬀerent routes to go from the beginning to the end of the orchestration, forecasting all possible faults and exceptions that may happen at run-time. This results in a code which is hard to write and maintain, not to mention the impossibility of changing the expected ﬂow at run-time and the limited support to reusing part of pre-deﬁned orchestrations. This thesis proposes a radically diﬀerent approach, which adopts a declarative, goal-oriented language to model service orchestrations, which is easier to use and results in more ﬂexible compositions. At runtime, an ad-hoc engine interprets those models, using planning techniques to ﬁnd a path to accomplish the orchestration goals, while providing powerful exception-handling mechanisms to cope with (un)expected situations, either re-building failed plans or allowing users to modify orchestration models at run-time, thus supporting the construction of highly-dynamic self-adaptative systems."
},
{
  "Title": "On-demand Feature Recommendations Derived from Mining Public Product Descriptions",
  "Type": "Technical/Research Track",
  "Key": "demand-feature-recommendations-derived-mining-public-product-descriptions",
  "Authors": ["Horatiu Dumitru", "Marek Gibiec", "Negar Hariri", "Jane Cleland-Huang", "Bamshad Mobasher", "Carlos Castro-Herrera", "Mehdi Mirakhorli"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "We present a recommender system that models and recommends product features for a given domain. Our approach mines product descriptions from publicly available online specifications, utilizes text mining and a novel incremental diffusive clustering algorithm to discover domain-specific features, generates a feature model that differentiates between commonalities and variants, identifies cross-category features, and then uses association rule mining and the k-Nearest-Neighbor machine learning strategy to generate product specific feature recommendations. Our recommender system supports the relatively labor-intensive task of domain analysis, potentially increasing opportunities for re-use, reducing time-to-market, and delivering more competitive software products. The approach is empirically validated against 20 different product categories using thousands of product descriptions mined from a repository of free software applications."
},
{
  "Title": "A Demonstration of a Distributed Software Design Sketching Tool",
  "Type": "Demonstrations Track",
  "Key": "demonstration-distributed-software-design-sketching-tool",
  "Authors": ["Nicolas Mangano", "Mitch Dempsey", "Nicolas Lopez", "André van der Hoek"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "Software designers frequently sketch when they design, particularly during the early phases of exploration of a design problem and its solution. In so doing, they shun formal design tools, the reason being that such tools impose conformity and precision prematurely. Sketching on the other hand is a highly fluid and flexible way of expressing oneself. In this paper, we present Calico, a sketch-based distributed software design tool that supports software designers with a variety of features that improve over the use of just pen-and-paper or a regular whiteboard, and are tailored specifically for software design. Calico is meant to be used on electronic whiteboards or tablets, and provides for rapid creation and manipulation of design content by sets of developers who can collaborate distributedly."
},
{
  "Title": "Deploying CogTool: Integrating Quantitative Usability Assessment into Real-World Software Development",
  "Type": "Software Engineering in Practice Track",
  "Key": "deploying-cogtool-integrating-quantitative-usability-assessment-real-world-software-developm",
  "Authors": ["Rachel Bellamy", "Bonnie John", "Sandra Kogan"],
  "Affiliations": ["IBM T. J. Watson Research Center, USA", "Carnegie Mellon University, USA", "IBM Software Group, USA"],
  "Abstract": "Usability concerns are often difficult to integrate into real-world software development processes. To remedy this situation, IBM research and development, partnering with Carnegie Mellon University, has begun to employ a repeatable and quantifiable usability analysis method, embodied in CogTool, in its development practice. CogTool analyzes tasks performed on an interactive system from a storyboard and a demonstration of tasks on that storyboard, and predicts the time a skilled user will take to perform those tasks. We discuss how IBM designers and UX professionals used CogTool in their existing practice for contract compliance, communication within a product team and between a product team and its customer, assigning appropriate personnel to fix customer complaints, and quantitatively assessing design ideas before a line of code is written. We then reflect on the lessons learned by both the development organizations and the researchers attempting this technology transfer from academic research to integration into real-world practice, and we point to future research to even better serve the needs of practice."
},
{
  "Title": "Design and Implementation of a Data Analytics Infrastructure in Support of Crisis Informatics Research",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "design-and-implementation-data-analytics-infrastructure-support-crisis-informatics-research",
  "Authors": ["Kenneth M. Anderson", "Aaron Schram"],
  "Affiliations": ["University of Colorado, USA"],
  "Abstract": "Crisis informatics is an emerging research area that studies how information and communication technology (ICT) is used in emergency response. An important branch of this area includes investigations of how members of the public make use of ICT to aid them during mass emergencies. Data collection and analytics during crisis events is a critical prerequisite for performing such research, as the data generated during these events on social media networks are ephemeral and easily lost. We report on the current state of a crisis informatics data analytics infrastructure that we are developing in support of a broader, interdisciplinary research program. We also comment on the role that software engineering research plays in these increasingly common, highly interdisciplinary research eﬀorts."
},
{
  "Title": "Detecting Architecturally-Relevant Code Smells in Evolving Software Systems",
  "Type": "Paper",
  "Key": "detecting-architecturally-relevant-code-smells-evolving-software-systems",
  "Authors": ["Isela Macia"],
  "Affiliations": ["Pontifical Catholic University of Rio de Janeiro, Brazil"],
  "Abstract": "Refactoring tends to avoid the early deviation of a program from its intended architecture design. However, there is little knowledge about whether the manifestation of code smells in evolving software is indicator of architectural deviations. A fundamental difficulty in this process is that developers are only equipped with static analysis techniques for the source code, which do not exploit traceable architectural information. This work addresses this problem by: (1) identifying a family of architecturally-relevant code smells; (2) providing empirical evidence about the correlation of code smell patterns and architectural degeneration; (3) proposing a set of metrics and detection strategies and that exploit traceable architectural information in smell detection; and (4) conceiving a technique to support the early identification of architecture degeneration symptoms by reasoning about code smell patterns."
},
{
  "Title": "Detecting Cross-browser Issues in Web Applications",
  "Type": "Paper",
  "Key": "detecting-cross-browser-issues-web-applications",
  "Authors": ["Shauvik Roy Choudhary"],
  "Affiliations": ["Georgia Institute of Technology, USA"],
  "Abstract": "Cross-browser issues are prevalent in web applications. However, existing tools require considerable manual effort from developers to detect such issues. Our technique and prototype tool - WEBDIFF detects such issues automatically and reports them to the developer. Along with each issue reported, the tool also provides details about the affected HTML element, thereby helping the developer to ﬁx the issue. WEBDIFF is the ﬁrst technique to apply concepts from computer vision and graph theory to identify cross-browser issues in web applications. Our results show that W EB D IFF is practical and can ﬁnd issues in real world web applications."
},
{
  "Title": "Detecting Software Modularity Violations",
  "Type": "Technical/Research Track",
  "Key": "detecting-software-modularity-violations",
  "Authors": ["Sunny Wong", "Yuanfang Cai", "Miryung Kim", "Michael Dalton"],
  "Affiliations": ["Drexel University, USA", "University of Texas at Austin, USA"],
  "Abstract": "This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identiﬁed using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identiﬁed violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches."
},
{
  "Title": "Diagnosing New Faults Using Mutants and Prior Faults",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "diagnosing-new-faults-using-mutants-and-prior-faults",
  "Authors": ["Syed Shariyar Murtaza", "Nazim Madhavji", "Mechelle Gittens", "Zude Li"],
  "Affiliations": ["University of Western Ontario, Canada", "University of West Indies, Barbados"],
  "Abstract": "Literature indicates that 20% of a program’s code is responsible for 80% of the faults, and 50-90% of the field failures are rediscoveries of previous faults. Despite this, identification of faulty code can consume 30-40% time of error correction. Previous fault-discovery techniques focusing on field failures either require many pass-fail traces, discover only crashing failures, or identify faulty “files” (which are of large granularity) as origin of the source code. In our earlier work (the F007 approach), we identify faulty “functions” (which are of small granularity) in a field trace by using earlier resolved traces of the same release, which limits it to the known faulty functions. This paper overcomes this limitation by proposing a new “strategy” to identify new and old faulty functions using F007. This strategy uses failed traces of mutants (artificial faults) and failed traces of prior releases to identify faulty functions in the traces of succeeding release. Our results on two UNIX utilities (i.e., Flex and Gzip) show that faulty functions in the traces of the majority (60-85%) of failures of a new software release can be identified by reviewing only 20% of the code. If compared against prior techniques then this is a notable improvement in terms of contextual knowledge required and accuracy in the discovery of finer-grain fault origin."
},
{
  "Title": "Digitally Annexing Desk Space for Software Development",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "digitally-annexing-desk-space-software-development",
  "Authors": ["John Hardy", "Christopher Bull", "Gerald Kotonya", "Jon Whittle"],
  "Affiliations": ["Lancaster University, UK"],
  "Abstract": "Software engineering is a team activity yet the programmer’s key tool, the IDE, is still largely that of a soloist. This paper describes the vision, implementation and initial evaluation of CoffeeTable – a fully featured research prototype resulting from our reflections on the software design process. CoffeeTable exchanges the traditional IDE for one built around a shared interactive desk. The proposed solution encourages smooth transitions between agile and traditional modes of working whilst helping to create a shared vision and common reference frame – key to sustaining a good design. This paper also presents early results from the evaluation of CoffeeTable and offers some insights from the lessons learned. In particular, it highlights the role of developer tools and the software constructions that are shaped by them."
},
{
  "Title": "Directed Test Suite Augmentation",
  "Type": "Paper",
  "Key": "directed-test-suite-augmentation",
  "Authors": ["Zhihong Xu"],
  "Affiliations": ["University of Nebraska-Lincoln, USA"],
  "Abstract": "Test suite augmentation techniques are used in regression testing to identify code elements aﬀected by changes and to generate test cases to cover those elements. Whereas methods and techniques to ﬁnd aﬀected elements have been extensively researched in regression testing, how to generate new test cases to cover these elements cost-eﬀectively has rarely been studied. It is known that generating test cases is very expensive, so we want to focus on this second step. We believe that reusing existing test cases will help us achieve this task. This research intends to provide a framework for test suite augmentation techniques that will reuse existing test cases to automatically generate new test cases to cover as many aﬀected elements as possible cost-eﬀectively."
},
{
  "Title": "Does the Initial Environment Impact the Future of Developers?",
  "Type": "Technical/Research Track",
  "Key": "does-initial-environment-impact-future-developers",
  "Authors": ["Minghui Zhou", "Audris Mockus"],
  "Affiliations": ["Peking University, China", "Avaya Labs Research, USA"],
  "Abstract": "Software developers need to develop technical and social skills to be successful in large projects. We model the relative sociality of a developer as a ratio between the size of her communication network and the number of tasks she participates in. We obtain both measures from the problem tracking systems with her work-flow peer network representing her social learning, and the issues she has been working on representing her technical learning. Using three open source and three traditional projects we investigate how the initial project environment reflected by the sociality measure at the time a developer joins, affects her learning trajectory. In particular, the probability that a new developer will become one of long-term and productive developers is highest when the project sociality is low and there are significant differences between the social learning trajectories of the developers who join in low and in high sociality environments and between open source and commercial projects. These findings point out the importance of the initial environment in determining the future of the developers and may lead to better training and learning strategies in software organizations."
},
{
  "Title": "A Domain Specific Requirements Model for Scientific Computing",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "domain-specific-requirements-model-scientific-computing",
  "Authors": ["Yang Li", "Nitesh Narayan", "Jonas Helming", "Maximilian Koegel"],
  "Affiliations": ["TU München, Germany"],
  "Abstract": "Requirements engineering is a core activity in software engineering. However, formal requirements engineering methodologies and documented requirements are often missing in scientiﬁc computing projects. We claim that there is a need for methodologies, which capture requirements for scientiﬁc computing projects, because traditional requirements engineering methodologies are diﬃcult to apply in this domain. We propose a novel domain speciﬁc requirements model to meet this need. We conducted an exploratory experiment to evaluate the usage of this model in scientiﬁc computing projects. The results indicate that the proposed model facilitates the communication across the domain boundary, which is between the scientiﬁc computing domain and the software engineering domain. It supports requirements elicitation for the projects eﬃciently."
},
{
  "Title": "Dynamic Shape Analysis of Program Heap using Graph Spectra",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "dynamic-shape-analysis-program-heap-using-graph-spectra",
  "Authors": ["Muhammad Zubair Malik"],
  "Affiliations": ["University of Texas at Austin, USA"],
  "Abstract": "Programs written in languages such as Java and C# maintain most of their state on the heap. The size and complexity of these programs pose a challenge in understanding and maintaining them; Heap analysis by summarizing the state of heap graphs assists programmers in these tasks. In this paper we present a novel dynamic heap analysis technique that uses spectra of the heap graphs to summarize them. These summaries capture the shape of recursive data structures as dynamic invariants or likely properties of these structures that must be preserved after any destructive update. Initial experiments show that this approach can generate meaningful summaries for a range of subject structures."
},
{
  "Title": "DyTa: Dynamic Symbolic Execution Guided with Static Verification Results",
  "Type": "Demonstrations Track",
  "Key": "dyta-dynamic-symbolic-execution-guided-static-verification-results",
  "Authors": ["Xi Ge", "Kunal Taneja", "Tao Xie", "Nikolai Tillmann"],
  "Affiliations": ["North Carolina State University, USA", "Microsoft Research, USA"],
  "Abstract": "Software-defect detection is an increasingly important research topic in software engineering. To detect defects in a program, static veriﬁcation and dynamic test generation are two important proposed techniques. However, both of these techniques face their respective issues. Static veriﬁcation produces false positives, and on the other hand, dynamic test generation is often time consuming. To address the limitations of static veriﬁcation and dynamic test generation, we present an automated defect-detection tool, called DyTa, that combines both static veriﬁcation and dynamic test generation. DyTa consists of a static phase and a dynamic phase. The static phase detects potential defects with a static checker; the dynamic phase generates test inputs through dynamic symbolic execution to conﬁrm these potential defects. DyTa reduces the number of false positives compared to static veriﬁcation and performs more efﬁciently compared to dynamic test generation."
},
{
  "Title": "Empirical Assessment of MDE in Industry",
  "Type": "Technical/Research Track",
  "Key": "empirical-assessment-mde-industry",
  "Authors": ["John Hutchinson", "Jon Whittle", "Mark Rouncefield", "Steinar Kristoffersen"],
  "Affiliations": ["Lancaster University, UK", "Østfold University College and Møreforskning Molde AS, Norway"],
  "Abstract": "This paper presents some initial results from a twelve-month empirical research study of model driven engineering (MDE). Using largely qualitative questionnaire and interview methods we investigate and document a range of technical, organizational and social factors that apparently influence organizational responses to MDE, specifically its perception as a successful or unsuccessful organizational intervention. We then outline a range of lessons learned. Whilst, as with all qualitative research, these lessons should be interpreted with care, they should also be seen as providing a greater understanding of MDE practice in industry, as well as the varied, and occasionally surprising, social, technical and organizational factors that affect success and failure. We conclude by suggesting how the next phase of the research will attempt to investigate some of these issues from a different angle and in greater depth."
},
{
  "Title": "An Empirical Investigation into the Role of API-Level Refactorings during Software Evolution",
  "Type": "Technical/Research Track",
  "Key": "empirical-investigation-role-api-level-refactorings-during-software-evolution",
  "Authors": ["Miryung Kim", "Dongxiang Cai", "Sunghun Kim"],
  "Affiliations": ["University of Texas at Austin, USA", "Hong Kong University of Science and Technology, China"],
  "Abstract": "It is widely believed that refactoring improves software quality and programmer productivity by making it easier to maintain and understand software systems. However, the role of refactorings has not been systematically investigated using ﬁne-grained evolution history. We quantitatively and qualitatively studied API-level refactorings and bug ﬁxes in three large open source projects, totaling 26523 revisions of evolution.\n\nThe study found several surprising results: One, there is an increase in the number of bug ﬁxes after API-level refactorings. Two, the time taken to ﬁx bugs is shorter after API-level refactorings than before. Three, a large number of refactoring revisions include bug ﬁxes at the same time or are related to later bug ﬁx revisions. Four, API-level refactorings occur more frequently before than after major software releases. These results call for re-thinking refactoring’s true beneﬁts. Furthermore, frequent ﬂoss refactoring mistakes observed in this study call for new software engineering tools to support safe application of refactoring and behavior modifying edits together."
},
{
  "Title": "Empirical Results on the Study of Software Vulnerabilities",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "empirical-results-study-software-vulnerabilities",
  "Authors": ["Yan Wu", "Harvey Siy", "Robin Gandhi"],
  "Affiliations": ["University of Nebraska at Omaha, USA"],
  "Abstract": "While the software development community has put a significant effort to capture the artifacts related to a discovered vulnerability in organized repositories, much of this information is not amenable to meaningful analysis and requires a deep and manual inspection. In the software assurance community a body of knowledge that provides an enumeration of common weaknesses has been developed, but it is not readily usable for the study of vulnerabilities in specific projects and user environments. We propose organizing the information in project repositories around semantic templates. In this paper, we present preliminary results of an experiment conducted to evaluate the effectiveness of using semantic templates as an aid to studying software vulnerabilities."
},
{
  "Title": "An Empirical Study of Build Maintenance Effort",
  "Type": "Technical/Research Track",
  "Key": "empirical-study-build-maintenance-effort",
  "Authors": ["Shane McIntosh", "Bram Adams", "Thanh H. D. Nguyen", "Yasutaka Kamei", "Ahmed E. Hassan"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system speciﬁcations require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of diﬀerent sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the diﬀerence in scale, the build system churn rate is comparable to that of the source code, and build changes induce more relative churn on the build system than source code changes induce on the source code. Furthermore, build maintenance yields up to a 27% overhead on source code development and a 44% overhead on test development. Up to 79% of source code developers and 89% of test code developers are signiﬁcantly impacted by build maintenance, yet investment in build experts can reduce the proportion of impacted developers to 22% of source code developers and 24% of test code developers."
},
{
  "Title": "Enabling the Runtime Assertion Checking of Concurrent Contracts for the Java Modeling Language",
  "Type": "Software Engineering in Practice Track",
  "Key": "enabling-runtime-assertion-checking-concurrent-contracts-java-modeling-language",
  "Authors": ["Wladimir Araujo", "Lionel C. Briand", "Yvan Labiche"],
  "Affiliations": ["Juniper Networks, Canada", "Simula Research Laboratory and University of Oslo, Norway", "Carleton University, Canada"],
  "Abstract": "Though there exists ample support for Design by Contract (DbC) for sequential programs, applying DbC to concurrent programs presents several challenges. In previous work, we extended the Java Modeling Language (JML) with constructs to specify concurrent contracts for Java programs. We present a runtime assertion checker (RAC) for the expanded JML capable of verifying assertions for concurrent Java programs. We systematically evaluate the validity of system testing results obtained via runtime assertion checking using actual concurrent and functional faults on a highly concurrent industrial system from the telecommunications domain."
},
{
  "Title": "An End-User Demonstration Approach to Support Aspect-Oriented Modeling",
  "Type": "Paper",
  "Key": "end-user-demonstration-approach-support-aspect-oriented-modeling",
  "Authors": ["Yu Sun"],
  "Affiliations": ["University of Alabama at Birmingham, USA"],
  "Abstract": "Aspect-oriented modeling (AOM) is a technique to separate concerns that crosscut the modularity boundaries of a modeling hierarchy. AOM is traditionally supported by manual editing or writing model transformation rules that refine a base model. However, traditional model weaving approaches present challenges to those who are unfamiliar with a model transformation language or metamodel definitions. This poster describes an approach to weave aspect models by recording and analyzing demonstrated operations by end-users."
},
{
  "Title": "Estimating Footprints of Model Operations",
  "Type": "Technical/Research Track",
  "Key": "estimating-footprints-model-operations",
  "Authors": ["Cédric Jeanneret", "Martin Glinz", "Benoit Baudry"],
  "Affiliations": ["University of Zurich, Switzerland", "IRISA, France"],
  "Abstract": "When performed on a model, a set of operations (e.g., queries or model transformations) rarely uses all the information present in the model. Unintended underuse of a model can indicate various problems: the model may contain more detail than necessary or the operations may be immature or erroneous. Analyzing the footprints of the operations — i.e., the part of a model actually used by an operation — is a simple technique to diagnose and analyze such problems. However, precisely calculating the footprint of an operation is expensive, because it requires analyzing the operation’s execution trace.\n\nIn this paper, we present an automated technique to estimate the footprint of an operation without executing it. We evaluate our approach by applying it to 75 models and ﬁve operations. Our technique provides software engineers with an eﬃcient, yet precise, evaluation of the usage of their models."
},
{
  "Title": "An Evaluation of the Internal Quality of Business Applications: Does Size Matter?",
  "Type": "Software Engineering in Practice Track",
  "Key": "evaluation-internal-quality-business-applications-does-size-matter",
  "Authors": ["Bill Curtis", "Jay Sappidi", "Jitendra Subramanyam"],
  "Affiliations": ["CAST, USA"],
  "Abstract": "This study summarizes results of a study of the internal, structural quality of 288 business applications comprising 108 million lines of code collected from 75 companies in 8 industry segments. These applications were submitted to a static analysis that evaluates quality within and across application components that may be coded in different languages. The analysis consists of evaluating the application against a repository of over 900 rules of good architectural and coding practice. Results are presented for measures of security, performance, and changeability. The effect of size on quality is evaluated, and the ability of modularity to reduce the impact of size is suggested by the results."
},
{
  "Title": "Evolve: Tool Support for Architecture Evolution",
  "Type": "Demonstrations Track",
  "Key": "evolve-tool-support-architecture-evolution",
  "Authors": ["Andrew McVeigh", "Jeff Kramer", "Jeff Magee"],
  "Affiliations": ["Imperial College London, UK"],
  "Abstract": "Incremental change is intrinsic to both the initial development and subsequent evolution of large complex software systems. Evolve is a graphical design tool that captures this incremental change in the definition of software architecture. It supports a principled and manageable way of dealing with unplanned change and extension. In addition, Evolve supports decentralized evolution in which software is extended and evolved by multiple independent developers. Evolve supports a model-driven approach in that architecture definition is used to directly construct both initial implementations and extensions to these implementations. The tool implements Backbone - an architectural description language (ADL), which has both a textual and a UML2, based graphical representation. The demonstration focuses on the graphical representation."
},
{
  "Title": "Experiences with Text Mining Large Collections of Unstructured Systems Development Artifacts at JPL",
  "Type": "Software Engineering in Practice Track",
  "Key": "experiences-text-mining-large-collections-unstructured-systems-development-artifacts-jpl",
  "Authors": ["Dan Port", "Allen Nikora", "Jairus Hihn", "Liguo Huang"],
  "Affiliations": ["University of Hawaii, USA", "Jet Propulsion Laboratory and California Institute of Technology, USA", "Southern Methodist University, USA"],
  "Abstract": "Often repositories of systems engineering artifacts at NASA’s Jet Propulsion Laboratory (JPL) are so large and poorly structured that they have outgrown our capability to effectively manually process their contents to extract useful information. Sophisticated text mining methods and tools seem a quick, low-effort approach to automating our limited manual efforts. Our experiences of exploring such methods mainly in three areas including historical risk analysis, defect identification based on requirements analysis, and over-time analysis of system anomalies at JPL, have shown that obtaining useful results requires substantial unanticipated efforts - from preprocessing the data to transforming the output for practical applications. We have not observed any quick “wins” or realized benefit from short-term effort avoidance through automation in this area. Surprisingly we have realized a number of unexpected long-term benefits from the process of applying text mining to our repositories. This paper elaborates some of these benefits and our important lessons learned from the process of preparing and applying text mining to large unstructured system artifacts at JPL aiming to benefit future TM applications in similar problem domains and also in hope for being extended to broader areas of applications."
},
{
  "Title": "Exploiting Hardware Advances for Software Testing and Debugging",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "exploiting-hardware-advances-software-testing-and-debugging",
  "Authors": ["Mary Lou Soffa", "Kristen R. Walcott", "Jason Mars"],
  "Affiliations": ["University of Virginia, USA"],
  "Abstract": "Despite the emerging ubiquity of hardware monitoring mechanisms and prior research work in other fields, the applicability and usefulness of hardware monitoring mechanisms have not been fully scrutinized for software engineering. In this work, we identify several recently developed hardware mechanisms that lend themselves well to structural test coverage analysis and automated fault localization and explore their potential. We discuss key factors impacting the applicability of hardware monitoring mechanism for these software engineering tasks, present novel online analyses leveraging these mechanisms, and provide preliminary results demonstrating the promise of this emerging hardware."
},
{
  "Title": "Exploring, Exposing, and Exploiting Emails to Include Human Factors in Software Engineering",
  "Type": "Paper",
  "Key": "exploring-exposing-and-exploiting-emails-include-human-factors-software-engineering",
  "Authors": ["Alberto Bacchelli"],
  "Affiliations": ["University of Lugano, Switzerland"],
  "Abstract": "Researchers mine software repositories to support software maintenance and evolution. The analysis of the structured data, mainly source code and changes, has several beneﬁts and oﬀers precise results. This data, however, leaves communication in the background, and does not permit a deep investigation of the human factor, which is crucial in software engineering. Software repositories also archive documents, such as emails or comments, that are used to exchange knowledge among people— we call it “people-centric information.” By covering this data, we include the human factor in our analysis, yet its unstructured nature makes it currently sub-exploited. Our work, by focusing on email communication and by implementing the necessary tools, investigates methods for exploring, exposing, and exploiting unstructured data. We believe it is possible to close the gap between development and communication, extract opinions, habits, and views of developers, and link implementation to its rationale; we see in a future where software analysis and development is routinely augmented with people-centric information."
},
{
  "Title": "Factors Leading to Integration Failures in Global Feature-Oriented Development: An Empirical Analysis",
  "Type": "Technical/Research Track",
  "Key": "factors-leading-integration-failures-global-feature-oriented-development-empirical-analysis",
  "Authors": ["Marcelo Cataldo", "James D. Herbsleb"],
  "Affiliations": ["Carnegie Mellon University, USA"],
  "Abstract": "Feature-driven software development is a novel approach that has grown in popularity over the past decade. Researchers and practitioners alike have argued that numerous benefits could be garnered from adopting a feature-driven development approach. However, those persuasive arguments have not been matched with supporting empirical evidence. Moreover, developing software systems around features involves new technical and organizational elements that could have significant implications for outcomes such as software quality. This paper presents an empirical analysis of a large-scale project that implemented 1195 features in a software system. We examined the impact that technical attributes of product features, attributes of the feature teams and crossfeature interactions have on software integration failures. Our results show that technical factors such as the nature of component dependencies and organizational factors such as the geographic dispersion of the feature teams and the role of the feature owners had complementary impact suggesting their independent and important role in terms of software quality. Furthermore, our analyses revealed that cross-feature interactions, measured as the number of architectural dependencies between two product features, are a major driver of integration failures. The research and practical implications of our results are discussed."
},
{
  "Title": "Feature Cohesion in Software Product Lines: An Exploratory Study",
  "Type": "Technical/Research Track",
  "Key": "feature-cohesion-software-product-lines-exploratory-study",
  "Authors": ["Sven Apel", "Dirk Beyer"],
  "Affiliations": ["University of Passau, Germany", "Simon Fraser University, Canada"],
  "Abstract": "Software product lines gain momentum in research and industry. Many product-line approaches use features as a central abstraction mechanism. Feature-oriented software development aims at encapsulating features in cohesive units to support program comprehension, variability, and reuse. Surprisingly, not much is known about the characteristics of cohesion in feature-oriented product lines, although proper cohesion is of special interest in product-line engineering due to its focus on variability and reuse. To ﬁll this gap, we conduct an exploratory study on forty software product lines of different sizes and domains. A distinguishing property of our approach is that we use both classic software measures and novel measures that are based on distances in clustering layouts, which can be used also for visual exploration of product-line architectures. This way, we can draw a holistic picture of feature cohesion. In our exploratory study, we found several interesting correlations (e.g., between development process and feature cohesion) and we discuss insights and perspectives of investigating feature cohesion (e.g., regarding feature interfaces and programming style)."
},
{
  "Title": "Finding Relevant Functions in Millions of Lines of Code",
  "Type": "Paper",
  "Key": "finding-relevant-functions-millions-lines-code",
  "Authors": ["Collin McMillan"],
  "Affiliations": ["College of William and Mary, USA"],
  "Abstract": "Source code search engines locate and display fragments of code relevant to user queries. These fragments are often isolated and detached from one another. Programmers need to see how source code interacts in order to understand the concepts implemented in that code, however. In this paper, we present Portfolio, a source code search engine that retrieves and visualizes relevant functions as chains of function invocations. We evaluated Portfolio against Google Code Search and Koders in a case study with 49 professional programmers. Portfolio outperforms both of these engines in terms of relevance and visualization of the returned results."
},
{
  "Title": "FireDetective: Understanding Ajax Client/Server Interactions",
  "Type": "Demonstrations Track",
  "Key": "firedetective-understanding-ajax-clientserver-interactions",
  "Authors": ["Nick Matthijssen", "Andy Zaidman"],
  "Affiliations": ["Delft University of Technology, Netherlands"],
  "Abstract": "Ajax-enabled web applications are a new breed of highly interactive, highly dynamic web applications. Although Ajax allows developers to create rich web applications, Ajax applications can be difficult to comprehend and thus to maintain. FireDetective aims to facilitate the understanding of Ajax applications. It uses dynamic analysis at both the client (browser) and server side and subsequently connects both traces for further analysis."
},
{
  "Title": "Flexible Generators for Software Reuse and Evolution",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "flexible-generators-software-reuse-and-evolution",
  "Authors": ["Stan Jarzabek", "Ha Duy Trung"],
  "Affiliations": ["National University of Singapore, Singapore"],
  "Abstract": "Developers tend to use models and generators during initial development, but often abandon them later in software evolution and reuse. One reason for that is that code generated from models (e.g., UML) is often manually modified, and changes cannot be easily propagated back to models. Once models become out of sync with code, any future re-generation of code overrides manual modifications. We propose a flexible generator solution that alleviates the above problem. The idea is to let developers weave arbitrary manual modifications into the generation process, rather than modify already generated code. A flexible generator stores specifications of manual modifications in executable form, so that weaving can be automatically re-done any time code is regenerated from modified models. In that way, models and manual modification can evolve independently but in sync with each other, and the generated code never gets directly changed. As a proof of concept, we have already built a flexible generator prototype by a merger of conventional generation system and variability technique to handle manual modifications. We believe a flexible generator approach alleviates an important problem that hinders wide spread adoption of MDD in software practice."
},
{
  "Title": "A Formal Approach to Software Synthesis for Architectural Platforms",
  "Type": "Paper",
  "Key": "formal-approach-software-synthesis-architectural-platforms",
  "Authors": ["Hamid Bagheri"],
  "Affiliations": ["University of Virginia, USA"],
  "Abstract": "Software-intensive systems today often rely on middleware platforms as major building blocks. As such, the architectural choices of such systems are being driven to a significant extent by such platforms. However, the diversity and rapid evolution of these platforms lead to architectural choices quickly becoming obsolete. Yet architectural choices are among the most difficult to change. This paper presents a novel and formal approach to end-to-end transformation of application models into architecturally correct code, averting the problem of mapping application models to such architectural platforms."
},
{
  "Title": "A Framework for Automated Testing of JavaScript Web Applications",
  "Type": "Technical/Research Track",
  "Key": "framework-automated-testing-javascript-web-applications",
  "Authors": ["Shay Artzi", "Julian Dolby", "Simon Holm Jensen", "Anders Møller", "Frank Tip"],
  "Affiliations": ["IBM Research, USA", "Aarhus University, Denmark"],
  "Abstract": "Current practice in testing JavaScript web applications requires manual construction of test cases, which is difficult and tedious. We present a framework for feedback-directed automated test generation for JavaScript in which execution is monitored to collect information that directs the test generator towards inputs that yield increased coverage. We implemented several instantiations of the framework, corresponding to variations on feedback-directed random testing, in a tool called Artemis. Experiments on a suite of JavaScript applications demonstrate that a simple instantiation of the framework that uses event handler registrations as feedback information produces surprisingly good coverage if enough tests are generated. However, by also using coverage information and read-write sets as feedback information, a slightly better level of coverage can be achieved, and sometimes with many fewer tests. The generated tests can be used for detecting HTML validity problems and other programming errors."
},
{
  "Title": "A Framework for the Integration of User Centered Design and Agile Software Development Processes",
  "Type": "Paper",
  "Key": "framework-integration-user-centered-design-and-agile-software-development-processes",
  "Authors": ["Dina Salah"],
  "Affiliations": ["University of York, UK"],
  "Abstract": "Agile and user centered design integration (AUCDI) is of signiﬁcant interest to researchers who want to achieve synergy and eliminate limitations of each. Currently, there are no clear principles or guidelines for practitioners to achieve successful integration. In addition, substantial diﬀerences exist between agile and UCD approaches which pose challenges to integration attempts. As a result, practitioners developed individual integration strategies. However, success evaluation of current AUCDI attempts has been anecdotal. Moreover, AUCDI attempts cannot be generalized to provide guidance and assistance to other projects or organizations with diﬀerent needs. My thesis aims to provide a Software Process Improvement (SPI) framework for AUCDI by providing generic guidelines and practices for organizations aspiring to achieve AUCDI in order to address AUCDI challenges including: introducing systematicity and structure into AUCDI, assessing AUCDI processes, and accommodating project and organizational characteristics."
},
{
  "Title": "Frequency and Risks of Changes to Clones",
  "Type": "Technical/Research Track",
  "Key": "frequency-and-risks-changes-clones",
  "Authors": ["Nils Göde", "Rainer Koschke"],
  "Affiliations": ["University of Bremen, Germany"],
  "Abstract": "Code Clones—duplicated source fragments—are said to increase maintenance eﬀort and to facilitate problems caused by inconsistent changes to identical parts. While this is certainly true for some clones and certainly not true for others, it is unclear how many clones are real threats to the system’s quality and need to be taken care of. Our analysis of clone evolution in mature software projects shows that most clones are rarely changed and the number of unintentional inconsistent changes to clones is small. We thus have to carefully select the clones to be managed to avoid unnecessary eﬀort managing clones with no risk potential."
},
{
  "Title": "Fuzzy Set-based Automatic Bug Triaging",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "fuzzy-set-based-automatic-bug-triaging",
  "Authors": ["Ahmed Tamrawi", "Tung T. Nguyen", "Jafar Al-Kofahi", "Tien N. Nguyen"],
  "Affiliations": ["Iowa State University, USA"],
  "Abstract": "Assigning a bug to the right developer is a key in reducing the cost, time, and efforts for developers in a bug fixing process. This assignment process is often referred to as bug triaging. In this paper, we propose Bugzie, a novel approach for automatic bug triaging based on fuzzy set-based modeling of bug-fixing expertise of developers. Bugzie considers a system to have multiple technical aspects, each is associated with technical terms. Then, it uses a fuzzy set to represent the developers who are capable/competent of fixing the bugs relevant to each term. The membership function of a developer in a fuzzy set is calculated via the terms extracted from the bug reports that (s)he has fixed, and the function is updated as new fixed reports are available. For a new bug report, its terms are extracted and corresponding fuzzy sets are union'ed. Potential fixers will be recommended based on their membership scores in the union'ed fuzzy set. Our preliminary results show that Bugzie achieves higher accuracy and efficiency than other state-of-the-art approaches."
},
{
  "Title": "GATE: Game-based Testing Environment",
  "Type": "Paper",
  "Key": "gate-game-based-testing-environment",
  "Authors": ["Ning Chen"],
  "Affiliations": ["Hong Kong University of Science and Technology, China"],
  "Abstract": "In this paper, we propose a game-based public testing mechanism called GATE. The purpose of GATE is to make use of the rich human resource on the Internet to help increase effectiveness in software testing and improve test adequacy. GATE facilitates public testing in three main steps: 1) decompose the test criterion satisfaction problem into many smaller sub-model satisfaction problems; 2) construct games for each individual sub-models and presenting the games to the public through web servers; 3) collect and convert public users’ action sequence data into real test cases which guarantee to cover not adequately tested elements. A preliminary study on apache-commons-math library shows that 44% of the branches have not been adequately tested by state of the art automatic test generation techniques. Among these branches, at least 42% are decomposable by GATE into smaller sub-problems. These elements naturally become the potential targets of GATE for public game-based testing."
},
{
  "Title": "The Hidden Experts in Software-Engineering Communication",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "hidden-experts-software-engineering-communication",
  "Authors": ["Irwin Kwan", "Daniela Damian"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "Sharing knowledge in a timely fashion is important in distributed software development. However, because experts are difficult to locate, developers tend to broadcast information to find the right people, which leads to overload and to communication breakdowns. We study the context in which experts are included in an email discussion so that team members can identify experts sooner. In this paper, we conduct a case study examining why people emerge in discussions by examining email within a distributed team. We find that people emerge in the following four situations: when a crisis occurs, when they respond to explicit requests, when they are forwarded in announcements, and when discussants follow up on a previous event such as a meeting. We observe that emergent people respond not only to situations where developers are seeking expertise, but also to execute routine tasks. Our findings have implications for expertise seeking and knowledge management processes."
},
{
  "Title": "How Do Programmers Ask and Answer Questions on the Web?",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "how-do-programmers-ask-and-answer-questions-web",
  "Authors": ["Christoph Treude", "Ohad Barzilay", "Margaret-Anne Storey"],
  "Affiliations": ["University of Victoria, Canada", "Tel-Aviv University, Israel"],
  "Abstract": "Question and Answer (Q&A) websites, such as Stack Overﬂow, use social media to facilitate knowledge exchange between programmers and ﬁll archives with millions of entries that contribute to the body of knowledge in software development. Understanding the role of Q&A websites in the documentation landscape will enable us to make recommendations on how individuals and companies can leverage this knowledge eﬀectively. In this paper, we analyze data from Stack Overﬂow to categorize the kinds of questions that are asked, and to explore which questions are answered well and which ones remain unanswered. Our preliminary ﬁndings indicate that Q&A websites are particularly eﬀective at code reviews and conceptual questions. We pose research questions and suggest future work to explore the motivations of programmers that contribute to Q&A websites, and to understand the implications of turning Q&A exchanges into technical mini-blogs through the editing of questions and answers."
},
{
  "Title": "Identifying Method Friendships to Remove the Feature Envy Bad Smell",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "identifying-method-friendships-remove-feature-envy-bad-smell",
  "Authors": ["Rocco Oliveto", "Malcom Gethers", "Gabriele Bavota", "Denys Poshyvanyk", "Andrea De Lucia"],
  "Affiliations": ["University of Molise, Italy", "College of William and Mary, USA", "University of Salerno, Italy"],
  "Abstract": "We propose a novel approach to identify Move Method refactoring opportunities and remove the Feature Envy bad smell from source code. The proposed approach analyzes both structural and conceptual relationships between methods and uses Relational Topic Models (RTM) to identify sets of methods that share several responsibilities, i.e., \"friend methods\". The analysis of method friendships of a given method can be used to pinpoint the target class (envied class) where the method should be moved in. The results of a preliminary empirical evaluation indicate that the proposed approach provides accurate and meaningful refactoring opportunities."
},
{
  "Title": "Identifying Opaque Behavioural Changes",
  "Type": "Demonstrations Track",
  "Key": "identifying-opaque-behavioural-changes",
  "Authors": ["Reid Holmes", "David Notkin"],
  "Affiliations": ["University of Waterloo, Canada", "University of Washington, USA"],
  "Abstract": "Developers modify their systems by changing source code, updating test suites, and altering their system's execution context. When they make these modi cations, they have an understanding of the behavioural changes they expect to happen when the system is executed; when the system does not conform to their expectations, developers try to ensure their modi cation did not introduce some unexpected or undesirable behavioural change. We present an approach that integrates with existing continuous integration systems to help developers identify situations whereby their changes may have introduced unexpected behavioural consequences. In this research demonstration, we show how our approach can help developers identify and investigate unanticipated behavioural changes."
},
{
  "Title": "Identifying Program, Test, and Environmental Changes That Affect Behaviour",
  "Type": "Technical/Research Track",
  "Key": "identifying-program-test-and-environmental-changes-affect-behaviour",
  "Authors": ["Reid Holmes", "David Notkin"],
  "Affiliations": ["University of Waterloo, Canada", "University of Washington, USA"],
  "Abstract": "Developers evolve a software system by changing the program source code, by modifying its context by updating libraries or changing its configuration, and by improving its test suite. Any of these changes can cause differences in program behaviour. In general, program paths may appear or disappear between subsequent executions of a system. Some of these behavioural differences are expected by a developer; for example, executing new program paths is often precisely what is intended when adding a new test. Other behavioural differences may or may not be benign. For example, changing an XML configuration file may cause a previously-executed path to disappear, which may or may not be expected and could be problematic. Furthermore, the degree to which a behavioural change might be problematic may only become apparent over time as the new behaviour interacts with other changes.\n\nWe present an approach to identify specific program call dependencies where the programmer's changes to the program source code, its tests, or its environment are not apparent in the system's behaviour, or vice versa. Using a static and a dynamic call graph from each of two program versions, we partition dependencies based on their presence in each of the four graphs. Particular partitions contain dependencies that help a programmer develop insights about often subtle behavioural changes."
},
{
  "Title": "The Impact of Fault Models on Software Robustness Evaluations",
  "Type": "Technical/Research Track",
  "Key": "impact-fault-models-software-robustness-evaluations",
  "Authors": ["Stefan Winter", "Constantin Sârbu", "Neeraj Suri", "Brendan Murphy"],
  "Affiliations": ["TU Darmstadt, Germany", "Microsoft Research, UK"],
  "Abstract": "Following the design and in-lab testing of software, the evaluation of its resilience to actual operational perturbations in the field is a key validation need. Software-implemented fault injection (SWIFI) is a widely used approach for evaluating the robustness of software components. Recent research indicates that the selection of the applied fault model has considerable influence on the results of SWIFI-based evaluations, thereby raising the question how to select appropriate fault models (i.e. that provide justified robustness evidence).\n\nThis paper proposes several metrics for comparatively evaluating fault models's abilities to reveal robustness vulnerabilities. It demonstrates their application in the context of OS device drivers by investigating the influence (and relative utility) of four commonly used fault models, i.e. bit flips (in function parameters and in binaries), data type dependent parameter corruptions, and parameter fuzzing. We assess the efficiency of these models at detecting robustness vulnerabilities during the SWIFI evaluation of a real embedded operating system kernel and discuss application guidelines for our metrics alongside."
},
{
  "Title": "Impact of Process Simulation on Software Practice: An Initial Report",
  "Type": "Impact Project Focus Area",
  "Key": "impact-process-simulation-software-practice-initial-report",
  "Authors": ["He Zhang", "Ross Jeffery", "Dan Houston", "Liguo Huang", "Liming Zhu"],
  "Affiliations": ["National ICT, Australia", "The Aerospace Corporation, USA", "Southern Methodist University, USA"],
  "Abstract": "Process simulation has become a powerful technology in support of software project management and process improvement over the past decades. This research, inspired by the Impact Project, intends to investigate the technology transfer of software process simulation to the use in industrial settings, and further identify the best practices to release its full potential in software practice. We collected the reported applications of process simulation in software industry, and identiﬁed its wide adoption in the organizations delivering various software intensive systems. This paper, as an initial report of the research, briefs a historical perspective of the impact upon practice based on the documented evidence, and also elaborates the research-practice transition by examining one detailed case study. It is shown that research has a signiﬁcant impact on practice in this area. The analysis of impact trace also reveals that the success of software process simulation in practice highly relies on the association with other software process techniques or practices and the close collaboration between researchers and practitioners."
},
{
  "Title": "Impact of Software Resource Estimation Research on Practice: Achievements, Synergies, and Challenges",
  "Type": "Impact Project Focus Area",
  "Key": "impact-software-resource-estimation-research-practice-achievements-synergies-and-challenges",
  "Authors": ["Barry Boehm", "Ricardo Valerdi"],
  "Affiliations": ["USC, USA", "MIT, USA"],
  "Abstract": "This paper is a contribution to the Impact Project in the area of software resource estimation. The objective of the Impact Project has been to analyze the impact of software engineering research investments on software engineering practice. The paper begins by summarizing the motivation and context for analyzing software resource estimation; and by summarizing the study’s purpose, scope, and approach. The approach includes analyses of the literature; interviews of leading software resource estimation researchers, practitioners, and users; and value/impact surveys of estimators and users. The study concludes that research in software resource estimation has had a significant impact on the practice of software engineering, but also faces significant challenges in addressing likely future software trends."
},
{
  "Title": "Improving Open Source Software Patch Contribution Process: Methods and Tools",
  "Type": "Paper",
  "Key": "improving-open-source-software-patch-contribution-process-methods-and-tools",
  "Authors": ["Bhuricha Sethanandha"],
  "Affiliations": ["Portland State University, USA"],
  "Abstract": "The patch contribution process (PCP) is very important to the sustainability of OSS projects. Nevertheless, there are several issues on patch contribution in mature OSS projects, which include time consuming process, lost and ignored patches, slow review process. These issues are recognized by researchers and OSS projects, but have not been addressed. In this dissertation, I apply Kanban method to guide process improvement and tools development to reduce PCP cycle time."
},
{
  "Title": "Improving Requirements Quality using Essential Use Case Interaction Patterns",
  "Type": "Technical/Research Track",
  "Key": "improving-requirements-quality-using-essential-use-case-interaction-patterns",
  "Authors": ["Massila Kamalrudin", "John Hosking", "John Grundy"],
  "Affiliations": ["University of Auckland, New Zealand", "Swinburne University of Technology at Hawthorn, Australia"],
  "Abstract": "Requirements specifications need to be checked against the 3C’s - Consistency, Completeness and Correctness -- in order to achieve high quality. This is especially difficult when working with both natural language requirements and associated semi-formal modeling representations. We describe a technique and support tool that allows us to perform semi-automated checking of natural language and semi-formal requirements models, supporting both consistency management between representations but also correctness and completeness analysis. We use a concept of essential use case interaction patterns to perform the correctness and completeness analysis on the semi-formal representation. We highlight potential inconsistencies, incompleteness and incorrectness using visual differencing in our support tool. We have evaluated our approach via an end user study which focused on the tool’s usefulness, ease of use, ease of learning and user satisfaction and provided data for cognitive dimensions of notations analysis of the tool."
},
{
  "Title": "Inconsistency Management Framework for Model-Based Development",
  "Type": "Paper",
  "Key": "inconsistency-management-framework-model-based-development",
  "Authors": ["Alexander Reder"],
  "Affiliations": ["Johannes Kepler University, Austria"],
  "Abstract": "In contrast to programming environments, model-based development tools lack in an eﬃcient support in detecting and repairing design errors. However, inconsistencies must be resolved eventually and the detection of inconsistencies is of little use if it is not known how to use this information. Quite many approaches exist for detecting inconsistencies. Only some of them provide solutions for repairing individual inconsistencies but none of them are able to investigate the repair problem comprehensively – in particular considering the side eﬀects that might occur when applying a repair. My PhD thesis focuses on resolving inconsistencies, the different strategies one can follow and how to deal with the critical problem of side eﬀects. My approach is based on an incremental approach for detecting inconsistencies and combines runtime analysis of the design rules’ evaluation behavior with static analysis on the design rules’ structure. The main contribution is an eﬃcient and eﬀective inconsistency management framework that can be applied generically to (most) modeling and design rule languages."
},
{
  "Title": "Inconsistent Path Detection for XML IDEs",
  "Type": "Demonstrations Track",
  "Key": "inconsistent-path-detection-xml-ides",
  "Authors": ["Pierre Genevès", "Nabil Layaida"],
  "Affiliations": ["CNRS, France", "INRIA, France"],
  "Abstract": "We present the ﬁrst IDE augmented with static detection of inconsistent paths for simplifying the development and debugging of any application involving XPath expressions."
},
{
  "Title": "An Industrial Case Study on Quality Impact Prediction for Evolving Service-Oriented Software",
  "Type": "Software Engineering in Practice Track",
  "Key": "industrial-case-study-quality-impact-prediction-evolving-service-oriented-software",
  "Authors": ["Heiko Koziolek", "Bastian Schlich", "Carlos Bilich", "Roland Weiss", "Steffen Becker", "Klaus Krogmann", "Mircea Trifu", "Raffaela Mirandola", "Anne Koziolek"],
  "Affiliations": ["ABB, Germany", "University of Paderborn, Germany", "Forschungszentrum Informatik (FZI), Germany", "Politecnico di Milano, Italy", "Karlsruhe Institute of Technology (KIT), Germany"],
  "Abstract": "Systematic decision support for architectural design decisions is a major concern for software architects of evolving service-oriented systems. In practice, architects often analyse the expected performance and reliability of design alternatives based on prototypes or former experience. Modeldriven prediction methods claim to uncover the tradeoﬀs between diﬀerent alternatives quantitatively while being more cost-eﬀective and less error-prone. However, they often suffer from weak tool support and focus on single quality attributes. Furthermore, there is limited evidence on their eﬀectiveness based on documented industrial case studies. Thus, we have applied a novel, model-driven prediction method called Q-ImPrESS on a large-scale process control system consisting of several million lines of code from the automation domain to evaluate its evolution scenarios. This paper reports our experiences with the method and lessons learned. Beneﬁts of Q-ImPrESS are the good architectural decision support and comprehensive tool framework, while one drawback is the time-consuming data collection."
},
{
  "Title": "Inference of Field Initialization",
  "Type": "Technical/Research Track",
  "Key": "inference-field-initialization",
  "Authors": ["Fausto Spoto", "Michael D. Ernst"],
  "Affiliations": ["Università di Verona, Italy", "University of Washington, USA"],
  "Abstract": "A raw object is partially initialized, with only some fields set to legal values. It may violate its object invariants, such as that a given field is non-null. Programs often manipulate partially-initialized objects, but they must do so with care. Furthermore, analyses must be aware of field initialization: proving the absence of null pointer dereferences or of division by zero, or proving that object invariants are satisfied, requires initialization information.\n\nWe present a static analysis that infers a safe over-approximation of the program elements that might hold raw objects. We have proved the analysis sound and implemented it in a tool called Julia. We have evaluated Julia on over 160K lines of code. We compared its output to manually-written initialization and nullness information, and to an independently-written type-checking tool that checks initialization and nullness. Julia's output is accurate and, we believe, useful both to programmers and to static analyses."
},
{
  "Title": "Inferring Better Contracts",
  "Type": "Technical/Research Track",
  "Key": "inferring-better-contracts",
  "Authors": ["Yi Wei", "Carlo A. Furia", "Nikolay Kazmin", "Bertrand Meyer"],
  "Affiliations": ["ETH Zurich, Switzerland"],
  "Abstract": "Considerable progress has been made towards automatic support for one of the principal techniques available to enhance program reliability: equipping programs with extensive contracts. The results of current contract inference tools are still often unsatisfactory in practice, especially for programmers who already apply some kind of basic Design by Contract discipline, since the inferred contracts tend to be simple assertions—the very ones that programmers ﬁnd easy to write. We present new, completely automatic inference techniques and a supporting tool, which take advantage of the presence of simple programmer-written contracts in the code to infer sophisticated assertions, involving for example implication and universal quantiﬁcation. Applied to a production library of classes covering standard data structures such as linked lists, arrays, stacks, queues and hash tables, the tool is able, entirely automatically, to infer 75% of the complete contracts—contracts yielding the full formal speciﬁcation of the classes—with very few redundant or irrelevant clauses."
},
{
  "Title": "Information Foraging as a Foundation for Code Navigation",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "information-foraging-foundation-code-navigation",
  "Authors": ["Nan Niu", "Anas Mahmoud", "Gary Bradshaw"],
  "Affiliations": ["Mississippi State University, USA"],
  "Abstract": "A major software engineering challenge is to understand the fundamental mechanisms that underlie the developer’s code navigation behavior. We propose a novel and unified theory based on the premise that we can study developer’s information seeking strategies in light of the foraging principles that evolved to help our animal ancestors to find food. Our preliminary study on code navigation graphs suggests that the tenets of information foraging provide valuable insight into software maintenance. Our research opens the avenue towards the development of ecologically valid tool support to augment developers’ code search skills."
},
{
  "Title": "Interface Decomposition for Service Compositions",
  "Type": "Technical/Research Track",
  "Key": "interface-decomposition-service-compositions",
  "Authors": ["Domenico Bianculli", "Dimitra Giannakopoulou", "Corina S. Păsăreanu"],
  "Affiliations": ["University of Lugano, Switzerland", "NASA Ames Research Center and Carnegie Mellon Silicon Valley, USA"],
  "Abstract": "Service-based applications can be realized by composing existing services into new, added-value composite services. The external services with which a service composition interacts are usually known by means of their syntactical interface. However, an interface providing more information, such as a behavioral speciﬁcation, could be more useful to a service integrator for assessing that a certain external service can contribute to fulﬁll the functional requirements of the composite application.\n\nGiven the requirements speciﬁcation of a composite service, we present a technique for obtaining the behavioral interfaces — in the form of labeled transition systems — of the external services, by decomposing the global interface speciﬁcation that characterizes the environment of the service composition. The generated interfaces guarantee that the service composition fulﬁlls its requirements during the execution. Our approach has been implemented in the LTSA tool and has been applied to two case studies."
},
{
  "Title": "Iterative Context-Aware Feature Location",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "iterative-context-aware-feature-location",
  "Authors": ["Xin Peng", "Zhenchang Xing", "Xi Tan", "Yijun Yu", "Wenyun Zhao"],
  "Affiliations": ["Fudan University, China", "National University of Singapore, Singapore", "The Open University, UK"],
  "Abstract": "Locating the program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyze each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input, and they tend to miss the nonlocal interactions among features. In this paper, we propose to address the proceeding two issues in feature location using an iterative context-aware approach. The underlying intuition is that the features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: 1) it takes into account the structural similarity between a feature and a program element to determine their relevance; 2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighboring features and program elements. Our initial evaluation suggests the proposed approach is more robust and can significantly increase the recall of feature location with a slight decrease in precision."
},
{
  "Title": "JavAdaptor: Unrestricted Dynamic Software Updates for Java",
  "Type": "Demonstrations Track",
  "Key": "javadaptor-unrestricted-dynamic-software-updates-java",
  "Authors": ["Mario Pukall", "Alexander Grebhahn", "Reimar Schröter", "Christian Kästner", "Walter Cazzola", "Sebastian Götz"],
  "Affiliations": ["University of Magdeburg, Germany", "Philipps-University Marburg, Germany", "University of Milano, Italy", "University of Dresden, Germany"],
  "Abstract": "Dynamic software updates (DSU) are one of the top-most features requested by developers and users. As a result, DSU is already standard in many dynamic programming languages. But, it is not standard in statically typed languages such as Java. Even if at place number three of Oracle’s current request for enhancement (RFE) list, DSU support in Java is very limited. Therefore, over the years many diﬀerent DSU approaches for Java have been proposed. Nevertheless, DSU for Java is still an active ﬁeld of research, because most of the existing approaches are too restrictive. Some of the approaches have shortcomings either in terms of ﬂexibility or performance, whereas others are platform dependent or dictate the program’s architecture. With JavAdaptor, we present the ﬁrst DSU approach which comes without those restrictions. We will demonstrate JavAdaptor based on the well-known arcade game Snake which we will update stepwise at runtime."
},
{
  "Title": "JDeodorant: Identification and Application of Extract Class Refactorings",
  "Type": "Demonstrations Track",
  "Key": "jdeodorant-identification-and-application-extract-class-refactorings",
  "Authors": ["Marios Fokaefs", "Nikolaos Tsantalis", "Eleni Stroulia", "Alexander Chatzigeorgiou"],
  "Affiliations": ["University of Alberta, Canada", "University of Macedonia, Greece"],
  "Abstract": "Evolutionary changes in object-oriented systems can result in large, complex classes, known as “God Classes”. In this paper, we present a tool, developed as part of the JDeodorant Eclipse plugin, that can recognize opportunities for extracting cohesive classes from “God Classes” and automatically apply the refactoring chosen by the developer."
},
{
  "Title": "The Lazy Initialization Multilayered Modeling Framework",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "lazy-initialization-multilayered-modeling-framework",
  "Authors": ["Fahad R. Golra", "Fabien Dagnat"],
  "Affiliations": ["Université Européenne de Bretagne and Telecom Bretagne, France"],
  "Abstract": "Lazy Initialization Multilayer Modeling (LIMM) is an object oriented modeling language targeted to the declarative defi nition of Domain Speci c Languages (DSLs) for Model Driven Engineering. It focuses on the precise defi nition of modeling frameworks spanning over multiple layers. In particular, it follows a two dimensional architecture instead of the linear architecture followed by many other modeling frameworks. The novelty of our approach is to use lazy initialization for the defi nition of mapping between diff erent modeling abstractions, within and across multiple layers, hence providing the basis for exploiting the potential of metamodeling."
},
{
  "Title": "Learning to Adapt Requirements Specifications of Evolving Systems",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "learning-adapt-requirements-specifications-evolving-systems",
  "Authors": ["Rafael V. Borges", "Artur D'Avila Garcez", "Luis C. Lamb", "Bashar Nuseibeh"],
  "Affiliations": ["City University London, UK", "UFRGS, Brazil", "The Open University, UK and Lero, Ireland"],
  "Abstract": "We propose a novel framework for adapting and evolving software requirements models. The framework uses model checking and machine learning techniques for verifying properties and evolving model descriptions. The paper offers two novel contributions and a preliminary evaluation and application of the ideas presented. First, the framework is capable of coping with errors in the specification process so that performance degrades gracefully. Second, the framework can also be used to re-engineer a model from examples only, when an initial model is not available. We provide a preliminary evaluation of our framework by applying it to a Pump System case study, and integrate our prototype tool with the NuSMV model checker. We show how the tool integrates verification and evolution of abstract models, and also how it is capable of re-engineering partial models given examples from an existing system."
},
{
  "Title": "Leveraging Software Architectures to Guide and Verify the Development of Sense/Compute/Control Applications",
  "Type": "Technical/Research Track",
  "Key": "leveraging-software-architectures-guide-and-verify-development-sensecomputecontrol-applicati",
  "Authors": ["Damien Cassou", "Emilie Balland", "Charles Consel", "Julia Lawall"],
  "Affiliations": ["University of Bordeaux / INRIA, France", "DIKU/INRIA/LIP6, France"],
  "Abstract": "A software architecture describes the structure of a computing system by specifying software components and their interactions. Mapping a software architecture to an implementation is a well known challenge. A key element of this mapping is the architecture's description of the data and control-flow interactions between components. The characterization of these interactions can be rather abstract or very concrete, providing more or less implementation guidance, programming support, and static verification.\n\nIn this paper, we explore one point in the design space between abstract and concrete component interaction specifications. We introduce a notion of behavioral contract that expresses the set of allowed interactions between components, describing both data and control-flow constraints. This declaration is part of the architecture description, allows generation of extensive programming support, and enables various verifications. We instantiate our approach in an architecture description language for the domain of Sense/Compute/Control applications, and describe associated compilation and verification strategies."
},
{
  "Title": "A Lightweight Code Analysis and its Role in Evaluation of a Dependability Case",
  "Type": "Technical/Research Track",
  "Key": "lightweight-code-analysis-and-its-role-evaluation-dependability-case",
  "Authors": ["Joseph P. Near", "Aleksandar Milicevic", "Eunsuk Kang", "Daniel Jackson"],
  "Affiliations": ["Massachusetts Institute of Technology, USA"],
  "Abstract": "A dependability case is an explicit, end-to-end argument, based on concrete evidence, that a system satisfies a critical property. We report on a case study constructing a dependability case for the control software of a complex medical device. The key novelty of the approach used is a lightweight code analysis that generates a list of side conditions, corresponding to assumptions to be discharged (about the code and the environment in which it executes). This represents an unconventional trade-off between, at one extreme, more ambitious analyses that attempt to discharge all conditions automatically (but which cannot even in principle handle environmental assumptions), and at the other, flow- or context-insensitive analyses that require more user involvement. The results of the analysis suggested a variety of ways in which dependability might be improved."
},
{
  "Title": "LIME: A Framework for Debugging Load Imbalance in Multi-threaded Execution",
  "Type": "Technical/Research Track",
  "Key": "lime-framework-debugging-load-imbalance-multi-threaded-execution",
  "Authors": ["Jungju Oh", "Christopher J. Hughes", "Guru Venkataramani", "Milos Prvulovic"],
  "Affiliations": ["Georgia Institute of Technology, USA", "Intel Corporation, USA", "George Washington University, USA"],
  "Abstract": "With the ubiquity of multi-core processors, software must make effective use of multiple cores to obtain good performance on modern hardware. One of the biggest roadblocks to this is load imbalance, or the uneven distribution of work across cores. We propose LIME, a framework for analyzing parallel programs and reporting the cause of load imbalance in application source code. This framework uses statistical techniques to pinpoint load imbalance problems stemming from both control flow issues (e.g., unequal iteration counts) and interactions between the application and hardware (e.g., unequal cache miss rates). We evaluate LIME on applications from widely used parallel benchmark suites, and show that LIME accurately reports the causes of load imbalance, their nature and origin in the code, and their relative importance."
},
{
  "Title": "Matching Logic: A New Program Verification Approach",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "matching-logic-new-program-verification-approach",
  "Authors": ["Grigore Roşu", "Andrei Ştefănescu"],
  "Affiliations": ["University of Illinois at Urbana-Champaign, USA"],
  "Abstract": "Matching logic is a new program verification logic, which builds upon operational semantics. Matching logic specifications are constrained symbolic program configurations, called patterns, which can be matched by concrete configurations. By building upon an operational semantics of the language and allowing specifications to directly refer to the structure of the configuration, matching logic has at least three benefits: (1) One's familiarity with the formalism reduces to one's familiarity with the operational semantics of the language, that is, with the language itself; (2) The verification process proceeds the same way as the program execution, making debugging failed proof attempts manageable because one can always see the \"current configuration\" and \"what went wrong\", same like in a debugger; and (3) Nothing is lost in translation, that is, there is no gap between the language itself and its verifier. Moreover, direct access to the structure of the configuration facilitates defining sub-patterns that one may reason about, such as disjoint lists or trees in the heap, as well as supporting framing in various components of the configuration at no additional costs."
},
{
  "Title": "Measuring Subversions: Security and Legal Risk in Reused Software Artifacts",
  "Type": "Paper",
  "Key": "measuring-subversions-security-and-legal-risk-reused-software-artifacts",
  "Authors": ["Julius Davies"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "A software system often includes a set of library dependencies and other software artifacts necessary for the system's proper operation. However, long-term maintenance problems related to reused software can gradually emerge over the lifetime of the deployed system. In our exploratory study we propose a manual technique to locate documented security and legal problems in a set of reused software artifacts. We evaluate our technique with a case study of 81 Java libraries found in a proprietary e-commerce web application. Using our approach we discovered both a potential legal problem with one library, and a second library that was affected by a known security vulnerability. These results support our larger thesis: software reuse entails long-term maintenance costs. In future work we strive to develop automated techniques by which developers, managers, and other software stakeholders can measure, address, and minimize these costs over the lifetimes of their software assets."
},
{
  "Title": "MeCC: Memory Comparison-based Clone Detector",
  "Type": "Technical/Research Track",
  "Key": "mecc-memory-comparison-based-clone-detector",
  "Authors": ["Heejung Kim", "Yungbum Jung", "Sunghun Kim", "Kwankeun Yi"],
  "Affiliations": ["Seoul National University, South Korea", "Hong Kong University of Science and Technology, China"],
  "Abstract": "In this paper, we propose a new semantic clone detection technique by comparing programs' abstract memory states, which are computed by a semantic-based static analyzer. Our experimental study using three large-scale open source projects shows that our technique can detect semantic clones that existing syntactic- or semantic-based clone detectors miss. Our technique can help developers identify inconsistent clone changes, and refactoring candidates, and understand software evolution related to semantic clones."
},
{
  "Title": "Mental Models and Parallel Program Maintenance",
  "Type": "Paper",
  "Key": "mental-models-and-parallel-program-maintenance",
  "Authors": ["Caitlin Sadowski"],
  "Affiliations": ["UC Santa Cruz, USA"],
  "Abstract": "Parallel programs are diﬃcult to write, test, and debug. This thesis explores how programmers build mental models about parallel programs, and demonstrates, through user evaluations, that maintenance activities can be improved by incorporating theories based on such models. By doing so, this work aims to increase the reliability and performance of today’s information technology infrastructure by improving the practice of maintaining and testing parallel software."
},
{
  "Title": "A Method for Selecting SOA Pilot Projects Including a Pilot Metrics Framework",
  "Type": "Software Engineering in Practice Track",
  "Key": "method-selecting-soa-pilot-projects-including-pilot-metrics-framework",
  "Authors": ["Liam O'Brien", "James Gibson", "Jon Gray"],
  "Affiliations": ["CSIRO, Australia", "NICTA, Australia", "ANU, Australia"],
  "Abstract": "Many organizations are introducing Service Oriented Architecture (SOA) as part of their business transformation projects to take advantage of the proposed benefits associated with using SOA. However, in many cases organizations don’t necessarily know on which projects introducing SOA would be of value and show real benefits to the organization. In this paper we outline a method and pilot metrics framework (PMF) to help organization’s select from a set of candidate projects those which would be most suitable for piloting SOA. The PMF is used as part of a method based on identifying a set of benefit and risk criteria, investigating each of the candidate projects, mapping them to the criteria and then selecting the most suitable project(s). The paper outlines a case study where the PMF was applied in a large government organization to help them select pilot projects and develop an overall strategy for introducing SOA into their organization."
},
{
  "Title": "Miler: A Toolset for Exploring Email Data",
  "Type": "Demonstrations Track",
  "Key": "miler-toolset-exploring-email-data",
  "Authors": ["Alberto Bacchelli", "Michele Lanza", "Marco D'Ambros"],
  "Affiliations": ["University of Lugano, Switzerland"],
  "Abstract": "Source code is the target and ﬁnal outcome of software development. By focusing our research and analysis on source code only, we risk forgetting that software is the product of human eﬀorts, where communication plays a pivotal role. One of the most used communications means are emails, which have become vital for any distributed development project. Analyzing email archives is non-trivial, due to the noisy and unstructured nature of emails, the vast amounts of information, the unstandardized storage systems, and the gap with development tools. We present Miler, a toolset that allows the exploration of this form of communication, in the context of software maintenance and evolution. With Miler we can retrieve data from mailing list repositories in diﬀerent formats, model emails as ﬁrst-class entities, and transparently store them in databases. Miler oﬀers tools and support for navigating the content, manually labelling emails with discussed source code entities, automatically linking emails to source code, measuring code entities’ popularity in mailing lists, exposing structured content in the unstructured content, and integrating email communication in an IDE."
},
{
  "Title": "Mining Message Sequence Graphs",
  "Type": "Technical/Research Track",
  "Key": "mining-message-sequence-graphs",
  "Authors": ["Sandeep Kumar", "Siau Cheng Khoo", "Abhik Roychoudhury", "David Lo"],
  "Affiliations": ["National University of Singapore, Singapore", "Singapore Management University, Singapore"],
  "Abstract": "Dynamic specification mining involves discovering software behavior from traces for the purpose of program comprehension and bug detection. However, in concurrent/distributed programs, the inherent partial order relationships among events occurring across processes pose a big challenge to specification mining. In this paper, we propose a framework for mining partial orders so as to understand concurrent program behavior. Our miner takes in a set of concurrent program traces, and produces a message sequence graph (MSG) to represent the concurrent program behavior. An MSG represents a graph where the nodes of the graph are partial orders, represented as Message Sequence Charts. Mining an MSG allows us to understand concurrent behaviors since the nodes of the MSG depict important ``phases\" or ``interaction snippets\" involving several concurrently executing processes. Experiments on mining behaviors of several fairly complex distributed systems show that our miner can produce the corresponding MSGs with both high precision and recall."
},
{
  "Title": "Mining Parametric Specifications",
  "Type": "Technical/Research Track",
  "Key": "mining-parametric-specifications",
  "Authors": ["Choonghwan Lee", "Feng Chen", "Grigore Roşu"],
  "Affiliations": ["University of Illinois at Urbana-Champaign, USA"],
  "Abstract": "Specifications carrying formal parameters that are bound to concrete data at runtime can effectively and elegantly capture multi-object behaviors or protocols. Unfortunately, parametric specifications are not easy to formulate by non-experts and, consequently, are rarely available. This paper presents a general approach for mining parametric specifications from program executions, based on a strict separation of concerns: (1) a trace slicer first extracts sets of independent interactions from parametric execution traces; and (2) the resulting non-parametric trace slices are then passed to any conventional non-parametric property learner. The presented technique has been implemented in jMiner, which has been used to automatically mine many meaningful and non-trivial parametric properties of OpenJDK 6."
},
{
  "Title": "Mining Service Abstractions",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "mining-service-abstractions",
  "Authors": ["Dionysis Athanasopoulos", "Apostolos V. Zarras", "Panos Vassiliadis", "Valerie Issarny"],
  "Affiliations": ["University of Ioannina, Greece", "INRIA, France"],
  "Abstract": "Several lines of research rely on the concept of service abstractions to enable the organization, the composition and the adaptation of services. However, what is still missing, is a systematic approach for extracting service abstractions out of the vast amount of services that are available all over the Web. To deal with this issue, we propose an approach for mining service abstractions, based on an agglomerative clustering algorithm. Our experimental ﬁndings suggest that the approach is promising and can serve as a basis for future research."
},
{
  "Title": "Mining Software Repositories Using Topic Models",
  "Type": "Paper",
  "Key": "mining-software-repositories-using-topic-models",
  "Authors": ["Stephen W. Thomas"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "Software repositories, such as source code, email archives, and bug databases, contain unstructured and unlabeled text that is discult to analyze with traditional techniques. We propose the use of statistical topic models to automatically discover structure in these textual repositories. This discovered structure has the potential to be used in software engineering tasks, such as bug prediction and traceability link recovery. Our research goal is to address the challenges of applying topic models to software repositories."
},
{
  "Title": "Model-based Performance Testing",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "model-based-performance-testing",
  "Authors": ["Cornel Barna", "Marin Litoiu", "Hamoun Ghanbari"],
  "Affiliations": ["York University, Canada"],
  "Abstract": "In this paper, we present a method for performance testing of transactional systems. The methods models the system under test, ﬁnds the software and hardware bottlenecks and generate the workloads that saturate them. The framework is adaptive, the model and workloads are determined during the performance test execution by measuring the system performance, ﬁtting a performance model and by analytically computing the number and mix of users that will saturate the bottlenecks. We model the software system using a two layers queuing model and use analytical techniques to ﬁnd the workload mixes that change the bottlenecks in the system. Those workload mixes become stress vectors and initial starting points for the stress test cases. The rest of test cases are generated based on a feedback loop that drives the software system towards the worst case behaviour."
},
{
  "Title": "Model-Driven Engineering Practices in Industry",
  "Type": "Software Engineering in Practice Track",
  "Key": "model-driven-engineering-practices-industry",
  "Authors": ["John Hutchinson", "Mark Rouncefield", "Jon Whittle"],
  "Affiliations": ["Lancaster University, UK"],
  "Abstract": "In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some ‘lessons learned’, in particular the importance of complex organizational, managerial and social factors – as opposed to simple technical factors – in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus."
},
{
  "Title": "Model Projection: Simplifying Models in Response to Restricting the Environment",
  "Type": "Technical/Research Track",
  "Key": "model-projection-simplifying-models-response-restricting-environment",
  "Authors": ["Kelly Androutsopoulos", "David Binkley", "David Clark", "Nicolas Gold", "Mark Harman", "Kevin Lano", "Zheng Li"],
  "Affiliations": ["University College London, UK", "Loyola University Maryland, USA", "King's College London, UK"],
  "Abstract": "This paper introduces Model Projection. Finite state models such as Extended Finite State Machines are being used in an ever increasing number of software engineering activities. Model projection facilitates model development by specializing models for a speciﬁc operating environment. A projection is useful in many design-level applications including speciﬁcation reuse and property veriﬁcation.\n\nThe applicability of model projection rests upon three critical concerns: correctness, effectiveness, and efﬁciency, all of which are addressed in this paper. We introduce four related algorithms for model projection and prove each correct. We also present an empirical study of effectiveness and efﬁciency using ten models, including widely–studied benchmarks as well as industrial models. Results show that a typical projection includes about half of the states and a third of the transitions from the original model."
},
{
  "Title": "MT-Scribe: An End-User Approach to Automate Software Model Evolution",
  "Type": "Demonstrations Track",
  "Key": "mt-scribe-end-user-approach-automate-software-model-evolution",
  "Authors": ["Yu Sun", "Jeff Gray", "Jules White"],
  "Affiliations": ["University of Alabama at Birmingham, USA", "University of Alabama, USA", "Virginia Tech, USA"],
  "Abstract": "Model evolution is an essential activity in software system modeling, which is traditionally supported by manual editing or writing model transformation rules. However, the current state of practice for model evolution presents challenges to those who are unfamiliar with model transformation languages or metamodel definitions. This demonstration presents a demonstration-based approach that assists end-users through automation of model evolution tasks (e.g., refactoring, model scaling, and aspect weaving)."
},
{
  "Title": "Multifractal Aspects of Software Development",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "multifractal-aspects-software-development",
  "Authors": ["Abram Hindle", "Michael W. Godfrey", "Richard C. Holt"],
  "Affiliations": ["UC Davis, USA", "University of Waterloo, Canada"],
  "Abstract": "Software development is difficult to model, particularly the noisy, non-stationary signals of changes per time unit, extracted from version control systems (VCSs). Currently researchers are utilizing timeseries analysis tools such as ARIMA to model these signals extracted from a project's VCS. Unfortunately current approaches are not very amenable to the underlying power-law distributions of this kind of signal. We propose modeling changes per time unit using multifractal analysis. This analysis can be used when a signal exhibits multiscale self-similarity, as in the case of complex data drawn from power-law distributions. Specifically we utilize multifractal analysis to demonstrate that software development is multifractal, that is the signal is a fractal composed of multiple fractal dimensions along a range of Hurst exponents. Thus we show that software development has multi-scale self-similarity, that software development is multifractal. We also pose questions that we hope multifractal analysis can answer."
},
{
  "Title": "Non-Essential Changes in Version Histories",
  "Type": "Technical/Research Track",
  "Key": "non-essential-changes-version-histories",
  "Authors": ["David Kawrykow", "Martin P. Robillard"],
  "Affiliations": ["McGill University, Canada"],
  "Abstract": "Numerous techniques involve mining change data captured in software archives to assist engineering efforts, for example to identify components that tend to evolve together. We observed that important changes to software artifacts are sometimes accompanied by numerous non-essential modiﬁcations, such as local variable refactorings, or textual differences induced as part of a rename refactoring. We developed a tool-supported technique for detecting nonessential code differences in the revision histories of software systems. We used our technique to investigate code changes in over 24 000 change sets gathered from the change histories of seven long-lived open-source systems. We found that up to 15.5% of a system’s method updates were due solely to non-essential differences. We also report on numerous observations on the distribution of non-essential differences in change history and their potential impact on change-based analyses."
},
{
  "Title": "Ownership, Experience and Defects: A Fine-Grained Study of Authorship",
  "Type": "Technical/Research Track",
  "Key": "ownership-experience-and-defects-fine-grained-study-authorship",
  "Authors": ["Foyzur Rahman", "Premkumar Devanbu"],
  "Affiliations": ["UC Davis, USA"],
  "Abstract": "Recent research indicates that “people” factors such as ownership, experience, organizational structure, and geographic distribution have a big impact on software quality. Understanding these factors, and properly deploying people resources can help managers improve quality outcomes. This paper considers the impact of code ownership and developer experience on software quality. In a large project, a ﬁle might be entirely owned by a single developer, or worked on by many. Some previous research indicates that more developers working on a ﬁle might lead to more defects. Prior research considered this phenomenon at the level of modules or ﬁles, and thus does not tease apart and study the eﬀect of contributions of diﬀerent developers to each module or ﬁle. We exploit a modern version control system to examine this issue at a ﬁne-grained level. Using version history, we examine contributions to code fragments that are actually repaired to ﬁx bugs. Are these code fragments “implicated” in bugs the result of contributions from many? or from one? Does experience matter? What type of experience? We ﬁnd that implicated code is more strongly associated with a single developer’s contribution; our ﬁndings also indicate that an author’s specialized experience in the target ﬁle is more important than general experience. Our ﬁndings suggest that quality control eﬀorts could be proﬁtably targeted at changes made by single developers with limited prior experience on that ﬁle."
},
{
  "Title": "Palus: A Hybrid Automated Test Generation Tool for Java",
  "Type": "Paper",
  "Key": "palus-hybrid-automated-test-generation-tool-java",
  "Authors": ["Sai Zhang"],
  "Affiliations": ["University of Washington, USA"],
  "Abstract": "In object-oriented programs, a unit test often consists of a sequence of method calls that create and mutate objects. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible. This paper proposes a combined static and dynamic test generation approach to address these problems, for code without a formal speciﬁcation. Our approach ﬁrst uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the ﬁelds they may read or write. Finally, both the dynamicallyinferred model (which tends to be accurate but incomplete) and the statically-identiﬁed dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests. Our Palus tool implements this approach. We compared it with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on six popular open-source Java programs. Tests generated by Palus achieved 35% higher structural coverage on average. Palus is also internally used in Google, and has found 22 new bugs in four well-tested products."
},
{
  "Title": "Patching Vulnerabilities with Sanitization Synthesis",
  "Type": "Technical/Research Track",
  "Key": "patching-vulnerabilities-sanitization-synthesis",
  "Authors": ["Fang Yu", "Muath Alkhalaf", "Tevfik Bultan"],
  "Affiliations": ["National Chengchi University, Taiwan", "UC Santa Barbara, USA"],
  "Abstract": "We present automata-based static string analysis techniques that automatically generate sanitization statements for patching vulnerable web applications. Our approach consists of three phases: Given an attack pattern we ﬁrst conduct a vulnerability analysis to identify if strings that match the attack pattern can reach the security-sensitive functions. Next, we compute vulnerability signatures that characterize all input strings that can exploit the discovered vulnerability. Given the vulnerability signatures, we then construct sanitization statements that 1) check if a given input matches the vulnerability signature and 2) modify the input in a minimal way so that the modiﬁed input does not match the vulnerability signature. Our approach is capable of generating relational vulnerability signatures (and corresponding sanitization statements) for vulnerabilities that are due to more than one input."
},
{
  "Title": "Permission-Based Programming Languages",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "permission-based-programming-languages",
  "Authors": ["Jonathan Aldrich", "Ronald Garcia", "Mark Hahnenberg", "Manuel Mohr", "Karl Naden", "Darpan Saini", "Sven Stork", "Joshua Sunshine", "Éric Tanter", "Roger Wolff"],
  "Affiliations": ["Carnegie Mellon University, USA", "Karlsruhe Institute of Technology, Germany", "University of Chile, Chile"],
  "Abstract": "Linear permissions have been proposed as a lightweight way to specify how an object may be aliased, and whether those aliases allow mutation. Prior work has demonstrated the value of permissions for addressing many software engineering concerns, including information hiding, protocol checking, concurrency, security, and memory management. We propose the concept of a permission-based programming language--a language whose object model, type system, and runtime are all co-designed with permissions in mind. This approach supports an object model in which the structure of an object can change over time, a type system that tracks changing structure in addition to addressing the other concerns above, and a runtime system that can dynamically check permission assertions and leverage permissions to parallelize code. We sketch the design of the permission-based programming language Plaid, and argue that the approach may provide significant software engineering benefits."
},
{
  "Title": "Perspectives of Delegation in Team-Based Distributed Software Development over the GENI Infrastructure",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "perspectives-delegation-team-based-distributed-software-development-over-geni-infrastructure",
  "Authors": ["Pierre F. Tiako"],
  "Affiliations": ["Langston University, USA"],
  "Abstract": "Team-based distributed software development (TBDSD) is one of the single biggest challenges facing software companies. The need to manage development efforts and resources in different locations increase the complexity and cost of modern day software development. Current software development environments do not provide suitable support to delegate task among teams with appropriate directives. TBDSD is also limited to the current internet capabilities. One of the resulting problems is the difficulty to delegate and control tasks assigned among remote teams. This paper proposes (1) a new framework for delegation in TBDSD, and (2) perspectives for deploying Process-centered Software Engineering Environments (PSEE) over the Global Environment for Network Innovations (GENI) infrastructure. GENI, the “future Internet” that is taking shape in prototypes across the US, will allow, in the context of our study, to securely access and share software artifacts, resources, and tools as never before seen over the current Internet."
},
{
  "Title": "Portfolio: Finding Relevant Functions and Their Usages",
  "Type": "Technical/Research Track",
  "Key": "portfolio-finding-relevant-functions-and-their-usages",
  "Authors": ["Collin McMillan", "Mark Grechanik", "Denys Poshyvanyk", "Qing Xie", "Chen Fu"],
  "Affiliations": ["College of William and Mary, USA", "Accenture Technology Lab, USA"],
  "Abstract": "Different studies show that programmers are more interested in finding definitions of functions and their uses than variables, statements, or arbitrary code fragments. Therefore, programmers require support in finding relevant functions and determining how those functions are used. Unfortunately, existing code search engines do not provide enough of this support to developers, thus reducing the effectiveness of code reuse.\n\nWe provide this support to programmers in a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We have built Portfolio using a combination of models that address surfing behavior of programmer and sharing related concepts among functions. We conducted an experiment with 49 professional programmers to compare Portfolio to Google Code Search and Koders using a standard methodology. The results show with strong statistical significance that users find more relevant functions with higher precision with Portfolio than with Google Code Search and Koders."
},
{
  "Title": "Portfolio: A Search Engine for Finding Functions and Their Usages",
  "Type": "Demonstrations Track",
  "Key": "portfolio-search-engine-finding-functions-and-their-usages",
  "Authors": ["Collin McMillan", "Mark Grechanik", "Denys Poshyvanyk", "Qing Xie", "Chen Fu"],
  "Affiliations": ["College of William and Mary, USA", "University of Illinois at Chicago, USA", "Accenture Technology Labs, USA"],
  "Abstract": "In this demonstration, we present a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We will show how chains of relevant functions and their usages can be visualized to users in response to their queries."
},
{
  "Title": "Positive Effects of Utilizing Relationships Between Inconsistencies for more Effective Inconsistency Resolution",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "positive-effects-utilizing-relationships-between-inconsistencies-more-effective-inconsistenc",
  "Authors": ["Alexander Nöhrer", "Alexander Reder", "Alexander Egyed"],
  "Affiliations": ["Johannes Kepler University, Austria"],
  "Abstract": "State-of-the-art modeling tools can help detect inconsistencies in software models. Some can even generate fixing actions for these inconsistencies. However such approaches handle inconsistencies individually, assuming that each single inconsistency is a manifestation of an individual defect. We believe that inconsistencies are merely expressions of defects. That is, inconsistencies highlight situations under which defects are observable. However, a single defect in a software model may result in many inconsistencies and a single inconsistency may be the result of multiple defects. Inconsistencies may thus be related to other inconsistencies and we believe that during fixing, one should consider clusters of such related inconsistencies. This paper provides first evidence and emerging results that several inconsistencies can be linked to a single defect and show that with such knowledge only a subset of fixes need to be considered during inconsistency resolution."
},
{
  "Title": "Practical Change Impact Analysis Based on Static Program Slicing for Industrial Software Systems",
  "Type": "Software Engineering in Practice Track",
  "Key": "practical-change-impact-analysis-based-static-program-slicing-industrial-software-systems",
  "Authors": ["Mithun Acharya", "Brian Robinson"],
  "Affiliations": ["ABB Corporate Research, USA"],
  "Abstract": "Change impact analysis, i.e., knowing the potential consequences of a software change, is critical for the risk analysis, developer effort estimation, and regression testing of evolving software. Static program slicing is an attractive option for enabling routine change impact analysis for newly committed changesets during daily software build. For small programs with a few thousand lines of code, static program slicing scales well and can assist precise change impact analysis. However, as we demonstrate in this paper, static program slicing faces unique challenges when applied routinely on large and evolving industrial software systems. Despite recent advances in static program slicing, to our knowledge, there have been no studies of static change impact analysis applied on large and evolving industrial software systems. In this paper, we share our experiences in designing a static change impact analysis framework for such software systems. We have implemented our framework as a tool called Imp1 and have applied Imp on an industrial codebase with over a million lines of C/ C++ code with promising empirical results."
},
{
  "Title": "A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering",
  "Type": "Technical/Research Track",
  "Key": "practical-guide-using-statistical-tests-assess-randomized-algorithms-software-engineering",
  "Authors": ["Andrea Arcuri", "Lionel C. Briand"],
  "Affiliations": ["Simula Research Laboratory, Norway"],
  "Abstract": "Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering."
},
{
  "Title": "Pragmatic Prioritization of Software Quality Assurance Efforts",
  "Type": "Paper",
  "Key": "pragmatic-prioritization-software-quality-assurance-efforts",
  "Authors": ["Emad Shihab"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate ﬁndings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic."
},
{
  "Title": "Pragmatic Reuse in Web Application Development",
  "Type": "Paper",
  "Key": "pragmatic-reuse-web-application-development",
  "Authors": ["Josip Maras"],
  "Affiliations": ["University of Split, Croatia"],
  "Abstract": "Highly interactive web applications that oﬀer user experience and responsiveness of desktop applications are becoming increasingly popular. They are often composed out of visually distinctive user-interface (UI) elements that encapsulate a certain behavior – the so called UI controls. Similar controls are often used in a large number of web pages, and facilitating their reuse would oﬀer considerable beneﬁts. Unfortunately, because of a very short time-to-market, and a fast pace of technology development, preparing controls for reuse is usually not a primary concern. The focus of my research will be to circumvent this limitation by developing a method, and the accompanying tool for supporting web UI control reuse."
},
{
  "Title": "Precise Identification of Problems for Structural Test Generation",
  "Type": "Technical/Research Track",
  "Key": "precise-identification-problems-structural-test-generation",
  "Authors": ["Xusheng Xiao", "Tao Xie", "Nikolai Tillmann", "Jonathan de Halleux"],
  "Affiliations": ["North Carolina State University, USA", "Microsoft Research, USA"],
  "Abstract": "When tools built for automated test-generation approaches are applied on complex programs in practice, these tools face two main types of problems: (1) external-method-call problem (EMCP), where method calls from external libraries prevent tools achieving higher coverage; (2) object-creation problem (OCP), where tools fails to generate desirable object states. Since tools are not powerful enough, the developers can provide guidance to help tools. To reduce the efforts of developers in providing guidance to tools, we propose Covana to precisely identify and report problems by computing data dependency between problem candidates and not-covered branches or statements. To show the effectiveness of Covana, we conduct evaluations on two open source projects. Our results show that Covana effectively identifies 43 EMCP out of 1610 EMCP candidates with only 1 false positive and 2 false negative, and 155 OCP out of 451 OCP candidates with 20 false positives and 28 false negatives."
},
{
  "Title": "Predictable Dynamic Deployment of Components in Embedded Systems",
  "Type": "Paper",
  "Key": "predictable-dynamic-deployment-components-embedded-systems",
  "Authors": ["Ana Petričić"],
  "Affiliations": ["Mälardalen University, Sweden"],
  "Abstract": "Dynamic reconfiguration – the ability to hot swap a component or to introduce a new component into a system – is essential to supporting evolutionary change in long-live and highly available systems. A major issue related to this process is to ensure application consistency and performance after reconfiguration. This task is especially challenging for embedded systems which run with limited resources and have specific dependability requirements. We focus on checking resources constraints and propose for a component compliance checking to be performed during its deployment. Our main objective is preserving system integrity during and after reconfiguration by developing a resource efficient dynamic deployment mechanism that will include component validation in respect to available system resources and performance requirements."
},
{
  "Title": "Problem Identification for Structural Test Generation: First Step Towards Cooperative Developer Testing",
  "Type": "Paper",
  "Key": "problem-identification-structural-test-generation-first-step-towards-cooperative-developer-t",
  "Authors": ["Xusheng Xiao"],
  "Affiliations": ["North Carolina State University, USA"],
  "Abstract": "Achieving high structural coverage is an important goal of software testing. Instead of manually producing high-covering test inputs that achieve high structural coverage, testers or developers can employ tools built based on automated test-generation approaches to automatically generate such test inputs. Although these tools can easily generate high-covering test inputs for simple programs, when applied on complex programs in practice, these tools face various problems, such as the problems of dealing with method calls to external libraries, generating method-call sequences to produce desired object states, and exceeding defined boundaries of resources due to loops. Since these tools currently are not powerful enough to deal with these various problems in testing complex programs, we propose cooperative developer testing, where developers provide guidance to help tools achieve higher structural coverage. To reduce the efforts of developers in providing guidance to the tools, we propose a novel approach, called Covana. Covana precisely identifies and reports problems that prevent the tools from achieving high structural coverage primarily by determining whether branch statements containing not-covered branches have data dependencies on problem candidates."
},
{
  "Title": "Program Abstractions for Behaviour Validation",
  "Type": "Technical/Research Track",
  "Key": "program-abstractions-behaviour-validation",
  "Authors": ["Guido de Caso", "Víctor Braberman", "Diego Garbervetsky", "Sebastián Uchitel"],
  "Affiliations": ["Universidad de Buenos Aires, Argentina", "Imperial College London, UK"],
  "Abstract": "Code artefacts that have non-trivial requirements with respect to the ordering in which their methods or procedures ought to be called are common and appear, for instance, in the form of API implementations and objects. This work addresses the problem of validating if API implementations provide their intended behaviour when descriptions of this behaviour are informal, partial or non-existent. The proposed addresses this problem by generating abstract behaviour models which resemble typestates. These models are statically computed and encode all admissible sequences of method calls. The level of abstraction at which such models are constructed has shown to be useful for validating code artefacts and identifying findings which led to the discovery of bugs, adjustment of the requirements expected by the engineer to the requirements implicit in the code, and the improvement of available documentation."
},
{
  "Title": "Program Analysis: From Qualitative Analysis to Quantitative Analysis",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "program-analysis-qualitative-analysis-quantitative-analysis",
  "Authors": ["Sheng Liu", "Jian Zhang"],
  "Affiliations": ["Chinese Academy of Sciences, China"],
  "Abstract": "We propose to combine symbolic execution with volume computation to compute the exact execution frequency of program paths and branches. Given a path, we use symbolic execution to obtain the path condition which is a set of constraints; then we use volume computation to obtain the size of the solution space for the constraints. With such a methodology and supporting tools, we can decide which paths in a program are executed more often than the others. We can also generate certain test cases that are related to the execution frequency, e.g., those covering cold paths."
},
{
  "Title": "Programs, Tests, and Oracles: The Foundations of Testing Revisited",
  "Type": "Technical/Research Track",
  "Key": "programs-tests-and-oracles-foundations-testing-revisited",
  "Authors": ["Matt Staats", "Michael W. Whalen", "Mats P.E. Heimdahl"],
  "Affiliations": ["University of Minnesota, USA"],
  "Abstract": "In previous decades, researchers have explored the formal foundations of program testing. By exploring the foundations of testing largely separate from any specific method of testing, these researchers provided a general discussion of the testing process, including the goals, the underlying problems, and the limitations of testing. Unfortunately, a common, rigorous foundation has not been widely adopted in empirical software testing research, making it difficult to generalize and compare empirical research.\n\nWe continue this foundational work, providing a framework intended to serve as a guide for future discussions and empirical studies concerning software testing. Specifically, we extend Gourlay's functional description of testing with the notion of a test oracle, an aspect of testing largely overlooked in previous foundational work and only lightly explored in general. We argue additional work exploring the interrelationship between programs, tests, and oracles should be performed, and use our extension to clarify concepts presented in previous work, present new concepts related to test oracles, and demonstrate that oracle selection must be considered when discussing the efficacy of a testing process."
},
{
  "Title": "The Quamoco Tool Chain for Quality Modeling and Assessment",
  "Type": "Demonstrations Track",
  "Key": "quamoco-tool-chain-quality-modeling-and-assessment",
  "Authors": ["Florian Deissenboeck", "Lars Heinemann", "Markus Herrmannsdoerfer", "Klaus Lochmann", "Stefan Wagner"],
  "Affiliations": ["Technische Universität München, Germany"],
  "Abstract": "Continuous quality assessment is crucial for the long-term success of evolving software. On the one hand, code analysis tools automatically supply quality indicators, but do not provide a complete overview of software quality. On the other hand, quality models deﬁne abstract characteristics that inﬂuence quality, but are not operationalized. Currently, no tool chain exists that integrates code analysis tools with quality models. To alleviate this, the Quamoco project provides a tool chain to both deﬁne and assess software quality. The tool chain consists of a quality model editor and an integration with the quality assessment toolkit ConQAT. Using the editor, we can deﬁne quality models ranging from abstract characteristics down to operationalized measures. From the quality model, a ConQAT conﬁguration can be generated that can be used to automatically assess the quality of a software system."
},
{
  "Title": "RACEZ: A Lightweight and Non-invasive Race Detection Tool for Production Applications",
  "Type": "Technical/Research Track",
  "Key": "racez-lightweight-and-non-invasive-race-detection-tool-production-applications",
  "Authors": ["Tianwei Sheng", "Neil Vachharajani", "Stephane Eranian", "Robert Hundt", "Wenguang Chen", "Weimin Zheng"],
  "Affiliations": ["Tsinghua University, China", "Google Inc., USA"],
  "Abstract": "Concurrency bugs, particularly data races, are notoriously difficult to debug and are a significant source of unreliability in multithreaded applications. Many tools to catch data races rely on program instrumentation to obtain memory instruction traces. Unfortunately, this instrumentation introduces significant runtime overhead, is extremely invasive, or has a limited domain of applicability making these tools unsuitable for many production systems. Consequently, these tools are typically used during application testing where many data races go undetected.\n\nThis paper proposes RACEZ, a novel race detection mechanism which uses a sampled memory trace collected by the hardware performance monitoring unit rather than invasive instrumentation. The approach introduces only a modest overhead making it usable in production environments. We validate RACEZ using two open source server applications and the PARSEC benchmarks. Our experiments show that RACEZ catches a set of known bugs with reasonable probability while introducing only 2.8% runtime slow down on average."
},
{
  "Title": "ReAssert: A Tool for Repairing Broken Unit Tests",
  "Type": "Demonstrations Track",
  "Key": "reassert-tool-repairing-broken-unit-tests",
  "Authors": ["Brett Daniel", "Danny Dig", "Tihomir Gvero", "Vilas Jagannath", "Johnston Jiaa", "Damion Mitchell", "Jurand Nogiec", "Shin Hwei Tan", "Darko Marinov"],
  "Affiliations": ["University of Illinois, USA", "EPFL, Switzerland"],
  "Abstract": "Successful software systems continuously change their requirements and thus code. When this happens, some existing tests get broken because they no longer reﬂect the intended behavior, and thus they need to be updated. Repairing broken tests can be time-consuming and diﬃcult. We present ReAssert, a tool that can automatically suggest repairs for broken unit tests. Examples include replacing literal values in tests, changing assertion methods, or replacing one assertion with several. Our experiments show that ReAssert can repair many common test failures and that its suggested repairs match developers’ expectations."
},
{
  "Title": "Reengineering Legacy Software Products into Software Product Line Based on Automatic Variability Analysis",
  "Type": "Paper",
  "Key": "reengineering-legacy-software-products-software-product-line-based-automatic-variability-ana",
  "Authors": ["Yinxing Xue"],
  "Affiliations": ["National University of Singapore, Singapore"],
  "Abstract": "In order to deliver the various and short time-to-market software products to customers, the paradigm of Software Product Line (SPL) represents a new endeavor to the software development. To migrate a family of legacy software products into SPL for effective reuse, one has to understand commonality and variability among existing products variants. The existing techniques rely on manual identification and modeling of variability, and the analysis based on those techniques is performed at several mutually independent levels of abstraction. We propose a sandwich approach that consolidates feature knowledge from top-down domain analysis with bottom-up analysis of code similarities in subject software products. Our proposed method integrates model differencing, clone detection, and information retrieval techniques, which can provide a systematic means to reengineer the legacy software products into SPL based on automatic variability analysis."
},
{
  "Title": "Refactoring Java Programs for Flexible Locking",
  "Type": "Technical/Research Track",
  "Key": "refactoring-java-programs-flexible-locking",
  "Authors": ["Max Schäfer", "Manu Sridharan", "Julian Dolby", "Frank Tip"],
  "Affiliations": ["Oxford University, UK", "IBM T.J. Watson Research Center, USA"],
  "Abstract": "Recent versions of the Java standard library offer flexible locking constructs that go beyond the language's built-in monitor locks in terms of features, and that can be fine-tuned to suit specific application scenarios. Under certain conditions, the use of these constructs can improve performance significantly, by reducing lock contention. However, the code transformations needed to convert between locking constructs are non-trivial, and great care must be taken to update lock usage throughout the program consistently. We present Relocker, an automated tool that assists programmers with refactoring synchronized blocks into ReentrantLocks and ReadWriteLocks, to make exploring the performance tradeoffs among these constructs easier. In experiments on a collection of real-world Java applications, Relocker was able to refactor over 80% of built-in monitors into ReentrantLocks. Additionally, in most cases the tool could automatically infer the same ReadWriteLock usage that programmers had previously introduced manually."
},
{
  "Title": "Refactoring Pipe-like Mashups for End-User Programmers",
  "Type": "Technical/Research Track",
  "Key": "refactoring-pipe-mashups-end-user-programmers",
  "Authors": ["Kathryn T. Stolee", "Sebastian Elbaum"],
  "Affiliations": ["University of Nebraska-Lincoln, USA"],
  "Abstract": "Mashups are becoming increasingly popular as end users are easily accessing, manipulating, and composing data from many web sources. We have observed, however, that mashups tend to suffer from deficiencies that propagate through reuse. To address these deficiencies, we aim to bring some of the benefits of software engineering techniques to end users programming mashups. Here, we identify code smells indicative of deficiencies we observed in mashups programmed in the popular Yahoo! Pipes environment, and observe that end users generally prefer pipes that lack smells. We introduce refactorings that target those smells, reduce the mashup complexity, increase their abstraction, update broken data sources and dated components, and standardize their structures according to community development patterns. We shows that smells exist in 81% of a large sample of mashups and that refactorings reduce that number to 16%, illustrating the potential of refactoring to support the thousands of end users developing mashups."
},
{
  "Title": "Refactoring to Role Objects",
  "Type": "Technical/Research Track",
  "Key": "refactoring-role-objects",
  "Authors": ["Friedrich Steimann"],
  "Affiliations": ["Fernuniversitaet in Hagen, Germany"],
  "Abstract": "Role objects are a widely recognized design pattern for representing objects that expose different properties in different contexts. By developing a tool that automatically refactors legacy code towards this pattern and by applying this tool to several programs, we have found not only that refactoring to role objects as currently defined produces code that is hard to read and to maintain, but also that the refactoring has preconditions so strong that it is rarely applicable in practice. We have therefore taken a fresh look at role objects and devised an alternative form that solves the exact same design problems, yet is much simpler to introduce and to maintain. We describe refactoring to this new, lightweight form of role objects in informal terms and report on the implementation of our refactoring tool for the JAVA programming language, presenting evidence of the refactoring’s increased applicability in several sample programs."
},
{
  "Title": "Requirements Tracing: Discovering Related Documents through Artificial Pheromones and Term Proximity",
  "Type": "Paper",
  "Key": "requirements-tracing-discovering-related-documents-through-artificial-pheromones-and-term-pr",
  "Authors": ["Hakim Sultanov"],
  "Affiliations": ["University of Kentucky, USA"],
  "Abstract": "Requirements traceability is an important undertaking as part of ensuring the quality of software in the early stages of the Software Development Life Cycle. This paper demonstrates the applicability of swarm intelligence to the requirements tracing problem using pheromone communication and a focus on the common text around linking terms or words in order to find related textual documents. Through the actions and contributions of each individual member of the swarm, the swarm as a whole exposes relationships between documents in a collective manner. Two techniques have been examined, simple swarm and pheromone swarm. The techniques have been validated using two real-world datasets from two problem domains."
},
{
  "Title": "Reuse vs. Maintainability: Revealing the Impact of Composition Code Properties",
  "Type": "Paper",
  "Key": "reuse-vs-maintainability-revealing-impact-composition-code-properties",
  "Authors": ["Francisco Dantas"],
  "Affiliations": ["PUC-Rio, Brazil"],
  "Abstract": "Over the last years, several composition mechanisms have emerged to improve program modularity. Even though these mechanisms widely vary in their notation and semantics, they all promote a shift in the way programs are structured. They promote expressive means to define the composition of two or more reusable modules. However, given the complexity of the composition code, its actual effects on software quality are not well understood. This PhD research aims at investigating the impact of emerging composition mechanisms on the simultaneous satisfaction of software reuse and maintainability. In order to perform this analysis, we intend to define a set of compositiondriven metrics and compare their efficacy with traditional modularity metrics. Finally, we plan to derive guidelines on how to use new composition mechanisms to maximize reuse and stability of software modules."
},
{
  "Title": "Reverse Engineering Feature Models",
  "Type": "Technical/Research Track",
  "Key": "reverse-engineering-feature-models",
  "Authors": ["Steven She", "Rafael Lotufo", "Thorsten Berger", "Andrzej Wąsowski", "Krzysztof Czarnecki"],
  "Affiliations": ["University of Waterloo, Canada", "University of Leipzig, Germany", "IT University of Copenhagen, Denmark"],
  "Abstract": "Feature models describe the common and variable characteristics of a product line. Their advantages are well recognized in product line methods. Unfortunately, creating a feature model for an existing project is time-consuming and requires substantial eﬀort from a modeler.\n\nWe present procedures for reverse engineering feature models based on a crucial heuristic for identifying parents—the major challenge of this task. We also automatically recover constructs such as feature groups, mandatory features, and implies/excludes edges. We evaluate the technique on two large-scale software product lines with existing reference feature models—the Linux and eCos kernels—and FreeBSD, a project without a feature model. Our heuristic is eﬀective across all three projects by ranking the correct parent among the top results for a vast majority of features. The procedures eﬀectively reduce the information a modeler has to consider from thousands of choices to typically ﬁve or less."
},
{
  "Title": "Run-Time Efficient Probabilistic Model Checking",
  "Type": "Technical/Research Track",
  "Key": "run-time-efficient-probabilistic-model-checking",
  "Authors": ["Antonio Filieri", "Carlo Ghezzi", "Giordano Tamburrelli"],
  "Affiliations": ["Politecnico di Milano, Italy"],
  "Abstract": "Unpredictable changes continuously affect software systems and may have a severe impact on their quality of service, potentially jeopardizing the system's ability to meet the desired requirements. Changes may occur in critical components of the system, clients' operational profiles, requirements, or deployment environments.\n\nThe adoption of software models and model checking techniques at run time may support automatic reasoning about such changes, detect harmful configurations, and potentially enable appropriate (self-)reactions. However, traditional model checking techniques and tools may not be simply applied as they are at run time, since they hardly meet the constraints imposed by on-the-fly analysis, in terms of execution time and memory occupation.\n\nThis paper precisely addresses this issue and focuses on reliability models, given in terms of Discrete Time Markov Chains, and probabilistic model checking. It develops a mathematical framework for run-time probabilistic model checking that, given a reliability model and a set of requirements, statically generates a set of expressions, which can be efficiently used at run-time to verify system requirements. An experimental comparison of our approach with existing probabilistic model checkers shows its practical applicability in run-time verification."
},
{
  "Title": "Scalable Automatic Linearizability Checking",
  "Type": "Paper",
  "Key": "scalable-automatic-linearizability-checking",
  "Authors": ["Shao Jie Zhang"],
  "Affiliations": ["National University of Singapore, Singapore"],
  "Abstract": "Concurrent data structures are widely used but notoriously difficult to implement correctly. Linearizability is one main correctness criterion of concurrent data structure algorithms. It guarantees that a concurrent data structure appears as a sequential one to users. Unfortunately, linearizability is challenging to verify since a subtle bug may only manifest in a small portion of numerous thread interleavings. Model checking is therefore a potential primary candidate. However, current approaches of model checking linearizability suffer from severe state space explosion problem and are thus restricted in handling few threads and/or operations. This paper describes a scalable, fully automatic and general linearizability checking method based on [16] by incorporating symmetry and partial order reduction techniques. Our insights emerged from the observation that the similarity of threads using concurrent data structures causes model checking to generate large redundant equivalent portions of the state space, and the loose coupling of threads causes it to explore lots of unnecessary transition execution orders. We prove that symmetry reduction and partial order reduction can be combined in our approach and integrate them into the model checking algorithm. We demonstrate its efficiency using a number of real-world concurrent data structure algorithms."
},
{
  "Title": "Search-Enhanced Testing",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "search-enhanced-testing",
  "Authors": ["Colin Atkinson", "Oliver Hummel", "Werner Janjic"],
  "Affiliations": ["University of Mannheim, Germany"],
  "Abstract": "The prime obstacle to automated defect testing has always been the generation of “correct” results against which to judge the behavior of the system under test – the “oracle problem”. So called “back-to-back” testing techniques that exploit the availability of multiple versions of a system to solve the oracle problem have mainly been restricted to very special, safety critical domains such as military and space applications since it is so expensive to manually develop the additional versions. However, a new generation of software search engines that can ﬁnd multiple copies of software components at virtually zero cost promise to change this situation. They make it economically feasible to use the knowledge locked in reusable software components to dramatically improve the eﬃciency of the software testing process. In this paper we outline the basic ingredients of such an approach."
},
{
  "Title": "Searching, Selecting, and Synthesizing Source Code",
  "Type": "Paper",
  "Key": "searching-selecting-and-synthesizing-source-code",
  "Authors": ["Collin McMillan"],
  "Affiliations": ["College of William and Mary, USA"],
  "Abstract": "As a programmer writes new software, he or she may instinctively sense that certain functionality is generally or widely-enough applicable to have been implemented before. Unfortunately, programmers face major challenges when attempting to reuse this functionality: First, developers must search for source code relevant to the high-level task at hand. Second, they must select speciﬁc components from the relevant code to reuse. Third, they synthesize these components into their own software projects. Techniques exist to address speciﬁc instances of these three challenges, but these techniques do not support programmers throughout the reuse process. The goal of this research is to create a uniﬁed approach to searching, selecting, and synthesizing source code. We believe that this approach will provide programmers with crucial insight on how high-level functionality present in existing software can be reused."
},
{
  "Title": "SEREBRO: Facilitating Student Project Team Collaboration",
  "Type": "Demonstrations Track",
  "Key": "serebro-facilitating-student-project-team-collaboration",
  "Authors": ["Noah M. Jorgenson", "Matthew L. Hale", "Rose F. Gamble"],
  "Affiliations": ["University of Tulsa, USA"],
  "Abstract": "In this demonstration, we show SEREBRO, a lightweight courseware developed for student team collaboration in a software engineering class. SEREBRO couples an idea forum with software project management tools to maintain cohesive interaction between team discussion and resulting work products, such as tasking, documentation, and version control. SEREBRO has been used consecutively for two years of software engineering classes. Student input and experiments on student use in these classes has directed SERBRO to its current functionality."
},
{
  "Title": "Sketching Tools for Ideation",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "sketching-tools-ideation",
  "Authors": ["Rachel Bellamy", "Michael Desmond", "Jacquelyn Martino", "Paul Matchen", "Harold Ossher", "John Richards", "Cal Swart"],
  "Affiliations": ["IBM Research, USA"],
  "Abstract": "Sketching facilitates design in the exploration of ideas about concrete objects and abstractions. In fact, throughout the software engineering process when grappling with new ideas, people reach for a pen and start sketching. While pen and paper work well, digital media can provide additional features to benefit the sketcher. Digital support will only be successful, however, if it does not detract from the core sketching experience. Based on research that defines characteristics of sketches and sketching, this paper offers three preliminary tool examples. Each example is intended to enable sketching while maintaining its characteristic experience."
},
{
  "Title": "Socio-Technical Developer Networks: Should We Trust Our Measurements?",
  "Type": "Technical/Research Track",
  "Key": "socio-technical-developer-networks-should-we-trust-our-measurements",
  "Authors": ["Andrew Meneely", "Laurie Williams"],
  "Affiliations": ["North Carolina State University, USA"],
  "Abstract": "Software development teams must be properly structured to provide effective collaboration to produce quality software. Over the last several years, social network analysis (SNA) has emerged as a popular method for studying the collaboration and organization of people working in large software development teams. Researchers have been modeling networks of developers based on socio-technical connections found in software development artifacts. Using these developer networks, researchers have proposed several SNA metrics that can predict software quality factors and describe the team structure. But do SNA metrics measure what they purport to measure? The objective of this research is to investigate if SNA metrics represent socio-technical relationships by examining if developer networks can be corroborated with developer perceptions. To measure developer perceptions, we developed an online survey that is personalized to each developer of a development team based on that developer’s SNA metrics. Developers answered questions about other members of the team, such as identifying their collaborators and the project experts. A total of 124 developers responded to our survey from three popular open source projects: the Linux kernel, the PHP programming language, and the Wireshark network protocol analyzer. Our results indicate that connections in the developer network are statistically associated with the collaborators whom the developers named. Our results substantiate that SNA metrics represent socio-technical relationships in open source development projects, while also clarifying how the developer network can be interpreted by researchers and practitioners."
},
{
  "Title": "A Software Behaviour Analysis Framework Based on the Human Perception Systems",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "software-behaviour-analysis-framework-based-human-perception-systems",
  "Authors": ["Heidar Pirzadeh", "Abdelwahab Hamou-Lhadj"],
  "Affiliations": ["Concordia University, Canada"],
  "Abstract": "Understanding software behaviour can help in a variety of software engineering tasks if one can develop effective techniques for analyzing the information generated from a system's run. These techniques often rely on tracing. Traces, however, can be considerably large and complex to process. In this paper, we present an innovative approach for trace analysis inspired by the way the human brain and perception systems operate. The idea is to mimic the psychological processes that have been developed over the years to explain how our perception system deals with huge volume of visual data. We show how similar mechanisms can be applied to the abstraction and simplification of large traces. Some preliminary results are also presented."
},
{
  "Title": "Software Systems as Cities: A Controlled Experiment",
  "Type": "Technical/Research Track",
  "Key": "software-systems-cities-controlled-experiment",
  "Authors": ["Richard Wettel", "Michele Lanza", "Romain Robbes"],
  "Affiliations": ["University of Lugano, Switzerland", "University of Chile, Chile"],
  "Abstract": "Software visualization is a popular program comprehension technique used in the context of software maintenance, reverse engineering, and software evolution analysis. While there is a broad range of software visualization approaches, few have been empirically evaluated. This is detrimental to the acceptance of software visualization in both the academic and the industrial world.\n\nWe present a controlled experiment for the empirical evaluation of a 3D software visualization approach based on a city metaphor and implemented in a tool called CodeCity. The goal is to provide experimental evidence of the viability of our approach in the context of program comprehension by having subjects perform tasks related to program comprehension. We designed our experiment based on lessons extracted from the current body of research. We conducted the experiment in four locations across three countries, involving 41 participants from both academia and industry. The experiment shows that CodeCity leads to a statistically significant increase in terms of task correctness and decrease in task completion time. We detail the experiment we performed, discuss its results and reflect on the many lessons learned."
},
{
  "Title": "SORASCS: A Case Study in SOA-based Platform Design for Socio-Cultural Analysis",
  "Type": "Software Engineering in Practice Track",
  "Key": "sorascs-case-study-soa-based-platform-design-socio-cultural-analysis",
  "Authors": ["Bradley Schmerl", "David Garlan", "Vishal Dwivedi", "Michael Bigrigg", "Kathleen M. Carley"],
  "Affiliations": ["Carnegie Mellon University, USA"],
  "Abstract": "An increasingly important class of software-based systems is platforms that permit integration of third-party components, services, and tools. Service-Oriented Architecture (SOA) is one such plat-form that has been successful in providing integration and distri-bution in the business domain, and could be effective in other domains (e.g., scientific computing, healthcare, and complex deci-sion making). In this paper, we discuss our application of SOA to provide an integration platform for socio-cultural analysis, a domain that, through models, tries to understand, analyze and predict relationships in large complex social systems. In developing this platform, called SORASCS, we had to overcome issues we believe are generally applicable to any application of SOA within a domain that involves technically naïve users and seeks to establish a sustainable software ecosystem based on a common integration platform. We discuss those issues and the lessons learned about the kinds of problems that occur, and pathways toward a solution."
},
{
  "Title": "Specification Mining in Concurrent and Distributed Systems",
  "Type": "Paper",
  "Key": "specification-mining-concurrent-and-distributed-systems",
  "Authors": ["Sandeep Kumar"],
  "Affiliations": ["National University of Singapore, Singapore"],
  "Abstract": "Distributed systems contain several interacting components that perform complex computational tasks. Formal speciﬁcation of the interaction protocols are crucial to the understanding of these systems. Dynamic speciﬁcation mining from traces containing information about actual interactions during execution of distributed systems can play a useful role in veriﬁcation and comprehension when formal speciﬁcation is not available. A framework for behavioral speciﬁcation mining in distributed systems is proposed. Concurrency and complexity in the distributed models raise special challenges to speciﬁcation mining in such systems."
},
{
  "Title": "Specification Mining in Concurrent and Distributed Systems",
  "Type": "Paper",
  "Key": "specification-mining-concurrent-and-distributed-systems-0",
  "Authors": ["Sandeep Kumar"],
  "Affiliations": ["National University of Singapore, Singapore"],
  "Abstract": "Dynamic speciﬁcation mining involves discovering software behavior from traces for the purpose of program comprehension and bug detection. However, in concurrent/distributed programs, the inherent partial order relationships among events occurring across processes pose a big challenge to speciﬁcation mining. A framework for mining partial orders that takes in a set of concurrent program traces, and produces a message sequence graph (MSG) is proposed. Mining an MSG allows one to understand concurrent behaviors since the nodes of the MSG depict important “phases” or “interaction snippets” involving several concurrently executing processes. Experiments on mining behaviors of fairly complex distributed systems show that the proposed miner can produce the corresponding MSGs with both high precision and high recall."
},
{
  "Title": "StakeSource2.0: Using Social Networks of Stakeholders to Identify and Prioritise Requirements",
  "Type": "Demonstrations Track",
  "Key": "stakesource20-using-social-networks-stakeholders-identify-and-prioritise-requirements",
  "Authors": ["Soo Ling Lim", "Daniela Damian", "Anthony Finkelstein"],
  "Affiliations": ["University College London, UK", "University of Victoria, Canada"],
  "Abstract": "Software projects typically rely on system analysts to conduct requirements elicitation, an approach potentially costly for large projects with many stakeholders and requirements. This paper describes StakeSource2.0, a web-based tool that uses social networks and collaborative filtering, a “crowdsourcing” approach, to identify and prioritise stakeholders and their requirements."
},
{
  "Title": "Static Extraction of Program Configuration Options",
  "Type": "Technical/Research Track",
  "Key": "static-extraction-program-configuration-options",
  "Authors": ["Ariel S. Rabkin", "Randy Katz"],
  "Affiliations": ["UC Berkeley, USA"],
  "Abstract": "Many programs use a key-value model for configuration options. We examined how this model is used in seven open source Java projects totaling over a million lines of code. We present a static analysis that extracts a list of configuration options for a program. Our analysis finds 95% of the options read by the programs in our sample, making it more complete than existing documentation.\n\nMost configuration options we saw fall into a small number of types: a dozen types cover 90% of options. We present a second analysis that exploits this fact and can determine the type of most options. Together, these analyses enable more visibility into program configuration, helping reduce the burden of configuration documentation and configuration debugging."
},
{
  "Title": "A Study of Ripple Effects in Software Ecosystems",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "study-ripple-effects-software-ecosystems",
  "Authors": ["Romain Robbes", "Mircea Lungu"],
  "Affiliations": ["University of Chile, Chile", "University of Bern, Switzerland"],
  "Abstract": "When the Application Programming Interface (API) of a framework or library changes, its clients must be adapted. This change propagation—known as a ripple effect—is a problem that has garnered interest: several approaches have been proposed in the literature to react to these changes.\n\nAlthough studies of ripple effects exist at the single system level, no study has been performed on the actual extent and impact of these API changes in practice, on an entire software ecosystem associated with a community of developers. This paper reports on early results of such an empirical study of API changes that led to ripple effects across an entire ecosystem. Our case study subject is the development community gravitating arounf the Squeak and Pharo software ecosystems: six years of evolution, nearly 3,000 contributors, and close to 2,500 distinct systems."
},
{
  "Title": "Supporting Professional Spreadsheet Users by Generating Leveled Dataflow Diagrams",
  "Type": "Technical/Research Track",
  "Key": "supporting-professional-spreadsheet-users-generating-leveled-dataflow-diagrams",
  "Authors": ["Felienne Hermans", "Martin Pinzger", "Arie van Deursen"],
  "Affiliations": ["Delft University of Technology, Netherlands"],
  "Abstract": "Thanks to their flexibility and intuitive programming model, spreadsheets are widely used in industry, often for business-critical applications. Similar to software developers, professional spreadsheet users demand support for maintaining and transferring their spreadsheets.\n\nIn this paper, we first study the problems and information needs of professional spreadsheet users by means of a survey conducted at a large financial company. Based on these needs, we then present an approach that extracts this information from spreadsheets and presents it in a compact and easy to understand way, using leveled dataflow diagrams. Our approach comes with three different views on the dataflow and allows the user to analyze the dataflow diagrams in a top-down fashion also using slicing techniques.\n\nTo evaluate the usefulness of the proposed approach, we conducted a series of interviews as well as nine case studies in an industrial setting. The results of the evaluation clearly indicate the demand for and usefulness of our approach in ease the understanding of spreadsheets."
},
{
  "Title": "Symbolic Execution for Software Testing in Practice – Preliminary Assessment",
  "Type": "Impact Project Focus Area",
  "Key": "symbolic-execution-software-testing-practice--preliminary-assessment",
  "Authors": ["Cristian Cadar", "Patrice Godefroid", "Sarfraz Khurshid", "Corina S. Păsăreanu", "Koushik Sen", "Nikolai Tillmann", "Willem Visser"],
  "Affiliations": ["Imperial College London, UK", "Microsoft Research, USA", "University of Texas at Austin, USA", "CMU/NASA Ames Research Center, USA", "UC Berkeley, USA", "University Stellenbosch, South Africa)"],
  "Abstract": "We present results for the “Impact Project Focus Area” on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry."
},
{
  "Title": "Symbolic Model Checking of Software Product Lines",
  "Type": "Technical/Research Track",
  "Key": "symbolic-model-checking-software-product-lines",
  "Authors": ["Andreas Classen", "Patrick Heymans", "Pierre-Yves Schobbens", "Axel Legay"],
  "Affiliations": ["University of Namur, Belgium", "IRISA/INRIA Rennes, France and University of Liège, Belgium"],
  "Abstract": "We study the problem of model checking software product line (SPL) behaviours against temporal properties. This is more difficult than for single systems because an SPL with n features yields up to 2n individual systems to verify. As each individual verification suffers from state explosion, it is crucial to propose efficient formalisms and heuristics.\n\nWe recently proposed featured transition systems (FTS), a compact representation for SPL behaviour, and defined algorithms for model checking FTS against linear temporal properties. Although they showed to outperform individual system verifications, they still face a state explosion problem as they enumerate and visit system states one by one.\n\nIn this paper, we tackle this latter problem by using symbolic representations of the state space. This lead us to consider computation tree logic (CTL) which is supported by the industry-strength symbolic model checker NuSMV. We first lay the foundations for symbolic SPL model checking by defining a feature-oriented version of CTL and its dedicated algorithms. We then describe an implementation that adapts the NuSMV language and tool infrastructure. Finally, we propose theoretical and empirical evaluations of our results. The benchmarks show that for certain properties, our algorithm is over a hundred times faster than model checking each system with the standard algorithm."
},
{
  "Title": "Synthesis of Live Behaviour Models for Fallible Domains",
  "Type": "Technical/Research Track",
  "Key": "synthesis-live-behaviour-models-fallible-domains",
  "Authors": ["Nicolás D'Ippolito", "Víctor Braberman", "Nir Piterman", "Sebastián Uchitel"],
  "Affiliations": ["Imperial College London, UK", "Universidad de Buenos Aires, Argentina", "University of Leicester, UK"],
  "Abstract": "We revisit synthesis of live controllers for event-based operational models. We remove one aspect of an idealised problem domain by allowing to integrate failures of controller actions in the environment model. Classical treatment of failures through strong fairness leads to a very high computational complexity and may be insuﬃcient for many interesting cases. We identify a realistic stronger fairness condition on the behaviour of failures. We show how to construct controllers satisfying liveness speciﬁcations under these fairness conditions. The resulting controllers exhibit the only possible behaviour in face of the given topology of failures: they keep retrying and never give up. We then identify some well-structure conditions on the environment. These conditions ensure that the resulting controller will be eager to satisfy its goals. Furthermore, for environments that satisfy these conditions and have an underlying probabilistic behaviour, the measure of traces that satisfy our fairness condition is 1, giving a characterisation of the kind of domains in which the approach is applicable."
},
{
  "Title": "Systematizing Security Test Case Planning Using Functional Requirements Phrases",
  "Type": "Paper",
  "Key": "systematizing-security-test-case-planning-using-functional-requirements-phrases",
  "Authors": ["Ben Smith"],
  "Affiliations": ["North Carolina State University, USA"],
  "Abstract": "Security experts use their knowledge to attempt attacks on an application in an exploratory and opportunistic way in a process known as penetration testing. However, building security into a product is the responsibility of the whole team, not just the security experts who are often only involved in the final phases of testing. Through the development of a black box security test plan, software testers who are not necessarily security experts can work proactively with the developers early in the software development lifecycle. The team can then establish how security will be evaluated such that the product can be designed and implemented with security in mind. The goal of this research is to improve the security of applications by introducing a methodology that uses the software system's requirements specification statements to systematically generate a set of black box security tests. We used our methodology on a public requirements specification to create 137 tests and executed these tests on five electronic health record systems. The tests revealed 253 successful attacks on these five systems, which are used to manage the clinical records for approximately 59 million patients, collectively. If non-expert testers can surface the more common vulnerabilities present in an application, security experts can attempt more devious, novel attacks."
},
{
  "Title": "Taming Reflection: Aiding Static Analysis in the Presence of Reflection and Custom Class Loaders",
  "Type": "Technical/Research Track",
  "Key": "taiming-reflection-aiding-static-analysis-presence-reflection-and-custom-class-loaders-0",
  "Authors": ["Eric Bodden", "Andreas Sewe", "Jan Sinschek", "Hela Oueslati", "Mira Mezini"],
  "Affiliations": ["TU Darmstadt, Germany"],
  "Abstract": "Static program analyses and transformations for Java face many problems when analyzing programs that use reﬂection or custom class loaders: How can a static analysis know which reﬂective calls the program will execute? How can it get hold of classes that the program loads from remote locations or even generates on the ﬂy? And if the analysis transforms classes, how can these classes be re-inserted into a program that uses custom class loaders? In this paper, we present TamiFlex, a tool chain that oﬀers a partial but often eﬀective solution to these problems. With TamiFlex, programmers can use existing staticanalysis tools to produce results that are sound at least with respect to a set of recorded program runs. TamiFlex inserts runtime checks into the program that warn the user in case the program executes reﬂective calls that the analysis did not take into account. TamiFlex further allows programmers to re-insert oﬄine-transformed classes into a program. We evaluate TamiFlex in two scenarios: benchmarking with the DaCapo benchmark suite and analysing large-scale interactive applications. For the latter, TamiFlex signiﬁcantly improves code coverage of the static analyses, while for the former our approach even appears complete: the inserted runtime checks issue no warning. Hence, for the ﬁrst time, TamiFlex enables sound static whole-program analyses on DaCapo. During this process, TamiFlex usually incurs less than 10% runtime overhead."
},
{
  "Title": "Test Blueprint: An Effective Visual Support for Test Coverage",
  "Type": "Paper",
  "Key": "test-blueprint-effective-visual-support-test-coverage",
  "Authors": ["Vanessa Peña Araya"],
  "Affiliations": ["University of Chile, Chile"],
  "Abstract": "Test coverage is about assessing the relevance of unit tests against the tested application. It is widely acknowledged that a software with a “good” test coverage is more robust against unanticipated execution, thus lowering the maintenance cost. However, insuring a coverage of a good quality is challenging, especially since most of the available test coverage tools do not discriminate software components that require a “strong” coverage from the components that require less attention from the unit tests. Hapao is an innovative test coverage tool, implemented in the Pharo Smalltalk programming language. It employs an eﬀective and intuitive graphical representation to visually assess the quality of the coverage. A combination of appropriate metrics and relations visually shapes methods and classes, which indicates to the programmer whether more eﬀort on testing is required. This paper presents the essence of Hapao using a real world case study."
},
{
  "Title": "Topic-based Defect Prediction",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "topic-based-defect-prediction",
  "Authors": ["Tung T. Nguyen", "Tien N. Nguyen", "Tu M. Phuong"],
  "Affiliations": ["Iowa State University, USA", "Posts and Telecommunications Institute of Technology, Vietnam"],
  "Abstract": "Defects are unavoidable in software development and ﬁxing them is costly and resource-intensive. To build defect pre- diction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe diﬀerent technical concerns/aspects. Those concerns are assumed to have diﬀerent levels of defect-proneness, thus, cause diﬀerent levels of defect-proneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches."
},
{
  "Title": "Toward a Better Understanding of Tool Usage",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "toward-better-understanding-tool-usage",
  "Authors": ["Alberto Sillitti", "Giancarlo Succi", "Jelena Vlasenko"],
  "Affiliations": ["Free University of Bolzano, Italy"],
  "Abstract": "Developers use tools to develop software systems and always alleged better tools are being produced and purchased. Still there have been only limited studies on how people really use tools; these studies have used limited data, and the interactions between tools have not been properly elaborated. The advent of the AISEMA (Automated In-Process Software Engineering Measurement and Analysis) systems [3] has enabled a more detailed collection of tools data. Our “new idea” is to take advantage of such data to build a simple model based on an oriented graph that enables a good understanding on how tools are used individually and collectively. We have empirically validated the model analyzing an industrial team of 19 developers for a period of 10 months."
},
{
  "Title": "Toward Sustainable Software Engineering",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "toward-sustainable-software-engineering",
  "Authors": ["Nadine Amsel", "Zaid Ibrahim", "Amir Malik", "Bill Tomlinson"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "Current software engineering practices have significant effects on the environment. Examples include e-waste from computers made obsolete due to software upgrades, and changes in the power demands of new versions of software. Sustainable software engineering aims to create reliable, long-lasting software that meets the needs of users while reducing environmental impacts. We conducted three related research efforts to explore this area. First, we investigated the extent to which users thought about the environmental impact of their software usage. Second, we created a tool called GreenTracker, which measures the energy consumption of software in order to raise awareness about the environmental impact of software usage. Finally, we explored the indirect environmental effects of software in order to understand how software affects sustainability beyond its own power consumption. The relationship between environmental sustainability and software engineering is complex; understanding both direct and indirect effects is critical to helping humans live more sustainably."
},
{
  "Title": "Towards Architectural Information in Implementation",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "towards-architectural-information-implementation",
  "Authors": ["Henrik Bærbak Christensen", "Klaus Marius Hansen"],
  "Affiliations": ["Aarhus University, Denmark", "University of Copenhagen, Denmark"],
  "Abstract": "Agile development methods favor speed and feature producing iterations. Software architecture, on the other hand, is ripe with techniques that are slow and not oriented directly towards implementation of costumers’ needs. Thus, there is a major challenge in retaining architectural information in a fast-faced agile project. We propose to embed as much architectural information as possible in the central artefact of the agile universe, the code. We argue that thereby valuable architectural information is retained for (automatic) documentation, validation, and further analysis, based on a relatively small investment of eﬀort. We outline some preliminary examples of architectural annotations in Java and Python and their applicability in practice."
},
{
  "Title": "Towards Overcoming Human Analyst Fallibility in the Requirements Tracing Process",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "towards-overcoming-human-analyst-fallibility-requirements-tracing-process",
  "Authors": ["David Cuddeback", "Alex Dekhtyar", "Jane Huffman Hayes", "Jeff Holden", "Wei-Keat Kong"],
  "Affiliations": ["California Polytechnic State University, USA", "University of Kentucky, USA"],
  "Abstract": "Our research group recently discovered that human analysts, when asked to validate candidate traceability matrices, produce predictably imperfect results, in some cases less accurate than the starting candidate matrices. This discovery radically changes our understanding of how to design a fast, accurate and certifiable tracing process that can be implemented as part of software assurance activities. We present our vision for the new approach to achieving this goal. Further, we posit that human fallibility may impact other software engineering activities involving decision support tools."
},
{
  "Title": "Towards Quantitative Software Reliability Assessment in Incremental Development Processes",
  "Type": "Technical/Research Track",
  "Key": "towards-quantitative-software-reliability-assessment-incremental-development-processes",
  "Authors": ["Toshiya Fujii", "Tadashi Dohi", "Takaji Fujiwara"],
  "Affiliations": ["Hiroshima University, Japan", "Fujitsu Quality Laboratory, Japan"],
  "Abstract": "The iterative and incremental development is becoming a major development process model in industry, and allows us for a good deal of parallelism between development and testing. In this paper we develop a quantitative software reliability assessment method in incremental development processes, based on the familiar non-homogeneous Poisson processes. More specifically, we utilize the software metrics observed in each incremental development and testing, and estimate the associated software reliability measures. In a numerical example with a real incremental developmental project data, it is shown that the estimate of software reliability with a specific software reliability model can take a realistic value, and that the reliability growth phenomenon can be observed even in the incremental development scheme."
},
{
  "Title": "Tracing Architectural Concerns in High Assurance Systems",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "tracing-architectural-concerns-high-assurance-systems",
  "Authors": ["Mehdi Mirakhorli", "Jane Cleland-Huang"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "Software architecture is shaped by a diverse set of interacting and competing quality concerns, each of which may have broad-reaching impacts across multiple architectural views. Without traceability support, it is easy for developers to inadvertently change critical architectural elements during ongoing system maintenance and evolution, leading to architectural erosion. Unfortunately, existing traceability practices, tend to result in the proliferation of traceability links, which can be difficult to create, maintain, and understand. We therefore present a decision-centric approach that focuses traceability links around the architectural decisions that have shaped the delivered system. Our approach, which is informed through an extensive investigation of architectural decisions made in real-world safety-critical and performance-critical applications, provides enhanced support for advanced software engineering tasks."
},
{
  "Title": "Tracing Architecturally Significant Requirements: A Decision-Centric Approach",
  "Type": "Paper",
  "Key": "tracing-architecturally-significant-requirements-decision-centric-approach",
  "Authors": ["Mehdi Mirakhorli"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "This thesis describes a Decision-Centric traceability framework that supports software engineering activities such as architectural preservation, impact analysis, and visualization of design intent. We present a set of traceability patterns, derived from studying real-world architectural designs in high-assurance and highperformance systems. We further present a trace-retrieval approach that reverse engineers design decisions and their associated traceability links by training a classifier to recognize fragments of design decisions and then using the traceability patterns to reconstitute the decisions from their individual parts. traceability meta-models [2] for tracing ASRs can lead to the proliferation of a large number of unmanageable traceability links. This research presents a Decision-Centric Traceability (DCT) approach for tracing between ASRs and architectural components, and is designed to help developers ensure long-term integrity of the system-level qualities. In comparison to existing traceability meta-models [2], DCT reduces the number of traceability links and provides semantically rich traceability links that are used to support critical software engineering activities such as architectural preservation, impact analysis, and design visualization."
},
{
  "Title": "Tracking Data Structures for Postmortem Analysis",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "tracking-data-structures-postmortem-analysis",
  "Authors": ["Xiao Xiao", "Jinguo Zhou", "Charles Zhang"],
  "Affiliations": ["Hong Kong University of Science and Technology, Hong Kong"],
  "Abstract": "Analyzing the runtime behaviors of the data structures is important because they usually relate to the obscured program performance and understanding issues. The runtime evolution history of data structures creates the possibility of building a lightweight and non-checkpointing based solution for the backward analysis for validating and mining both the temporal and stationary properties of the data structure. We design and implement TAEDS, a framework that focuses on gathering the data evolution history of a program at the runtime and provides a virtual machine for programmers to examine the behavior of data structures back in time. We show that our approach facilitates many programming tasks such as diagnosing memory problems and improving the design of the data structures themselves."
},
{
  "Title": "Transformation for Class Immutability",
  "Type": "Technical/Research Track",
  "Key": "transformation-class-immutability",
  "Authors": ["Fredrik Kjolstad", "Danny Dig", "Gabriel Acevedo", "Marc Snir"],
  "Affiliations": ["University of Illinois at Urbana-Champaign, USA"],
  "Abstract": "It is common for object-oriented programs to have both mutable and immutable classes. Immutable classes simplify programing because the programmer does not have to reason about side-eﬀects. Sometimes programmers write immutable classes from scratch, other times they transform mutable into immutable classes. To transform a mutable class, programmers must ﬁnd all methods that mutate its transitive state and all objects that can enter or escape the state of the class. The analyses are non-trivial and the rewriting is tedious. Fortunately, this can be automated.\n\nWe present an algorithm and a tool, Immutator, that enables the programmer to safely transform a mutable class into an immutable class. Two case studies and one controlled experiment show that Immutator is useful. It (i) reduces the burden of making classes immutable, (ii) is fast enough to be used interactively, and (iii) is much safer than manual transformations."
},
{
  "Title": "Tuple Density: A New Metric for Combinatorial Test Suites",
  "Type": "New Ideas and Emerging Results Track",
  "Key": "tuple-density-new-metric-combinatorial-test-suites",
  "Authors": ["Baiqiang Chen", "Jian Zhang"],
  "Affiliations": ["Chinese Academy of Sciences, China"],
  "Abstract": "We propose tuple density to be a new metric for combinatorial test suites. It can be used to distinguish one test suite from another even if they have the same size and strength. Moreover, it is also illustrated how a given test suite can be optimized based on this metric. The initial experimental results are encouraging"
},
{
  "Title": "Understanding Broadcast Based Peer Review in Open Source Software Projects",
  "Type": "Technical/Research Track",
  "Key": "understanding-broadcast-based-peer-review-open-source-software-projects",
  "Authors": ["Peter C Rigby", "Margaret-Anne Storey"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "Software peer review has proven to be a successful technique in open source software (OSS) development. In contrast to industry, where reviews are typically assigned to specific individuals, changes are broadcast to thousands of potentially interested stakeholders. What is surprising is that this approach works very well, despite concerns that reviews may be ignored, or that discussions will deadlock because too many uninformed stakeholders are involved. In this paper, we describe an empirical study to investigate the mechanisms and behaviors that developers use to find code changes they are competent to review. We also explore how stakeholders interact with one another during the review process. We manually examine hundreds of reviews across five high profile OSS projects. Our findings provide insights into the simple, community-wide techniques that developers use to effectively manage large quantities of reviews. The themes that emerge from our study are validated by interviewing long-serving core developers."
},
{
  "Title": "Unifying Execution of Imperative and Declarative Code",
  "Type": "Technical/Research Track",
  "Key": "unifying-execution-imperative-and-declarative-code",
  "Authors": ["Aleksandar Milicevic", "Derek Rayside", "Kuat Yessenov", "Daniel Jackson"],
  "Affiliations": ["Massachusetts Institute of Technology, USA"],
  "Abstract": "We present a uniﬁed environment for running declarative speciﬁcations in the context of an imperative object-oriented programming language. Speciﬁcations are Alloy-like, written in ﬁrst-order relational logic with transitive closure, and the imperative language is Java. By being able to mix imperative code with executable declarative speciﬁcations, the user can easily express constraint problems in place, i.e., in terms of the existing data structures and objects on the heap. After a solution is found, the heap is updated to reﬂect the solution, so the user can continue to manipulate the program heap in the usual imperative way. We show that this approach is not only convenient, but, for certain problems can also outperform a standard imperative implementation. We also present an optimization technique that allowed us to run our tool on heaps with almost 2000 objects."
},
{
  "Title": "Using Impact Analysis in Industry",
  "Type": "Paper",
  "Key": "using-impact-analysis-industry",
  "Authors": ["Robert Goeritzer"],
  "Affiliations": ["University of Klagenfurt, Austria"],
  "Abstract": "Software is subjected to continuous change, and with increasing size and complexity performing changes becomes more critical. Impact analysis assists in estimating the consequences of a change, and is an important research topic. Nevertheless, until now researchers have not applied and evaluated those techniques in industry. This paper contributes an approach suitable for an industrial setting, and an evaluation of its application in a large software system."
},
{
  "Title": "Using MATCON to Generate CASE Tools That Guide Deployment of Pre-Packaged Applications",
  "Type": "Demonstrations Track",
  "Key": "using-matcon-generate-case-tools-guide-deployment-pre-packaged-applications",
  "Authors": ["Elad Fein", "Natalia Razinkov", "Shlomit Shachor", "Pietro Mazzoleni", "Sweefen Goh", "Richard Goodwin", "Manisha Bhandar", "Shyh-Kwei Chen", "Juhnyoung Lee", "Vibha Singhal Sinha", "Senthil Mani", "Debdoot Mukherjee", "Biplav Srivastava", "Pankaj Dhoolia"],
  "Affiliations": ["IBM Research Haifa, Israel", "IBM Research Watson, USA", "IBM Research Delhi, India"],
  "Abstract": "The complex process of adapting pre-packaged applications, such as Oracle or SAP, to an organization’s needs is full of challenges. Although detailed, structured, and well-documented methods govern this process, the consulting team implementing the method must spend a huge amount of manual effort to make sure the guidelines of the method are followed as intended by the method author. MATCON breaks down the method content, documents, templates, and work products into reusable objects, and enables them to be cataloged and indexed so these objects can be easily found and reused on subsequent projects. By using models and meta-modeling the reusable methods, we automatically produce a CASE tool to apply these methods, thereby guiding consultants through this complex process. The resulting tool helps consultants create the method deliverables for the initial phases of large customization projects. Our MATCON output, referred to as Consultant Assistant, has shown significant savings in training costs, a 20–30% improvement in productivity, and positive results in large Oracle and SAP implementations."
},
{
  "Title": "Using Software Evolution History to Facilitate Development and Maintenance",
  "Type": "Paper",
  "Key": "using-software-evolution-history-facilitate-development-and-maintenance",
  "Authors": ["Pamela Bhattacharya"],
  "Affiliations": ["UC Riverside, USA"],
  "Abstract": "Much research in software engineering have been focused on improving software quality and automating the maintenance process to reduce software costs and mitigating complications associated with the evolution process. Despite all these efforts, there are still high cost and effort associated with software bugs and software maintenance, software still continues to be unreliable, and software bugs can wreak havoc on software producers and consumers alike. My dissertation aims to advance the state-of-art in software evolution research by designing tools that can measure and predict software quality and to create integrated frameworks that helps in improving software maintenance and research that involves mining software repositories."
},
{
  "Title": "Value-Based Program Characterization and Its Application to Software Plagiarism Detection",
  "Type": "Software Engineering in Practice Track",
  "Key": "value-based-program-characterization-and-its-application-software-plagiarism-detection",
  "Authors": ["Yoon-Chan Jhi", "Xinran Wang", "Xiaoqi Jia", "Sencun Zhu", "Peng Liu", "Dinghao Wu"],
  "Affiliations": ["Pennsylvania State University, USA", "Chinese Academy of Sciences, China"],
  "Abstract": "Identifying similar or identical code fragments becomes much more challenging in code theft cases where plagiarizers can use various automated code transformation techniques to hide stolen code from being detected. Previous works in this ﬁeld are largely limited in that (1) most of them cannot handle advanced obfuscation techniques; (2) the methods based on source code analysis are less practical since the source code of suspicious programs is typically not available until strong evidences are collected; and (3) those depending on the features of speciﬁc operating systems or programming languages have limited applicability. Based on an observation that some critical runtime values are hard to be replaced or eliminated by semanticspreserving transformation techniques, we introduce a novel approach to dynamic characterization of executable programs. Leveraging such invariant values, our technique is resilient to various control and data obfuscation techniques. We show how the values can be extracted and reﬁned to expose the critical values and how we can apply this runtime property to help solve problems in software plagiarism detection. We have implemented a prototype with a dynamic taint analyzer atop a generic processor emulator. Our experimental results show that the value-based method successfully discriminates 34 plagiarisms obfuscated by SandMark, plagiarisms heavily obfuscated by KlassMaster, programs obfuscated by Thicket, and executables obfuscated by Loco/Diablo."
},
{
  "Title": "Verifying Multi-threaded Software using SMT-based Context-Bounded Model Checking",
  "Type": "Technical/Research Track",
  "Key": "verifying-multi-threaded-software-using-smt-based-context-bounded-model-checking",
  "Authors": ["Lucas Cordeiro", "Bernd Fischer"],
  "Affiliations": ["University of Southampton, United Kingdom"],
  "Abstract": "We describe and evaluate three approaches to model check multi-threaded software with shared variables and locks using bounded model checking based on Satisﬁability Modulo Theories (SMT) and our modelling of the synchronization primitives of the Pthread library. In the lazy approach, we generate all possible interleavings and call the SMT solver on each of them individually, until we either ﬁnd a bug, or have systematically explored all interleavings. In the schedule recording approach, we encode all possible interleavings into one single formula and then exploit the high speed of the SMT solvers. In the underapproximation and widening approach, we reduce the state space by abstracting the number of interleavings from the proofs of unsatisﬁability generated by the SMT solvers. In all three approaches, we bound the number of context switches allowed among threads in order to reduce the number of interleavings explored. We implemented these approaches in ESBMC, our SMT-based bounded model checker for ANSI-C programs. Our experiments show that ESBMC can analyze larger problems and substantially reduce the veriﬁcation time compared to stateof-the-art techniques that use iterative context-bounding algorithms or counter-example guided abstraction reﬁnement."
},
{
  "Title": "View Infinity: A Zoomable Interface for Feature-Oriented Software Development",
  "Type": "Demonstrations Track",
  "Key": "view-infinity-zoomable-interface-feature-oriented-software-development",
  "Authors": ["Michael Stengel", "Janet Feigenspan", "Mathias Frisch", "Christian Kästner", "Sven Apel", "Raimund Dachselt"],
  "Affiliations": ["University of Magdeburg, Germany", "University of Marburg, Germany", "University of Passau, Germany"],
  "Abstract": "Software product line engineering provides eﬃcient means to develop variable software. To support program comprehension of software product lines (SPLs), we developed View Inﬁnity, a tool that provides seamless and semantic zooming of diﬀerent abstraction layers of an SPL. First results of a qualitative study with experienced SPL developers are promising and indicate that View Inﬁnity is useful and intuitive to use."
},
{
  "Title": "Why Software Quality Improvement Fails (and How to Succeed Nevertheless)",
  "Type": "Software Engineering in Practice Track",
  "Key": "why-software-quality-improvement-fails-and-how-succeed-nevertheless",
  "Authors": ["Jonathan Streit", "Markus Pizka"],
  "Affiliations": ["itestra GmbH, Germany"],
  "Abstract": "Quality improvement is the key to enormous cost reduction in the IT business. However, improvement projects often fail in practice. In many cases, stakeholders fearing, e.g., a loss of power or not recognizing the beneﬁts inhibit the improvement. Systematic change management and an economic perspective help to overcome these issues, but are little known and seldom applied.\n\nThis industrial experience report presents the main challenges in software quality improvement projects as well as practices for tackling them. The authors have performed over 50 quality analyses and quality improvement projects in mission-critical software systems of European banking, insurance and automotive companies."
},
{
  "Title": "Metamorphic Testing of a Monte Carlo Modeling Program",
  "Type": "Paper",
  "Key": "828f1725ac9c68fa9f3c9c1c90037d",
  "Authors": ["Junhua Ding", "Tong Wu", "Dianxiang Xu", "Jun Q. Lu", "Xin-Hua Hu"],
  "Affiliations": ["East Carolina University, USA", "Dakota State University, USA"],
  "Abstract": "Photon propagation in biological tissue can be equivalently modeled with Monte Carlo simulations numerically or by the Radiative Transfer Equation (RTE) analytically. However, testing of a Monte Carlo program modeling photon propagation in biological tissue is difficult due to the unknown character of the test oracles. Although approaches based on Beer-Lambert law, van de Hulst’s table or RTE can be used for testing the Monte Carlo modeling program, these approaches are only applied to the program that is designed for homogeneous media. A rigorous way for testing the Monte Carlo modeling program for heterogeneous media is needed. In this paper, we investigate the effectiveness of the metamorphic testing approach to test a Monte Carlo modeling program for heterogeneous media. In addition, the metamorphic testing is extended with the evaluation of the adequacy of testing coverage criteria to measure the quality of the metamorphic testing, to guide the creation of metamorphic relations, to generate testing inputs, and to investigate the found exceptions. The effectiveness of the approach has been demonstrated through testing a Monte Carlo modeling program we developed for simulating photon propagation in human skins."
},
{
  "Title": "Abstracting Timing Information in UML State Charts via Temporal Ordering and LOTOS",
  "Type": "Paper",
  "Key": "8d709409bce142ccb89bd044936fe2",
  "Authors": ["Valentin Chimisliu", "Franz Wotawa"],
  "Affiliations": ["TU Graz, Austria"],
  "Abstract": "As testing of software systems becomes more and more important and expensive, there is a trend to automate as much as possible of this task. This article is intended as an attempt to breach the gap between academic model-based testing tools and their usage in industry. This is done by allowing the specification of a system in a widely accepted industry notation (UML state charts) and via a behind the scene transformation providing a formal representation of the system using the formal language LOTOS. As a byproduct of the transformation a formal semantics of UML state charts is given. An interesting class of software systems well suited for the application are distributed timed control oriented systems. As LOTOS contains no timing constructs, the timing information in the system is automatically abstracted by preserving the execution order of the timeout transitions."
},
{
  "Title": "Scalable Graph Analyzing Approach for Software Fault-Localization",
  "Type": "Paper",
  "Key": "cd5c41bac90e175931b2a72868a9c1",
  "Authors": ["Zaynab Mousavian", "Mojtaba Vahidi-Asl", "Saeed Parsa"],
  "Affiliations": ["Iran University of Science and Technology, Iran"],
  "Abstract": "In this paper, a new approach for analyzing program behavioral graphs to detect fault relevant paths is presented. The existing graph mining approaches for bug localization merely detect discriminative sub-graphs between failing and passing runs. However, they are not applicable when the context of a failure is not appeared in a discriminative pattern. In our proposed method, the suspicious transitions are identified by contrasting nearest neighbor failing and passing dynamic behavioral graphs. For finding similar failing and passing graphs, we first convert the graphs into adequate vectors. Then, a combination of Jacard-Cosine similarity measures is applied to identify the nearest graphs. The new scoring formula takes advantage of null hypothesis testing for ranking weighted transitions. The main advantage of the proposed technique is its scalability which makes it work on large and complex programs with huge number of predicates. Another main capability of our approach is providing the faulty paths constructed from fault suspicious transitions. Considering the weighted execution graphs in the analysis enables us to find those types of bugs which reveal themselves in specific number of transitions between two particular predicates. The experimental results on Siemens test suite and Space program manifest the effectiveness of the proposed method on weighted execution graphs for locating bugs in comparison with other methods."
},
{
  "Title": "Design of Intelligent Agents for Collaborative Testing of Service-Based Systems",
  "Type": "Paper",
  "Key": "9a7f0e191abaf7a896825f7d572ec5",
  "Authors": ["Xiaoying Bai", "Bin Chen", "Bo Ma", "Yunzhan Gong"],
  "Affiliations": ["Tsinghua University, China", "Beijing University of Posts and Telecommunications, China"],
  "Abstract": "Testing on services-based systems faces the challenges of dynamic collaboration. Services are distributed software that can be bound to establish collaborations on-demand. To verify and validate the services, testing needs to react automatically in a coordinated approach. Software agents, which are characterized by persistence, autonomy, social ability and reactivity, are thus introduced to facilitate test deployment, execution, collaboration, and run-time decision making. This paper proposes a design of test agent model, including agents' knowledge, events, actions and interpreter. The knowledge represents the detected environment status, such as test results and changes in services under test. The action models testing behavior such as test configuration, test deployment and test schedule. The Interpreter defines the rules to select actions or parameters on certain events and conditions. In this way, given a set of knowledge at a certain time, a test agent dynamically adjusts its behavior according to its pre-defined rules and strategies. Case studies and experiments are exercised to apply the generic agent design to specific testing tasks such as performance testing and coverage-based testing."
},
{
  "Title": "A Comparative Evaluation of State-of-the-Art Web Service Composition Testing Approaches",
  "Type": "Paper",
  "Key": "4681cc1dc8426c2c606a33f8b2f9ad",
  "Authors": ["Hazlifah Mohd Rusli", "Mazidah Puteh", "Suhaimi Ibrahim", "Sayed Gholam Hassan Tabatabaei"],
  "Affiliations": ["Universiti Teknologi MARA, Malaysia", "Universiti Teknologi Malaysia, Malaysia"],
  "Abstract": "More and more Web based systems are being developed by composing other single or even composite services. This is due to the fact that not all available services are able to satisfy the needs of a user. The process of composing Web services involves discovering the appropriate services, selecting the best services, combining those services together, and finally executing them. Although much research efforts have been dedicated to the discovery, selection, and composition of services, the process of testing the Web service composition has not been given the same attention. This paper discusses the importance of Web services composition testing, provides a classification of the most prominent approaches in that area, presents several criteria for comparison of those approaches, and conducts a comparative evaluation of the approaches. The results of the paper give an essential perspective to do research work on Web services composition testing."
},
{
  "Title": "A Comfortable TestPlayer for Analyzing Statistical Usage Testing Strategies",
  "Type": "Paper",
  "Key": "24b0f48ff7b50088c0d72527c8e72d",
  "Authors": ["Winfried Dulz"],
  "Affiliations": ["University of Erlangen-Nuremberg, Germany"],
  "Abstract": "We will first give a brief introduction to a versatile modeling environment that allows the early validation of system specifications by a test-driven agile simulation approach. The main focus of our paper is on providing techniques for automated test case generation relying on statistical usage models. Based on the open source software R for statistical computing and graphics the easy to handle test case generation and analyzing Testplayer tool was developped. Starting from customer-specific graphical Markov chain usage models test cases are automatically generated and visualized by highlighting selected nodes and arcs in the model. In addition, various metrics and corresponding diagrams offer analytical techniques to assess the quality of the derived test suite."
},
{
  "Title": "Automatically Testing Interactive Multimodal Systems Using Task Trees and Fusion Models",
  "Type": "Paper",
  "Key": "fabc4e3026412281545a1027665ca3",
  "Authors": ["Laya Madani", "Ioannis Parissis"],
  "Affiliations": ["University Al Baath, Syria and Laboratoire d’Informatique de Grenoble, France", "University of Grenoble and Grenoble INP - LCIS, France"],
  "Abstract": "Multimodal systems support communication with the user through di fferent modalities such as voice and gesture. In such systems, modalities may be used sequentially or concurrently, and independently or combined synergistically. Their use is characterized by four properties, called CARE (Complementarity, Assignment, Redundancy and Equivalence). This paper presents a method for generating automatically test data for multimodal systems, based on two models: a model describing the multimodal aspects and a task tree model, both supporting operational pro file speci fication."
},
{
  "Title": "Using Conditional Mutation to Increase the Efficiency of Mutation Analysis",
  "Type": "Paper",
  "Key": "f79dd1883841659eda0235b00a4ea5",
  "Authors": ["René Just", "Gregory M. Kapfhammer", "Franz Schweiggert"],
  "Affiliations": ["Ulm University, Germany", "Allegheny College, USA"],
  "Abstract": "Assessing testing strategies and test sets is a crucial part of software testing. Mutation analysis is, among other approaches, a suitable technique for this purpose. However, compared with other methods it is rather time-consuming and applying mutation analysis to large software systems is still problematic. This paper presents a versatile approach, called conditional mutation, which increases the efficiency of mutation analysis. This new method significantly reduces the time overhead for generating and executing the mutants. Results are reported for eight investigated programs up to 373,000 lines of code and 406,000 generated mutants. Furthermore, conditional mutation has been integrated into the Java 6 Standard Edition compiler. Thus, it is widely applicable and not limited to a certain testing tool or framework."
},
{
  "Title": "Better Predicate Testing",
  "Type": "Paper",
  "Key": "05ce481140e4760cdf566e16246d64",
  "Authors": ["Gary Kaminski", "Paul Ammann", "Jeff Offutt"],
  "Affiliations": ["George Mason University, USA"],
  "Abstract": "Mutation testing is widely recognized as being extremely powerful, but is considered difficult to automate enough for practical use. This paper theoretically addresses two possible reasons for this: the generation of redundant mutants and the lack of integration of mutation analysis with other test criteria. By addressing these two issues, this paper brings an important mutation operator, relational-operator-replacement (ROR), closer to practical use. First, we develop fault hierarchies for the six relational operators, each of which generates seven mutants per clause. These hierarchies show that, for any given clause, only three mutants are necessary. This theoretical result can be integrated easily into mutation analysis tools, thereby eliminating generation of 57% of the ROR mutants. Second, we show how to bring the power of the ROR operator to the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. This theoretical result includes an algorithm to transform any MCDC-adequate test set into a test set that also satisfies RORG, a new version of ROR appropriate for the MCDC context. The transformation does not use traditional mutation analysis, so can easily be integrated into existing MCDC tools and processes."
},
{
  "Title": "Hazard-based Selection of Test Cases",
  "Type": "Paper",
  "Key": "65f06da1fbf3796c5288697c3b82fc",
  "Authors": ["Mario Gleirscher"],
  "Affiliations": ["TU München, Germany"],
  "Abstract": "It is a challenge to engineer and select admissible test cases. This work approaches selection based on hazardous states through integrated modelling of the environment and the system, and by integration of requirements engineering activities into the tasks of system test engineering. The elaboration of test models and test case specifications focuses automation in system verification in order to detect hazard-oriented failures as well as in requirements validation in order to disclose potential defects in specification artefacts."
},
{
  "Title": "Automated Testing of Industrial Control Devices: The Delphi Database",
  "Type": "Paper",
  "Key": "520843123f4c88272ad38ffadef552",
  "Authors": ["Nate Kube", "Kevin Yoo", "Daniel Hoffman"],
  "Affiliations": ["Wurldtech Security Technologies, Canada", "University of Victoria, Canada"],
  "Abstract": "Delphi is a database designed to centralize and distribute current industrial automation vulnerability information. Over the past two years, with the aid of many of the world’s largest equipment vendors and operators, we have populated this database through extensive testing of industrial control devices. Delphi stores over 500 vulnerabilities on 31 popular distributed control system and safety instrumented system controllers. There are two primary reasons why Delphi data is useful: to distribute vulnerability data and to provide mitigation strategies. With detailed knowledge of which vulnerabilities are present, vendors can produce more robust devices and operators can construct business cases and calculate return-on-investment when considering investments in security measures. Furthermore, known vulnerabilities can be mitigated without applying device patches and shutting down plant operations."
},
{
  "Title": "Automating GUI Testing for Android Applications",
  "Type": "Paper",
  "Key": "c65aebc32ad2498b49257db44204fe",
  "Authors": ["Cuixiong Hu", "Iulian Neamtiu"],
  "Affiliations": ["UC Riverside, USA"],
  "Abstract": "Users increasingly rely on mobile applications for computational needs. Google Android is a popular mobile platform, hence the reliability of Android applications is becoming increasingly important. Many Android correctness issues, however, fall outside the scope of traditional verification techniques, as they are due to the novelty of the platform and its GUI-oriented application construction paradigm. In this paper we present an approach for automating the testing process for Android applications, with a focus on GUI bugs. We first conduct a bug mining study to understand the nature and frequency of bugs affecting Android applications; our study finds that GUI bugs are quite numerous. Next, we present techniques for detecting GUI bugs by automatic generation of test cases, feeding the application random events, instrumenting the VM, producing log/trace files and analyzing them post-run. We show how these techniques helped to re-discover existing bugs and find new bugs, and how they could be used to prevent certain bug categories. We believe our study and techniques have the potential to help developers increase the quality of Android applications."
},
{
  "Title": "Selection and Execution of User Level Test Cases for Energy Cost Evaluation of Smartphones",
  "Type": "Paper",
  "Key": "8c0802a992b909098bde351e898643",
  "Authors": ["Rajesh Palit", "Renuka Arya", "Kshirasagar Naik", "Ajit Singh"],
  "Affiliations": ["University of Waterloo, Canada"],
  "Abstract": "Smartphones are emerging as a preferred communication device of users. In this paper, we provide a methodology to select user level test cases for performing energy cost evaluation of smartphone applications. We define the concept of a user level test case for smartphones and show that, due to conﬁguration settings, there exist millions of such test cases. Next, we discuss a test selection technique to reduce the number of test cases. We apply the technique to ﬁve different smartphones and evaluate their energy costs for running common network related applications. We have developed a test bench to execute those test cases for real applications on smartphones and measure their actual energy costs. This work provides a framework for researchers and developers to conduct experiments for measuring the energy cost of applications on smartphones."
},
{
  "Title": "Testing an Optimising Compiler by Generating Random Lambda Terms",
  "Type": "Paper",
  "Key": "1424c5abefad16a02e820470c7475c",
  "Authors": ["Michał H. Pałka", "Koen Claessen", "Alejandro Russo", "John Hughes"],
  "Affiliations": ["Chalmers University of Technology, Sweden", "Quviq AB, Sweden"],
  "Abstract": "This paper considers random testing of a compiler, using randomly generated programs as inputs, and comparing their behaviour with and without optimisation. Since the generated programs must compile, then we need to take into account syntax, scope rules, and type checking during our random generation. Doing so, while attaining a good distribution of test data, proves surprisingly subtle; the main contribution of this paper is a workable solution to this problem. We used it to generate typed functions on lists, which we compiled using the Glasgow Haskell compiler, a mature production quality Haskell compiler. After around 20,000 tests we triggered an optimiser failure, and automatically simplified it to a program with just a few constructs."
},
{
  "Title": "Model-Driven Design and Validation of Embedded Software",
  "Type": "Paper",
  "Key": "6945f3f16fed06d030141f48f3efe4",
  "Authors": ["Giuseppe Di Guglielmo", "Masahiro Fujita", "Luigi Di Guglielmo", "Franco Fummi", "Graziano Pravadelli", "Cristina Marconcini", "Andreas Foltinek"],
  "Affiliations": ["The University of Tokyo, Japan", "University of Verona, Italy", "STM Products srl, Italy", "IMACS GmbH, Germany"],
  "Abstract": "This paper presents a model-based framework for designing and validating embedded software (ESW). The design infrastructure is a rapid-application-development suite for ESW, i.e., radCASE, which provides the user with an off the shelf designing environment based on model-driven paradigm. The validation infrastructure, i.e., radCHECK, is based on Property Editor. Such an editor simplifies the definition of PSL properties by exploiting PSL-based templates, that can be automatically compiled into executable checkers by using the integrated Checker Generator engine. Besides, radCHECK comprises a testcase generation infrastructure, i.e., Ulisse, which is based on an corner-case-oriented concolic approach for ESW, thus it is able to simulate the ESW and the checkers by using efficient testcases."
},
{
  "Title": "Test Data to Reduce the Complexity of Unit Test Automation",
  "Type": "Paper",
  "Key": "a670e2439bec74c93310b27b8d67b8",
  "Authors": ["Guy Collins Ndem", "Abbas Tahir", "Andreas Ulrich", "Helmut Goetz"],
  "Affiliations": ["Siemens AG, Germany"],
  "Abstract": "In order to automatically test large and complex software systems, a well defined test process and a huge amount of test data are needed. When it comes to test automation, the quality of test data always plays a significant role by reducing the cost and the time required for the test activities. In this work, we follow the principle of \"Quality of the test cases comes before Quantity\" to show how the right selection of test data helps to address the goal of qualitative testing while optimizing the test effort needed"
},
{
  "Title": "A Framework for Automatic Functional Testing Based on Formal Specifications",
  "Type": "Paper",
  "Key": "9729767f9f5bffb69c6df5e5698690",
  "Authors": ["Shaoying Liu", "Shin Nakajima"],
  "Affiliations": ["Hosei University, Japan", "NII, Japan"],
  "Abstract": "This paper describes a framework for automatic functional testing based on formal specifications. The framework is intended to expose all of the issues that must be addressed in automatic testing, to provide solutions for some of the major challenges, and to serve as a foundation for developing tool support in the future."
},
{
  "Title": "Towards Automated Testing of Web Service Choreographies",
  "Type": "Paper",
  "Key": "634e1eeaf1908199776d90738a7873",
  "Authors": ["Felipe M. Besson", "Pedro M. B. Leal", "Fabio Kon", "Alfredo Goldman", "Dejan Milojicic"],
  "Affiliations": ["University of São Paulo, Brazil", "Hewlett Packard Laboratories, USA"],
  "Abstract": "Web service choreographies have been proposed as a decentralized scalable way of composing services in a SOA environment. In spite of all the benefits of choreographies, the decentralized flow of information, the parallelism, and multiple party communication restrict the automated testing of choreographies at design and runtime. The goal of our research is to adapt the automated testing techniques used by the Agile Software Development community to the SOA context. To achieve that, we seek to develop software tools and a methodology to enable test-driven development of Choreographies. In this paper, we present our first step in that direction, a software prototype composed of ad hoc automated test case scripts for testing a web service choreography."
},
{
  "Title": "monadWS: A Monad-Based Testing Tool for Web Services",
  "Type": "Paper",
  "Key": "2bab594ee45a2cd550551456dad25c",
  "Authors": ["Yingzhou Zhang", "Wei Fu", "Changhai Nie"],
  "Affiliations": ["Nanjing University of Posts and Telecommunications, China", "Nanjing University, China"],
  "Abstract": "Testing Web Services (WS) is very important because it could guarantee a high degree of service quality and reliability. Automatic WS testing tools enable testers to complete testing in a shorter time, and they make it easy to repeat tests after each modification to a service. In this paper, we present monadWS, a prototype tool of monad-based automated testing for WS, which enables concrete description, autogeneration and execution for WS test cases with the design of service testing monads. The results show that monadWS is feasible and effective for testing WS."
},
{
  "Title": "Towards Automated Oracles for GUI Input Validation",
  "Type": "Paper",
  "Key": "f9303c3ab4e69a6d1cc49ab71bd4cc",
  "Authors": ["Gabriel L. Zenarosa", "Regis J. Leonard"],
  "Affiliations": ["University of Pittsburgh, USA"],
  "Abstract": "Testing input validation in web applications from specifications is a challenging and laborious process. GUI testing tools---with their record-and-playback and data-driven capabilities---ease the pains of testing through automation. Out-of-the-box, however, these tools have some scaling limitations as setup costs are incurred for every distinct web application to test. In environments where a line of many web applications are regularly created for various customers and purposes, scaling the test automation to span the entire product line is extremely valuable. In this paper, we report on our experience in generalizing the automatic specification-based testing of input validation in a line of web applications. Our approach is based on a nonstandard use of a GUI testing tool enabled by adjustments to coding standards and the requirements specification writing process."
},
{
  "Title": "A Case Study of Post-Deployment User Feedback Triage",
  "Type": "Paper",
  "Key": "90945d17f9e1bf3dfe1d7080f7033e",
  "Authors": ["Andrew J. Ko", "Michael J. Lee", "Valentina Ferrari", "Steven Ip", "Charlie Tran"],
  "Affiliations": ["University of Washington, USA"],
  "Abstract": "Many software requirements are identified only after a product is deployed, once users have had a chance to try the software and provide feedback. Unfortunately, addressing such feedback is not always straightforward, even when a team is fully invested in user-centered design. To investigate what constrains a teams evolution decisions, we performed a 6-month field study of a team employing iterative user-centered design methods to the design, deployment and evolution of a web application for a university community. Across interviews with the team, analyses of their bug reports, and further interviews with both users and non-adopters of the application, we found most of the constraints on addressing user feedback emerged from conflicts between users heterogeneous use of information and inflexible assumptions in the team’s software architecture derived from earlier user research. These findings highlight the need for new approaches to expressing and validating assumptions from user research as software evolves."
},
{
  "Title": "Branching and Merging: An Investigation into Current Version Control Practices",
  "Type": "Paper",
  "Key": "fee728623696ee3be981f514b20409",
  "Authors": ["Shaun Phillips", "Jonathan Sillito", "Rob Walker"],
  "Affiliations": ["University of Calgary, Canada"],
  "Abstract": "The use of version control has become ubiquitous in software development projects. Version control systems facilitate parallel development and maintenance through branching, the creation of isolated codelines. Merging is a consequence of branching and is the process of integrating codelines. However, there are unanswered questions about the use of version control to support parallel development; in particular, how are branching and merging used in practice? What defines a successful branching and merging strategy? As a first step towards answering these questions, we recruited a diverse sample of 140 version control users to participate in an online survey. In this paper, we present the survey results and 4 key observations about branching and merging practices in software development projects."
},
{
  "Title": "A Qualitative Study of the Determinants of Self-managing Team Effectiveness in a Scrum Team",
  "Type": "Paper",
  "Key": "ce8f846a9f0cd75f65dd4e77db501b",
  "Authors": ["Cleviton V. F. Monteiro", "Fabio Q. B. da Silva", "Isabella R. M. dos Santos", "Felipe Farias", "Elisa S. F. Cardozo", "André R. G. do A. Leitão", "Dacio N. M. Neto", "Miguel J. A. Pernambuco Filho"],
  "Affiliations": ["Universidade Federal de Pernambuco, Brazil"],
  "Abstract": "There are many evidences in the literature that the use self-managing teams has positive impacts on several dimensions of team effectiveness. Agile methods, supported by the Agile Manifesto, defend the use of self-managing teams in software development in substitution of hierarchically managed, traditional teams. The goal of this research was to study how a self-managing software team works in practice and how the behaviors of the software organization support or hinder the effectiveness of such teams. We performed a single case holistic case study, looking in depth into the actual behavior of a mature Scrum team in industry. Using interviews and participant observation, we collected qualitative data from five team members in several interactions. We extract the behavior of the team and of the software company in terms of the determinants of self-managing team effectiveness defined in a theoretical model from the literature. We found evidence that 17 out of 24 determinants of this model exist in the studied context. We concluded that certain determinants can support or facilitate the adoption of methodologies like Scrum, while the use of Scrum may affect other determinants."
},
{
  "Title": "Mining and Visualizing Developer Networks from Version Control Systems",
  "Type": "Paper",
  "Key": "8266379404a94d50370ce184016cee",
  "Authors": ["Andrejs Jermakovics", "Alberto Sillitti", "Giancarlo Succi"],
  "Affiliations": ["Free University of Bozen, Italy"],
  "Abstract": "Social network analysis has many applications in software engineering and is often performed through the use of visualizations. Discovery of these networks, however, presents a challenge since the relationships are initially not known. We present an approach for mining and visualizing networks of software developers from version control systems. It computes similarities among developers based on common file changes, constructs the network of collaborating developers and applies filtering techniques to improve the modularity of the network. We validate the approach on two projects from industry and demonstrate its use in a case study of an open-source project. Results indicate that the approach performs well in revealing the structure of development teams and improving the modularity in visualizations of developer networks."
},
{
  "Title": "How Do We Trace Requirements? An Initial Study of Analyst Behavior in Trace Validation Tasks",
  "Type": "Paper",
  "Key": "846609acb5555fa22c69a1c8069500",
  "Authors": ["Wei-Keat Kong", "Jane Huffman Hayes", "Alex Dekhtyar", "Jeff Holden"],
  "Affiliations": ["University of Kentucky, USA", "California Polytechnic State University, USA"],
  "Abstract": "Traceability recovery is a tedious, error-prone, person-power intensive task, even if aided by automated traceability tools. Human analysts must vet candidate traceability links retrieved by such tools and must often go looking for links that such tools fail to locate as they build a traceability matrix. This paper examines a research version of the traceability tool REquirements TRacing On target (RETRO) that logs analyst actions. We examine the user logs in order to understand how analysts work on traceability recovery tasks. Such information is a pre-requisite to understanding how to better design traceability tools to best utilize analyst time while developing a high quality final traceability matrix."
},
{
  "Title": "Impact of Collaborative Traces on Trustworthiness",
  "Type": "Paper",
  "Key": "45d2ec5236275ecd117bcb8e4df7c7",
  "Authors": ["Erik H. Trainer", "Ban Al-Ani", "David F. Redmiles"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "We investigated how trust among software developers would be affected by providing them with visualizations of collaborative traces. We define collaborative traces to be representations of the past and current activity of a group of developers manipulating software development artifacts. In this paper, we report two main findings. First, we report the results of our controlled experiment in which collaborative traces were visualized. Second, we present an overview of tools which aim to represent collaborative software engineering traces. Our experiment provides evidence that collaborative traces can support the development of several factors of trust identified in our field study. However, we also identified some shortcomings of our current visualizations, gaining insights into future improvements. From our review of tools that represent collaborative traces, we observed that such representations can drive the design of tools that aim to support trust. We also present a table of tools; the table can be used to guide discussion and the design of tools that promote trust in software development."
},
{
  "Title": "Some Non-Usage Data for a Distributed Editor: The Saros Outreach",
  "Type": "Paper",
  "Key": "ae1de5cd7dc98d80415a91629c5ec0",
  "Authors": ["Lutz Prechelt"],
  "Affiliations": ["Freie Universität Berlin, Germany"],
  "Abstract": "After contacting more than 40 companies and 11 OSS projects regarding using our distributed editor Saros, we find that almost all of those many who have a use case for it, are reluctant to even try it out. It appears that distance matters even by anticipatory obedience."
},
{
  "Title": "Strawberries are Nuts",
  "Type": "Paper",
  "Key": "f84f1221df2fca511449fcf7b7d52a",
  "Authors": ["Tim Frey", "Marius Gelhausen"],
  "Affiliations": ["Otto-von-Guericke University Magdeburg, Germany", "TU Darmstadt, Germany"],
  "Abstract": "Separation of concerns is a central element for program comprehension. This note briefly explains why human categorization can be interesting for program comprehension."
},
{
  "Title": "Of Code and Context: Collaboration Between Developers and Translators",
  "Type": "Paper",
  "Key": "a0596ae1da4c090cf8010914f3ce08",
  "Authors": ["Malte Ressin", "José Abdelnour-Nocera", "Andy Smith"],
  "Affiliations": ["Thames Valley University, UK"],
  "Abstract": "Software for international markets often requires cultural adaption in order to be successful in different markets. To achieve this, software developers work together with translators to internationalize and localize their product as necessary. In this paper, we reflect on our experiences of collaboration between these two specialist types from different disciplines. We contrast the differences in object of work, education, values and perception of product quality and illustrate what other factors might have an influence in collaboration. Our experiences suggest that the collaboration between developers and localizers might be improved by integrating translators into development teams, and by emphasizing the importance of understanding each other’s work better."
},
{
  "Title": "A Theory of Branches as Goals and Virtual Teams",
  "Type": "Paper",
  "Key": "2f01cfceb7615307cc4fc5f920ee04",
  "Authors": ["Christian Bird", "Thomas Zimmermann", "Alex Teterev"],
  "Affiliations": ["Microsoft Research, USA", "Microsoft, USA"],
  "Abstract": "A common method of managing the complexity of both technical and organizational relationships in a large software project is to use branches within the source code management system to partition the work into teams and tasks. We claim that the files modified on a branch are changed together in a cohesive way to accomplish some task such as adding a feature, fixing a related set of bugs, or implementing a subsystem, which we collectively refer to as the goal of the branch. Further, the developers that work on a branch represent a virtual team. In this paper, we develop a theory of the relationship between goals and virtual teams on different branches. Due to expertise, ownership, and awareness concerns, we expect that if two branches have similar goals, they will also have similar virtual teams or be at risk for communication and coordination breakdowns with the accompanying negative effects. In contrast, we do not expect the converse to always be true. In the first step towards an actionable result, we have evaluated this theory empirically on two releases of the Windows operating system and found support in both."
},
{
  "Title": "Workplace Warriors: Identifying Team Practices of Appropriation in Software Ecosystems",
  "Type": "Paper",
  "Key": "ba1df3adbda4beb8476be9177674d0",
  "Authors": ["Sebastian Draxler", "Adrian Jung", "Alexander Boden", "Gunnar Stevens"],
  "Affiliations": ["University of Siegen, Germany"],
  "Abstract": "Since the 1990s, the forms of production, distribution, configuration and appropriation of software have changed fundamentally. Nowadays, software is often embedded in software ecosystems, i.e. in complex interrelations between different stakeholders who are connected by a shared technological platform. In our paper, we investigate how small software teams deal with the challenges of appropriating and configuring software in the Eclipse ecosystem for their daily work. We empirically identify three different approaches for dealing with appropriation in software ecosystems which are represented by the “ideal types” lone warrior, centralized organization, and collegial collaboration. Based on a discussion of these strategies and the underlying appropriation practices we found in the field, we suggest theoretical and practical implications for supporting appropriation in software ecosystems."
},
{
  "Title": "STCML: An Extensible XML-based Language for Socio-Technical Modeling",
  "Type": "Paper",
  "Key": "905b27aee54bd5f9f0f6292928ad38",
  "Authors": ["John C. Georgas", "Anita Sarma"],
  "Affiliations": ["Northern Arizona University, USA", "University of Nebraska-Lincoln, USA"],
  "Abstract": "Understanding the complex dependencies between the technical artifacts of software engineering and the social processes involved in their development has the potential to improve the processes we use to engineer software as well as the eventual quality of the systems we produce. A foundational capability in grounding this study of socio-technical concerns is the ability to explicitly model technical and social artifacts as well as the dependencies between them. This paper presents the STCML language, intended to support the modeling of core socio-technical aspects in softwareof development by providing core representational support in a highly extensible fashion. We present the core basic structure of the language, discuss important language design principles, and offer an example of its application."
},
{
  "Title": "Collabode: Collaborative Coding in the Browser",
  "Type": "Paper",
  "Key": "c44f602b798a598ef3d5d498be7e9b",
  "Authors": ["Max Goldman", "Greg Little", "Robert C. Miller"],
  "Affiliations": ["Massachusetts Institute of Technology, USA"],
  "Abstract": "Collaborating programmers should use a development environment designed specifically for collaboration, not the same one designed for solo programmers with a few collaborative processes and tools tacked on. This paper describes Collabode, a web-based Java integrated development environment built to support close, synchronous collaboration between programmers. We discuss three collaboration models in which participants take on distinct roles: micro-outsourcing to combine small contributions from many assistants; test-driven pair programming for effective pairwise development; and a mobile instructor connected to the work of many students. In particular, we report very promising preliminary results using Collabode to support micro-outsourcing."
},
{
  "Title": "TeamBugs: A Collaborative Bug Tracking Tool",
  "Type": "Paper",
  "Key": "ea59b1a01a2dda1f361251807cdc9d",
  "Authors": ["Gerald Bortis", "André van der Hoek"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "Bug trackers are considered to be one of the primary coordination tools in software development. Interactions with the bug tracker occur throughout the development process, in different settings, from the meeting room to the developer's desk. In this paper, we look at previous field studies of bug trackers and identify the key challenges in supporting coordination. We then describe through several scenarios TeamBugs, a new bug tracker tool. We conclude by raising several research questions for future work."
},
{
  "Title": "Studying Team Evolution during Software Testing",
  "Type": "Paper",
  "Key": "13686072b6a501a85e28d945fc9d39",
  "Authors": ["Vibhu Saujanya Sharma", "Vikrant Kaulgud"],
  "Affiliations": ["Accenture Technology Labs, India"],
  "Abstract": "Software development teams are one of the most dynamic entities of any software development project. While the individuals are assigned planned roles at the start of any project, during the course of the project, the team constitution, structure, relationships and roles change. Such changes are often spontaneous and constitute the evolution of the team along different phases of the software development lifecycle. As software development is a team effort, these dynamics may have a significant effect on the development lifecycle itself. This work is aimed at studying the evolution of project teams and gathering insights that can be correlated with project health and outcomes. In this study we apply social network analysis techniques to investigate team evolution in a project in its testing phase. While the questions and insights that we investigate in this paper are valid and useful for all phases of the software development lifecycle, we have focused on software testing phase as it one of the most critical phases in the lifecycle. Our work aims to provide insights in the changes in team interactions and individual roles as the testing process continues and can help find if the same is aligned to the planned and desired project behavior."
},
{
  "Title": "Which Bug Should I Fix: Helping New Developers Onboard a New Project",
  "Type": "Paper",
  "Key": "f8b6d5464336a95ba2e63c8ab24070",
  "Authors": ["Jianguo Wang", "Anita Sarma"],
  "Affiliations": ["University of Nebraska-Lincoln, USA"],
  "Abstract": "A typical entry point for new developers in an open source project is to contribute a bug fix. However, finding an appropriate bug and an appropriate fix for that bug requires a good understanding of the project, which is nontrivial. Here, we extend Tesseract – an interactive project exploration environment – to allow new developers to search over bug descriptions in a project to quickly identify and explore bugs of interest and their related resources. More specifically, we extended Tesseract with search capabilities that enable synonyms and similar-bugs search over bug descriptions in a bug repository. The goal is to enable users to identify bugs of interest, resources related to that bug, (e.g., related files, contributing developers, communication records), and visually explore the appropriate socio-technical dependencies for the selected bug in an interactive manner. Here we present our search extension to Tesseract."
},
{
  "Title": "An Exploratory Study of Awareness Interests about Software Modifications",
  "Type": "Paper",
  "Key": "71dd2a3352efd8b43b78d1b2af7103",
  "Authors": ["Miryung Kim"],
  "Affiliations": ["University of Texas at Austin, USA"],
  "Abstract": "As software engineers collaboratively develop software, they need to often analyze past and present program modifications implemented by other developers. While several techniques focus on tool support for investigating past and present software modifications, do these techniques indeed address developers' awareness interests that are important to them? We conducted an initial focus group study and a web survey to understand in which task contexts and how often particular types of awareness-interests arise. Our preliminary study results indicate that developers have daily information needs about code changes that affect or interfere with their code, yet it is extremely challenging for them to identify relevant events out of a large number of change-events."
},
{
  "Title": "Supporting Collaboration in the Development of Complex Engineering Software",
  "Type": "Paper",
  "Key": "e03c9dae6e07f88f7e05fa6a954db7",
  "Authors": ["Victoria Shipp", "Peter Johnson"],
  "Affiliations": ["University of Bath, UK"],
  "Abstract": "Software development in engineering firms is a prominent and vital activity, with the success of the business often being dependent on the tools being used. Developing this software requires collaboration between a number of stakeholders, including end-users and other software teams who are often remotely located. This research has used an ethnographic approach to studying communications between stakeholders involved in the development of this type of software. Findings show that users play a vital role in the development team due to their knowledge of the domain and work processes. Supporting this relationship remotely can be challenging, especially due to the reliance on ad-hoc communication strategies. This can sometimes lead to misunderstandings, design rationale being lost, and poor and efficient designs and processes. Lightweight tools that enable flexible design artefacts to be shared and discussed could assist this process and will be investigated in future work."
},
{
  "Title": "Requirements Maturation Analysis based on the Distance between the Source and Developers",
  "Type": "Paper",
  "Key": "5f4f908857204722a240ce7f4af474",
  "Authors": ["Takako Nakatani", "Toshihiko Tsumaki"],
  "Affiliations": ["University of Tsukuba, Japan", "National Institute of Informatics, Japan"],
  "Abstract": "The success of a project is often affected by imperfect requirements. In order to cope with this risk, a requirements analyst needs to communicate with a client. However, communication between the requirements analyst and the client is not enough to prevent requirements imperfection, since requirements come from various sources, e.g. environment, laws, documents, actual usage, etc. The process of requirements elicitation is affected by the requirements stability, the ability of a requirements analyst, and accessibility of the source of requirements. This paper focuses on the distance between the source of requirements and a requirements analyst, and clarifies how the distance influences the requirements maturation. Requirements maturation represents the degree to which the requirements are elicited completely. We define a measure for observing requirements maturation and analyzing the accessibility of the source of the requirements. Then, we define a hypothesis. A case is analyzed in order to verify the hypothesis. As a result, there is a correlation between the requirements maturation efficiency and the accessibility of the source of the requirements."
},
{
  "Title": "Teaching a Global Project Course: Experiences and Lessons Learned",
  "Type": "Paper",
  "Key": "8d82fbf96cff50d6a2394bd2896794",
  "Authors": ["Peter Gloor", "Maria Paasivaara", "Casper Lassenius", "Detlef Schoder", "Kai Fischbach", "Christine Miller"],
  "Affiliations": ["MIT, USA", "Aalto University, Finland", "University of Cologne, Germany", "SCAD, USA"],
  "Abstract": "In this paper, we describe the goals, organization and content of a global project course we have taught for the last six years, as well as challenges and lessons learned. The course has involved two to four sites and 30-40 students each year, both from Europe and the US. The students form project teams spanning several sites, and jointly perform creative tasks, thus learning both the course substance, as well as how to effectively work together in multicultural and multi-disciplinary distributed teams. We hope that our experiences described in this paper will help and encourage other universities to organize globally distributed project courses. In the future, we plan to continue working with this course, as well as search partners to develop a global software engineering project course together with other universities."
},
{
  "Title": "Developing a Pedagogical Infrastructure for Teaching Globally Distributed Software Development",
  "Type": "Paper",
  "Key": "f0679012fba495f014180ca60d6020",
  "Authors": ["Ed Keenan", "Adam Steele"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "Teaching students about Globally Distributed Software Development (GDSD) is becoming increasingly important as a significant percentage of current projects are being developed by globally distributed teams. We discuss the pedagogical infrastructure used to teach GDSD at DePaul University and its partner institutions. We cover the educational, technical and institutional challenges that need to be resolved in order to successfully partner with globally distributed teaching institutions in order to teach GDSD in a realistic environment."
},
{
  "Title": "An Evolving Collaborative Model of Working in Students' Global Software Development Projects",
  "Type": "Paper",
  "Key": "beeb4cc9f727b1dc2865ff54b61c28",
  "Authors": ["Christelle Scharff"],
  "Affiliations": ["Pace University, USA"],
  "Abstract": "This paper describes the main points of the evolution of the collaborative model of working of our annual global software development project that started in 2005. The global software development project unites students from up to five countries with different roles to work on the development of software together. This paper briefly summarizes the refinements of the focus, process, and tooling used on the project. Some of the findings are succinctly presented as well as our future steps."
},
{
  "Title": "Can Distributed Software Development Help the Practitioners to Become Better Software Engineers? Insights from Academia",
  "Type": "Paper",
  "Key": "79ad65c910993d3ffeb67766baac07",
  "Authors": ["Rafael Prikladnicki"],
  "Affiliations": ["PUCRS, Brazil"],
  "Abstract": "Distributed software development is a reality. While this area is increasing, it is also necessary to prepare software development professionals to develop software in distributed environments. With the evolution of this area, it has been observed that the experience acquired in DSD projects can help to improve the performance in traditional projects as well. In this context, the purpose of this paper is to explore in what extent the experience with DSD can improve the performance of software development professionals, even in collocated environments. Data was collected based on the evaluation of a DSD teaching experience in a course taught in Brazil. The preliminary results indicate that experiences lived in a distributed context could help software engineers to become better practitioners."
},
{
  "Title": "Teaching Distributed Software Engineering with UCOSP: The Undergraduate Capstone Open-Source Project",
  "Type": "Paper",
  "Key": "5d78236fe6930f08b0f84928543436",
  "Authors": ["Eleni Stroulia", "Ken Bauer", "Michelle Craig", "Karen Reid", "Greg Wilson"],
  "Affiliations": ["University of Alberta, Canada", "University of Toronto, Canada"],
  "Abstract": "Software engineering courses in computer-science departments are meant to prepare students for the practice of designing, developing, understanding and maintaining software in the real world. The effectiveness of these courses have potentially a tremendous impact on the software industry, since it is through these courses that students must learn the state-of-the-art process and the tools of their eventual \"trade\", so that they can bring this knowledge to their job and thus advance the actual state of practice. The value of \"learning software engineering\" through project-based courses has long been recognized by educators and practitioners alike. In this paper, we discuss our experience with a distributed project-based course, which infuses the students' learning experience with an increased degree of realism, which, we believe, further improves the quality of their learning and advances their readiness to join the profession."
},
{
  "Title": "Avoiding Scylla and Charybdis in Distributed Software Development Course",
  "Type": "Paper",
  "Key": "c871e69807cf4bc359aeb265142079",
  "Authors": ["Ivana Bosnić", "Igor Čavrak", "Marin Orlić", "Mario Žagar", "Ivica Crnković"],
  "Affiliations": ["University of Zagreb, Croatia", "Mälardalen University, Sweden"],
  "Abstract": "Teaching Distributed Software Development (DSD) is a challenging task. A convincing simulation of distributed environment in a local environment is practically impossible. Teaching DSD in distributed environment is more realistic since the students directly experience all its specifics. However, teaching in distributed environment, in which several geographically separated teams participate, is very demanding. Different types of obstacles occur, from administrative and organizational to technical ones. This paper describes some of the challenges, lessons learned, but also success stories of the DSD course performed now eight year in a row."
},
{
  "Title": "Student Motivation in Distributed Software Development Projects",
  "Type": "Paper",
  "Key": "5aa93d09ace202e1200f72029c6b34",
  "Authors": ["Ivana Bosnić", "Igor Čavrak", "Marin Orlić", "Mario Žagar", "Ivica Crnković"],
  "Affiliations": ["University of Zagreb, Croatia", "Mälardalen University, Sweden"],
  "Abstract": "In this paper we discuss challenges faced in conducting distributed student projects within a scope of a distributed software development university course. Student motivation and demotivation factors, along with perceived cultural differences, are identified and analyzed on the basis of data collected from a number of student projects."
},
{
  "Title": "Teaching Software Engineering using Globally Distributed Projects: The DOSE Course",
  "Type": "Paper",
  "Key": "3370a71eb746d26d53eb8c3c946578",
  "Authors": ["Martin Nordio", "Carlo Ghezzi", "Bertrand Meyer", "Elisabetta Di Nitto", "Giordano Tamburrelli", "Julian Tschannen", "Nazareno Aguirre", "Vidya Kulkarni"],
  "Affiliations": ["ETH Zurich, Switzerland", "Politecnico di Milano, Italy", "University of Rio Cuarto, Argentina", "University of Delhi, India"],
  "Abstract": "Distributed software development poses new software engineering challenges. To prepare student for these new challenges, we have been teaching software engineering using globally distributed projects. The projects were developed in collaboration with eleven universities in ten different countries in Europe, Asia, and South America. This paper reports the experience teaching the course, describing the settings, problems faced organizing the projects and the lessons learned."
},
{
  "Title": "The Whats and the Whys of Games and Software Engineering",
  "Type": "Paper",
  "Key": "72d88d9b171f38b7b9f9e29961e663",
  "Authors": ["Chris Lewis", "Jim Whitehead"],
  "Affiliations": ["UC Santa Cruz, USA"],
  "Abstract": "The intersection of video games and software engineering is not yet well understood. This paper highlights the varied and exciting opportunities available at the intersection of these two disciplines. We investigate four main areas: the development of games, how they are designed, how middleware supports the creative process and how games are tested. We hope that it inspires readers to take on the challenges available in games and software engineering, and join together to create a vibrant community."
},
{
  "Title": "Modding as a Basis for Developing Game Systems",
  "Type": "Paper",
  "Key": "f58925fffe7582df1e0ef7b09788bd",
  "Authors": ["Walt Scacchi"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "This paper seeks to briefly examine what is known so far about game mods and modding practices. Game modding has become a leading method for developing games by customizing extensions to game software. The research method in this study is observational and qualitative, so as to highlight current practices and issues that can be associated with software engineering foundations. Numerous examples of different game mods and modding practices are identified throughout."
},
{
  "Title": "Software Engineering Senior Design Course: Experiences with Agile Game Development in a Capstone Project",
  "Type": "Paper",
  "Key": "f5bb91d83b03bd637b77d4f3275de5",
  "Authors": ["Tucker Smith", "Kendra M. L. Cooper", "C. Shaun Longstreet"],
  "Affiliations": ["The University of Texas at Dallas, USA"],
  "Abstract": "The importance of capstone senior design project courses is widely recognized for undergraduate software engineering curricula. They provide students with an opportunity to integrate and apply theoretical knowledge (both from previous courses and newly acquired for the project) on a team, improving both their technical and soft-skills. Here, we report our experiences using an agile development method for a game project; this is a radical shift from our previous course offerings that were based on waterfall, model driven development. This report is unique and valuable, especially for software engineering education, which goes beyond the discipline-specific limits of computer science curricula."
},
{
  "Title": "The Benefits and Challenges of Using Educational Game Projects in an Undergraduate Software Engineering Course",
  "Type": "Paper",
  "Key": "a9bea8cdfe3ec16dab6eca28e7af13",
  "Authors": ["Stephanie Ludi"],
  "Affiliations": ["Rochester Institute of Technology, USA"],
  "Abstract": "Devising team projects for an introductory software engineering course can be a challenge for educators. A balance between an engaging project that is complex enough for the team to complete in the timeframe of the course is required. This paper describes the experiences of using an educational game as the team project in a 10-week introductory software engineering course for second-year undergraduates. The motivation for the project and the project itself will be presented. Benefits of using a game project existing, but the challenges of using a game project were perceived to be high by some instructors. The reflection of the experience is meant to help other instructors to find the right fit for a game project in their introductory course."
},
{
  "Title": "Verified Gaming",
  "Type": "Paper",
  "Key": "3a811da800f9e5dbdf707f9c9e9a9d",
  "Authors": ["Joseph R. Kiniry", "Daniel M. Zimmerman"],
  "Affiliations": ["IT University of Copenhagen, Denmark", "University of Washington at Tacoma, USA"],
  "Abstract": "In recent years, several Grand Challenges (GCs) of computing have been identified and expounded upon by various professional organizations in the U.S. and England. These GCs are typically very difficult problems that will take many hundreds, or perhaps thousands, of man-years to solve. Researchers involved in identifying these problems are not going to solve them. That task will fall to our students, and our students' students. Unfortunately for GC6, the Grand Challenge focusing on Dependable Systems Evolution, interest in formal methods--both by students and within computer science faculties--falls every year and any mention of mathematics in the classroom seems to frighten students away. So the question is: How do we attract new students in computing to the area of dependable software systems? \nOver the past several years at three universities we have experimented with the use of computer games as a target domain for software engineering project courses that focus on reliable systems engineering. This position paper summarizes our experiences in incorporating rigorous software engineering into courses with computer game projects."
},
{
  "Title": "Exploring Game Architecture Best-Practices with Classic Space Invaders",
  "Type": "Paper",
  "Key": "dd1a48b519ff16423754db03506b9c",
  "Authors": ["Ed Keenan", "Adam Steele"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "The classic arcade game Space Invaders provides an ideal environment for students to learn about best practices in game software architectures. We discuss the challenges of creating a good game architecture, and show how our problem space is an ideal environment in which to experiment with the challenges and tradeoffs inherent in any software design. We discuss in detail how each student created and engineered their game using good architectural design principles in general and gang-of-four design patterns in particular."
},
{
  "Title": "Toward High-Level Reuse of Statechart-based AI in Computer Games",
  "Type": "Paper",
  "Key": "07ba3e97e30453bcf41bc8eea60259",
  "Authors": ["Christopher Dragert", "Jörg Kienzle", "Clark Verbrugge"],
  "Affiliations": ["McGill University, Canada"],
  "Abstract": "Designing an interesting AI for a computer game is a complex undertaking, providing motivation to reuse portions of successful AIs. Here we advocate a layered Statechart-based AI as a modular approach that simplifies reuse. We analyze Statechart interactions and communications with respect to AI design, and propose an interface for Statechart-based AI-modules that summarizes interactions. \nReuse is accomplished by adding and removing modules to a new AI, largely enabled through event-renaming to ensure coherence with the interfaces. We describe an approach to module composition using functional groups, which allow for the encapsulation of high level behaviours (e.g., fleeing or exploring). This enables a designer to compose new AIs by assigning high-level behaviours. Additionally, the interface describes interactions with the game at-large, leading naturally to portability between games and even implementation languages. Finally, we look ahead to the requirements for a tool that would implement these ideas."
},
{
  "Title": "HALO (Highly Addictive, sociaLly Optimized) Software Engineering",
  "Type": "Paper",
  "Key": "41ec092921ab8911fc52f177c5aea2",
  "Authors": ["Swapneel Sheth", "Jonathan Bell", "Gail Kaiser"],
  "Affiliations": ["Columbia University, USA"],
  "Abstract": "In recent years, computer games have become increasingly social and collaborative in nature. Massively multiplayer online games, in which a large number of players collaborate with each other to achieve common goals in the game, have become extremely pervasive. By working together towards a common goal, players become more engrossed in the game. In everyday work environments, this sort of engagement would be beneficial, and is often sought out. We propose an approach to software engineering called HALO that builds upon the properties found in popular games, by turning work into a game environment. Our proposed approach can be viewed as a model for a family of prospective games that would support the software development process. Utilizing operant conditioning and flow theory, we create an immersive software development environment conducive to increased productivity. We describe the mechanics of HALO and how it could fit into typical software engineering processes."
},
{
  "Title": "The Architectural Challenges of Adding Accessibility Features to ALICE as a Case Study of Maintenance in Educational Software",
  "Type": "Paper",
  "Key": "43c8409fdb7e5de5f5000f8998443b",
  "Authors": ["Stephanie Ludi", "George Adams", "Bradley Blankenship", "Michel Dapiran"],
  "Affiliations": ["Rochester Institute of Technology, USA"],
  "Abstract": "Games take many forms, including educational environments. Alice is an example of an engaging environment that teaches object-oriented programming through the development of animations. The AliceVI project sought to extent Alice to visually impaired users. This paper describes how the architectural of Alice impacted the ability to add accessibility features. The result has been to start a system, named BridgIT, from scratch. The technology and proposed architecture is discussed for the new system, which takes the tenets of Alice to a new group of users."
},
{
  "Title": "GameChanger: A Middleware for Social Exergames",
  "Type": "Paper",
  "Key": "3d7a27aa7fe55a1d6a234367c85858",
  "Authors": ["Jamie Payton", "Evie Powell", "Andrea Nickel", "Katelyn Doran", "Tiffany Barnes"],
  "Affiliations": ["University of North Carolina at Charlotte, USA"],
  "Abstract": "While the health benefits of exercise are wide-ranging and well-known, the population of the United States is suffering from a lack of physical activity. We believe that combining elements of social interaction with exercise in video games will lead to increased and sustained engagement in physical activity. In this paper, we present an initial design of GameChanger, a middleware to support the development of a new generation of social exergames that interweave physical activity as a core game mechanic with social elements such as competition and collaboration. The GameChanger middleware provides programming abstractions that are specific to social exergame mechanics, elevating their description so that even non-expert programmers can create interesting social exergames that utilize mobile phones and sensing technology to integrate physical activity into gameplay. In addition, the middleware provides constructs for performing continuous assessment of physical activity during gameplay; these constructs can be used to provide feedback to the gameplayer or to collect datasets for evaluation by health researchers. As such, the GameChanger middleware can also serve as a platform to support scientific experimentation and exploration to determine which combinations of social and physical elements have the greatest impact on physical activity."
},
{
  "Title": "Interoperability Standards for Pervasive Games",
  "Type": "Paper",
  "Key": "f1b891b2f393ea651f9f2c90080449",
  "Authors": ["Chris Branton", "Doris Carver", "Brygg Ullmer"],
  "Affiliations": ["Louisiana State University, USA"],
  "Abstract": "Pervasive games combine the real and virtual worlds to provide a more social and physical experience than traditional video games. Pervasive games can introduce significant new requirements for robustness and interoperability, and may encourage game developers to better align their practices with trends in other domains. In addition to new languages and middleware, developing and adopting standards for interoperability could benefit both developers and users of pervasive games, and facilitate the growth of the genre."
},
{
  "Title": "Serious Game Development as an Iterative User-Centered Agile Software Project",
  "Type": "Paper",
  "Key": "158203f9108e548756e8ff695e326a",
  "Authors": ["Hazeline Asuncion", "David Socha", "Kelvin Sung", "Scott Berfield", "Wanda Gregory"],
  "Affiliations": ["University of Washington at Bothell, USA"],
  "Abstract": "Commissioned by the campus Office of Admissions, we have built a series of three campus tour and orientation games over the past academic year with undergraduate student project teams. Based on well-established game industry practices we followed an iterative agile process with Scrum and managed to avoid many classical pitfalls in game development. While we achieved some measure of success, in post-project analysis, it becomes obvious that our process would have benefited from the heavy emphasis of “users” in the User-Centered Design (UCD) methods. In this position paper, we propose that the serious game development community continue to critically analyze the results from the UCD projects to benefit from its lessons, well-understood good practices, and development paradigms."
},
{
  "Title": "Towards Democratizing Computer Science Education through Social Game Design",
  "Type": "Paper",
  "Key": "5c6b3edcd806e6d2dd7170f3f03db6",
  "Authors": ["Navid Ahmadi", "Mehdi Jazayeri", "Alexander Repenning"],
  "Affiliations": ["University of Lugano, Switzerland", "University of Colorado, USA"],
  "Abstract": "Computer science and software engineering education are limited to formal courses that are being taught in the school. Those who do not have access to the educational courses miss the learning context, even if educational tools are accessible for free. Computer game design has been employed as an engaging medium for practicing software engineering and computer programming skills. However, collaborative work is not supported by educational game design environments and peer learning is limited to face-to-face communication in the classroom. In this paper, we suggest democratizing computer science education by incorporating social learning into the educational game design using existing Web 2.0 mechanisms. Consequently, online users will benefit from situated learning in the game design activities that take place in their social networking space. We present AgentWeb, a Web-based game design environment as the steppingstone to enable social game design activities, and explore the challenges in fostering social learning in online game design practices."
},
{
  "Title": "Software Engineering Challenges of Multi-player Outdoor Smartphone Games",
  "Type": "Paper",
  "Key": "d888eccfb8997fba034d3652dfe39a",
  "Authors": ["Robert J. Hall"],
  "Affiliations": ["AT&T Labs Research, USA"],
  "Abstract": "Physical inactivity and social isolation are the demons of computer gaming. To combat these and similar problems, my goal is to create attractive game experiences that require play outdoors, encourage multi-player interaction, and incorporate vigorous physical activity inherently. However, play outdoors in large scale naturalistic environments, using only the equipment (e.g., smartphones) people normally carry with them in the world, brings new challenges to game design. Gone is the central server that coordinates all activities and manages evolution of the game state; gone is perfect communication among all players at all times; and gone are specialized sensors and controllers purpose built for games. This paper lays out the motivation for this style of gaming, as well as the challenges we face in engineering them, from requirements capture through design, coding, and validation. Finally, I summarize first steps that have already been taken and hint at future directions."
},
{
  "Title": "Mesa: Automatic Generation of Lookup Table Optimizations",
  "Type": "Paper",
  "Key": "13ee4dc296f5acaa7663908ffbc842",
  "Authors": ["Chris Wilcox", "Michelle Mills Strout", "James M. Bieman"],
  "Affiliations": ["Colorado State University, USA"],
  "Abstract": "Scientific programmers strive constantly to meet performance demands. Tuning is often done manually, despite the significant development time and effort required. One example is lookup table (LUT) optimization, a technique that is generally applied by hand due to a lack of methodology and tools. LUT methods reduce execution time by replacing computations with memory accesses to precomputed tables of results. LUT optimizations improve performance when the memory access is faster than the original computation, and the level of reuse is sufficient to amortize LUT initialization. Current practice requires programmers to inspect program source to identify candidate expressions, then develop specific LUT code for each optimization. Measurement of LUT accuracy is usually ad hoc, and the interaction with multicore parallelization has not been explored. \nIn this paper we present Mesa, a standalone tool that implements error analysis and code generation to improve the process of LUT optimization. We evaluate Mesa on a multicore system using a molecular biology application and other scientific expressions. Our LUT optimizations realize a performance improvement of 5X for the application and up to 45X for the expressions, while tightly controlling error. We also show that the serial optimization is just as effective on a parallel version of the application. Our research provides a methodology and tool for incorporating LUT optimizations into existing scientific code."
},
{
  "Title": "Model-based Generation of Static Schedules for Safety Critical Multi-Core Systems in the Avionics Domain",
  "Type": "Paper",
  "Key": "a5a8d51028d43c7b5c2a5207cec8ca",
  "Authors": ["Robert Hilbrich", "Hans-Joachim Goltz"],
  "Affiliations": ["Fraunhofer FIRST, Germany"],
  "Abstract": "Static schedules are used in safety critical systems to achieve predictable, real-time behavior. While it was possible to construct static schedules manually for simple, single-core systems, the increase in complexity introduced by multi-core processors and the demand for flexible and dynamic engineering processes in the avionics domain, require a novel approach for their automatic generation. \nThis paper describes ongoing trends in the avionics domain to further underline engineering constraints encountered, when introducing multi-core processors in a safety critical area. By focussing on the requirement of a predictable behavior, a model-based approach for the generation of static schedules for complex multi-core systems is presented. It incorporates the usage of external resources, which is essential to achieve deterministic resource access and real-time behavior on hardware architectures with multiple execution units."
},
{
  "Title": "Improving Programmability of Heterogeneous Many-Core Systems via Explicit Platform Descriptions",
  "Type": "Paper",
  "Key": "c219e63c7cc665b8f1a107589de7c7",
  "Authors": ["Martin Sandrieser", "Siegfried Benkner", "Sabri Pllana"],
  "Affiliations": ["University of Vienna, Austria"],
  "Abstract": "In this paper we present ongoing work towards a programming framework for heterogeneous hardware- and software environments. Our framework aims at improving programmability and portability for heterogeneous many-core systems via a Platform Description Language (PDL) for expressing architectural patterns and platform information. We developed a prototypical code generator that takes as input an annotated serial task-based program and outputs, parametrized via PDL descriptors, code for a specific target heterogeneous computing system. By varying the target PDL descriptor, code for different target configurations can be generated without the need to modify the input program. We utilize a simple task-based programming model for demonstration of our approach and present preliminary results indicating its applicability on a state-of-the-art heterogeneous system."
},
{
  "Title": "Auto-tuning SkePU: A Multi-Backend Skeleton Programming Framework for Multi-GPU Systems",
  "Type": "Paper",
  "Key": "b59e6ecfcdd47ec05aed56ca3b2c22",
  "Authors": ["Usman Dastgeer", "Johan Enmyren", "Christoph W. Kessler"],
  "Affiliations": ["Linköping University, Sweden"],
  "Abstract": "SkePU is a C++ template library that provides a simple and unified interface for specifying data-parallel computations with the help of skeletons on GPUs using CUDA and OpenCL. The interface is also general enough to support other architectures, and SkePU implements both a sequential CPU and a parallel OpenMP backend. It also supports multi-GPU systems. Currently available skeletons in SkePU include map, reduce, mapreduce, map-with-overlap, map-array, and scan. The performance of SkePU generated code is comparable to that of hand-written code, even for more complex applications such as ODE solving. \nIn this paper, we discuss initial results from auto-tuning SkePU using an off-line, machine learning approach where we adapt skeletons to a given platform using training data. The prediction mechanism at execution time uses off-line pre-calculated estimates to construct an execution plan for any desired configuration with minimal overhead. The prediction mechanism accurately predicts execution time for repetitive executions and includes a mechanism to predict execution time for user functions of different complexity. The tuning framework covers selection between different backends as well as choosing optimal parameter values for the selected backend. We will discuss our approach and initial results obtained for different skeletons (map, mapreduce, reduce)."
},
{
  "Title": "Lightweight Parallel Accumulators Using C++ Templates",
  "Type": "Paper",
  "Key": "faf08d7756e10976fc4e4afcbeae3e",
  "Authors": ["Yossi Lev", "Mark Moir"],
  "Affiliations": ["Oracle Labs, USA"],
  "Abstract": "The Boost.Accumulators framework provides C++ template-based support for incremental computation of many important statistical functions, such as maximum, minimum, mean, count, variance, etc. Basic accumulators can be combined to build more sophisticated ones. We explore how this framework can be extended to implement lightweight parallel accumulators that allow multiple threads to Store sample data, and support concurrent GetResult operations that incrementally compute desired functions over the data. Our evaluation shows that our parallel accumulators are scalable and can effectively exploit programmer-supplied knowledge to achieve signiﬁcant optimizations for some important cases."
},
{
  "Title": "Open Language Implementation",
  "Type": "Paper",
  "Key": "570af13048ba13ff7a6ada0effe02b",
  "Authors": ["Mandana Vaziri", "Robert Fuhrer", "Evelyn Duesterwald"],
  "Affiliations": ["IBM Research, USA"],
  "Abstract": "Writing multi-threaded shared variable code is notoriously difficult because a given program can result in different possible executions due to non-determinism. Observed executions may also vary depending on the specific compiler and runtime system at hand. Many aspects of behavior are beyond the control of the programmer at the language's level of abstraction (e.g., compiler transformations), and as a result, debugging and performance tuning are very difficult tasks. In this position paper, we propose that the language implementation stack (compiler, runtime system) should be open, i.e., transparent, accountable, and interactive, so that the programmer may have direct access to invaluable information, and regain control over the behavior of the program in a given environment."
},
{
  "Title": "How Do Programs Become More Concurrent? A Story of Program Transformations",
  "Type": "Paper",
  "Key": "5cd7d6c891b9733f28a2bbb7e2bc77",
  "Authors": ["Danny Dig", "John Marrero", "Michael D. Ernst"],
  "Affiliations": ["University of Illinois, USA", "Massachusetts Institute of Technology, USA", "University of Washington, USA"],
  "Abstract": "In the multi-core era, programmers need to resort to parallelism if they want to improve program performance. Thus, a major maintenance task will be to make sequential programs more concurrent. Must concurrency be designed into a program, or can it be retrofitted later? What are the most common transformations to retrofit concurrency into sequential programs? Are these transformations random, or do they belong to certain categories? How can we automate these transformations? \nTo answer these questions we analyzed the source code of 5 open-source Java projects and looked at a total of 14 versions. We analyzed qualitatively and quantitatively the concurrency-related transformations. We found that these transformations belong to four categories: transformations that improve the responsiveness, the throughput, the scalability, or correctness of the applications. In 73.9% of these transformations, concurrency was retrofitted on existing program elements. In 20.5% of the transformations, concurrency was designed into new program elements. Our findings educate software developers on how to parallelize sequential programs, and provide hints for tool vendors about what transformations are worth automating."
},
{
  "Title": "Viewing Simple Clones from Structural Clones' Perspective",
  "Type": "Paper",
  "Key": "db391b9711f7fcd6e502160a8d456c",
  "Authors": ["Hamid Abdul Basit", "Usman Ali", "Stanislaw Jarzabek"],
  "Affiliations": ["Lahore University of Management Sciences, Pakistan", "National University of Singapore, Singapore"],
  "Abstract": "In previous work, we described a technique for detecting design-level similar program structures that we called structural clones. Structural clones are recurring configurations of simple clones (i.e., similar code fragments). In this paper, we show how structural clone analysis extends the benefits of analysis based on simple clones only. First, we present experimental results showing that in many cases simple clones participated in structural clones. In such cases, structural clones being larger than simple clones but smaller in number, allow analysts to see the “forest from the trees”, as far as the similarity situation is concerned. We provide arguments and examples to show how the knowledge of structural clones – their location and exact similarities and differences – helps in program understanding, design recovery, maintenance, and refactoring."
},
{
  "Title": "Extracting Code Clones for Refactoring Using Combinations of Clone Metrics",
  "Type": "Paper",
  "Key": "e896b28039ae7060078d93922d8f66",
  "Authors": ["Eunjong Choi", "Norihiro Yoshida", "Takashi Ishio", "Katsuro Inoue", "Tateki Sano"],
  "Affiliations": ["Osaka University, Japan", "Nara Institute of Science and Technology, Japan", "NEC Corporation, Japan"],
  "Abstract": "Code clone detection tools may report a large number of code clones, while software developers are interested in only a subset of code clones that are relevant to software development tasks such as refactoring. Our research group has supported many software developers with the code clone detection tool CCFinder and its GUI front-end Gemini. Gemini shows clone sets (i.e., a set of code clones identical or similar to each other) with several clone metrics including their length and the number of code clones; however, it is not clear how to use those metrics to extract interesting code clones for developers. In this paper, we propose a method combining clone metrics to extract code clones for refactoring activity. We have conducted an empirical study on a web application developed by a Japanese software company. The result indicates that combinations of simple clone metric is more effective to extract refactoring candidates in detected code clones than individual clone metric."
},
{
  "Title": "Oops! ... I Changed It Again",
  "Type": "Paper",
  "Key": "e84e84846dd15b9b06e0eb275af5a9",
  "Authors": ["Nils Göde", "Jan Harder"],
  "Affiliations": ["University of Bremen, Germany"],
  "Abstract": "Duplicated passages of source code—clones—are a threat to software maintenance as modifying duplicated code causes additional change effort and bears the risk of incomplete propagation of changes to all copies. Although previous studies have investigated the consistency and threats of changing clones, changes have always been analyzed detached from another—not considering that individual clones may change more than once during their lifetime. In this paper we present our study on the patterns of consecutive changes to clones in real systems and discuss in how far they are suitable for identifying unwanted inconsistencies."
},
{
  "Title": "Index-Based Model Clone Detection",
  "Type": "Paper",
  "Key": "1ace2b658c1f4592b1fb8e2b3d4d35",
  "Authors": ["Benjamin Hummel", "Elmar Juergens", "Daniela Steidl"],
  "Affiliations": ["TU München, Germany"],
  "Abstract": "Existing algorithms for model clone detection operate in batch mode. Consequently, if a small part of a large model changes during maintenance, the entire detection needs to be recomputed to produce updated cloning information. Since this can take several hours, the lack of incremental detection algorithms hinders clone management, which requires up-to-date cloning information. In this paper we present an index-based algorithm for model clone detection that is incremental and distributable. We present a case study that demonstrates its capabilities, outline its current limitations and present directions for future work."
},
{
  "Title": "Is Cloned Code Older than Non-Cloned Code?",
  "Type": "Paper",
  "Key": "af8c0a27a57dfeff4e79d64ac82645",
  "Authors": ["Jens Krinke"],
  "Affiliations": ["University College London, UK"],
  "Abstract": "It is still a debated question whether cloned code causes increased maintenance efforts. If cloned code is more stable than non-cloned code, i.e. it is changed less often, it will require less maintenance efforts. The more stable cloned code is, the longer it will not have been changed, so the stability can be estimated through the code's age. This paper presents a study on the average age of cloned code. For three large open source systems, the age of every line of source code is computed as the date of the last change in that line. In addition, every line is categorized whether it belongs to cloned code as detected by a clone detector. The study shows that on average, cloned code is older than non-cloned code. Moreover, if a file has cloned code, the average age of the cloned code of the file is lower than the average age of the non-cloned code in the same file. The results support the previous findings that cloned code is more stable than non-cloned code."
},
{
  "Title": "Automated Type-3 Clone Oracle Using Levenshtein Metric",
  "Type": "Paper",
  "Key": "f49a48ee861f39b07396ea6af3a779",
  "Authors": ["Thierry Lavoie", "Ettore Merlo"],
  "Affiliations": ["Ecole Polytechnique de Montréal, Canada"],
  "Abstract": "Clone detection techniques quality and performance evaluation require a system along with its clone oracle, that is a reference database of all accepted clones in the investigated system. Many challenges, including finding an adequate clone definition and scalability to industrial size systems, must be overcome to create good oracles. This paper presents an original method to construct clone oracles based on the Levenshtein metric. Although other oracles exist, this is the largest known oracle for type-3 clones that was created by an automated process on massive data sets. The method behind the creation of the oracle as well as actual oracles characteristics are presented. Discussion of the results in relation to other ways of building oracles is also provided along with future research possibilities."
},
{
  "Title": "Analyzing Web Service Similarity Using Contextual Clones",
  "Type": "Paper",
  "Key": "7fb5b25896d9ea9b8b0862f54a4916",
  "Authors": ["Douglas Martin", "James R. Cordy"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "There are several tools and techniques developed over the past decade for detecting duplicated code in software. However, there exists a class of languages for which clone detection is ill-suited. We discovered one of these languages when we attempted to use clone detection to find similar web service operations in service descriptions written in the Web Service Description Language (WSDL). WSDL is structured in such a way that identifying units for comparison becomes a challenge. WSDL service descriptions contain specifications of one or more operations that are divided into pieces and intermingled throughout the description. In this paper, we describe a method of reorganizing them in order to leverage clone detection technology to identify similar services. We introduce the idea of contextual clones – clones that can only be found by augmenting code fragments with related information referenced by the fragment to give it context. We demonstrate this idea for WSDL and propose other languages and situations for which contextual clones may be of interest."
},
{
  "Title": "Scalable Clone Detection Using Description Logic",
  "Type": "Paper",
  "Key": "0da010c8f46556c1d9a4cd5b0121b1",
  "Authors": ["Philipp Schugerl"],
  "Affiliations": ["Concordia University, Canada"],
  "Abstract": "The semantic web is slowly transforming the web as we know it into a machine understandable pool of information that can be consumed and reasoned about by various clients. Source-code is no exception to this trend and various communities have proposed standards to share code as linked data. With the availability of large amounts of open source code published in publically accessible repositories and the introduction of massively horizontally scaling frameworks and cloud computing infrastructure, a new era of software mining across information silos is reshaping the software engineering landscape. The so far unreachable goal of analyzing code at a global level, and therefore detecting global software clones, has become manageable. Description logic and semantic web reasoners have so far only plaid a minor role in this transformation and are mainly used to model source code data. In this paper, we introduce a clone detection algorithm that uses a semantic web reasoner and is based on the Hadoop map-reduce framework that can scale horizontally to a large amount of data. We also define a novel and compact clone model that only considers control-blocks and used data types while still yielding similar clone detection results than more complex representations. In order to validate our approach we have compared our algorithm to some of the leading clone detection tools (CCFinder, JCD and Simian) and show differences in performance and detection precision."
},
{
  "Title": "Representing Clones in a Localized Manner",
  "Type": "Paper",
  "Key": "d0e354d279f212f08832b294196534",
  "Authors": ["Robert Tairas", "Ferosh Jacob", "Jeff Gray"],
  "Affiliations": ["AtlanMod, France", "University of Alabama, USA"],
  "Abstract": "Code clones (i.e., duplicate sections of code) can be scattered throughout the source files of a program. Manually evaluating a group of such clones requires observing each clone in its original location (i.e., opening each file and finding the source location of each clone), which can be a time-consuming process. As an alternative, this paper introduces a new technique to localize the representation of code clones to provide a summary of the properties of two or more clones in one location. In our approach, the results of a clone detection tool are analyzed in an automated manner to determine the properties (i.e., similarities and differences) of the clones. These properties are visualized directly within the source editor. The localized representation is realized as part of the features of an Eclipse plug-in called CeDAR."
},
{
  "Title": "On the Need for Human-based Empirical Validation of Techniques and Tools for Code Clone Analysis",
  "Type": "Paper",
  "Key": "d85a83174fd804ee8ead125740e291",
  "Authors": ["Jeffrey C. Carver", "Debarshi Chatterji", "Nicholas A. Kraft"],
  "Affiliations": ["University of Alabama, USA"],
  "Abstract": "Code clone analysis techniques and tools are popular topics among the software engineering research community. Many studies draw conclusions solely based on an analytical analysis. These claims focus primarily on tool performance in terms of portability, scalability, robustness, precision, and recall. However, these types of analytical studies cannot adequately evaluate the behavior of the developers while using the tools. Human-based empirical studies are complementary to studies based on analytical data because they provide direct insight into developer behavior. In this paper we argue for the need for more human-based empirical studies in the area of code clone analysis techniques and tools."
},
{
  "Title": "Code Clone Detection Experience at Microsoft",
  "Type": "Paper",
  "Key": "1f21391255764f7cef9bb7465bc173",
  "Authors": ["Yingnong Dang", "Song Ge", "Ray Huang", "Dongmei Zhang"],
  "Affiliations": ["Microsoft Reesarch Asia, China"],
  "Abstract": "Cloning source code is a common practice in the software development process. In general, the number of code clones increases in proportion to the growth of the code base. It is challenging to proactively keep clones consistent and remove unnecessary clones during the entire software development process of large-scale commercial software. In this position paper, we briefly share some typical usage scenarios of code clone detection that we collected from Microsoft engineers. We also discuss our experience on building XIAO, a code clone detection tool, and the feedback we have received from Microsoft engineers on using XIAO in real development settings."
},
{
  "Title": "Determining the Provenance of Software Artifacts",
  "Type": "Paper",
  "Key": "eb0903cef2aa1aad500b13d6b5b649",
  "Authors": ["Michael W. Godfrey", "Daniel M. German", "Julius Davies", "Abram Hindle"],
  "Affiliations": ["University of Waterloo, Canada", "University of Victoria, Canada"],
  "Abstract": "Software clone detection has made substantial progress in the last 15 years, and software clone analysis is starting to provide real insight into how and why code clones are born, evolve, and sometimes die. In this position paper, we make the case that there is a more general problem lurking in the background: software artifact provenance analysis. We argue that determining the origin of software artifacts is an increasingly important problem with many dimensions. We call for simple and lightweight techniques that can be used to help narrow the search space, so that more expensive techniques -- including manual examination --- can be used effectively on a smaller candidate set. We predict the problem of software provenance will lead towards new avenues of research for the software clones community."
},
{
  "Title": "Research in Cloning Beyond Code: A First Roadmap",
  "Type": "Paper",
  "Key": "2e55d8fad3437a38e9db346530d583",
  "Authors": ["Elmar Juergens"],
  "Affiliations": ["TU München, Germany"],
  "Abstract": "Most research in software cloning has a strong focus on source code. However, cloning occurs in other software artifacts, as well. In this paper, we summarize existing work on cloning in other software artifacts and provide a list of research questions for future work."
},
{
  "Title": "How Code Skips Over Revisions",
  "Type": "Paper",
  "Key": "8d5849e382ed4edba00e295fabc2d7",
  "Authors": ["Toshihiro Kamiya"],
  "Affiliations": ["Future University Hakodate, Japan"],
  "Abstract": "This paper explores the need for `history-aware' searches, by experimentally showing a development process that includes code fragments which disappear at a revision and appear again at a later revision. Some of these code re-appearances are not a result of a revert command of a version control system, but a result of a developer who copied a code fragment from old source files."
},
{
  "Title": "Visualizing the Evolution of Code Clones",
  "Type": "Paper",
  "Key": "c994e646c70ee1dcf81ab46929c287",
  "Authors": ["Ripon K. Saha", "Chanchal K. Roy", "Kevin A. Schneider"],
  "Affiliations": ["University of Saskatchewan, Canada"],
  "Abstract": "The knowledge of code clone evolution throughout the history of a software system is essential in comprehending and managing its clones properly and cost-effectively. However, investigating and observing facts in a huge set of text-based data provided by a clone genealogy extractor could be challenging without the support of a visualization tool. In this position paper, we present an idea of visualizing code clone evolution by exploiting the advantages of existing clone visualization techniques that would be both scalable and useful."
},
{
  "Title": "Clone Detection through Process Algebras and Java Bytecode",
  "Type": "Paper",
  "Key": "821a844939c24ab8d266da02097a33",
  "Authors": ["Antonella Santone"],
  "Affiliations": ["University of Sannio, Italy"],
  "Abstract": "In this paper we present a formal method-based approach in detecting source code clones by means of analysing and comparing the Java Bytecode that is produced when the source code is compiled. A preliminary investigation has been also conducted to assess the validity of the proposed approach."
},
{
  "Title": "Towards Flexible Code Clone Detection, Management, and Refactoring in IDE",
  "Type": "Paper",
  "Key": "866bd8cd58eb5680d621550fd00644",
  "Authors": ["Minhaz F. Zibran", "Chanchal K. Roy"],
  "Affiliations": ["University of Saskatchewan, Canada"],
  "Abstract": "In this paper, we propose an IDE-based clone management system to flexibly detect, manage, and refactor both exact and near-miss code clones. Using a k-difference hybrid suffix tree algorithm we can efficiently detect both exact and near-miss clones. We have implemented the algorithm as a plugin to the Eclipse IDE, and have been extending this for real-time code clone management with semi-automated refactoring support during the actual development process."
},
{
  "Title": "VisCad: Flexible Code Clone Analysis Support For NiCad",
  "Type": "Paper",
  "Key": "014b34bdaf8ede00ee3aca438b4127",
  "Authors": ["Muhammad Asaduzzaman", "Chanchal K. Roy", "Kevin A. Schneider"],
  "Affiliations": ["University of Saskatchewan, Canada"],
  "Abstract": "Clone detector results can be better understood with tools that support visualization and facilitate in-depth analysis. In this tool demo paper we present VisCad, a comprehensive code clone analysis and visualization tool that provides such support for the near-miss hybrid clone detection tool, NiCad. Through carefully selected metrics and visualization techniques VisCad can guide users to explore the cloning of a system from different perspectives."
},
{
  "Title": "Live Scatterplots",
  "Type": "Paper",
  "Key": "3befc790cab8489f439f31ae0bb2ff",
  "Authors": ["James R. Cordy"],
  "Affiliations": ["Queen’s University, Canada"],
  "Abstract": "Scatterplots have been used to help understand clone relationships in large scale systems since the earliest large system studies more than a decade ago. They often expose interesting patterns of cloning between subsystems and point to opportunities for further analysis. However, the remaining question when such patterns are seen is always, \"but what is that?\" Live scatterplots are aimed at providing an immediate, intuitive answer that can help the analyst to quickly identify and access subsystems and clones involved in a pattern simply by directly pointing at it in the scatterplot. Live scatterplots exploit the table, title and hyperlink tags of standard HTML to provide this ability in any standard browser, without the need for custom frameworks."
},
{
  "Title": "Efficiently Handling Clone Data: RCF and cyclone",
  "Type": "Paper",
  "Key": "237cd7d0433e6d2852c0963de08f88",
  "Authors": ["Jan Harder", "Nils Göde"],
  "Affiliations": ["University of Bremen, Germany"],
  "Abstract": "Exchange and inspection of clone information is an essential building block of successful clone management. Until now, a variety of different formats and tools for handling clone data has evolved. This paper presents two of our tools, both of which work closely together. We introduce the Rich Clone Format, which we have developed to standardize and ease the exchange of clone data. Furthermore, we describe our tool cyclone, which fosters the multi-perspective analysis of clone information."
},
{
  "Title": "CloneDiff - Semantic Differencing of Clones",
  "Type": "Paper",
  "Key": "5e2baa2dc9fa4ad7da2dc3a4ad85bf",
  "Authors": ["Yinxing Xue", "Zhenchang Xing", "Stanislaw Jarzabek"],
  "Affiliations": ["National University of Singapore, Singapore"],
  "Abstract": "Clone detection provides a scalable and efficient way to detect similar codes, while program differencing is a powerful and effective way to analyze similar codes. CloneDiff, a Program Dependence Graphs (PDGs) differencing tool, complements clone detection with program differencing for the purpose of characterizing clones. It captures semantic information of clones from PDGs, and uses graph matching techniques to compute a precise characterization of clones in terms of a category of semantic differences."
},
{
  "Title": "An Empirical Model of Technical Debt and Interest",
  "Type": "Paper",
  "Key": "3d18b20e4d8d355bdd42e51c29cf74",
  "Authors": ["Ariadi Nugroho", "Joost Visser", "Tobias Kuipers"],
  "Affiliations": ["Software Improvement Group, Netherlands"],
  "Abstract": "Cunningham introduced the metaphor of technical debt as guidance for software developers that must trade engineering quality against short-term goals. \nWe revisit the technical debt metaphor, and translate it into terms that can help IT executives better understand their IT investments. An approach is proposed to quantify debts (cost to fix technical quality issues) and interest (extra cost spent on maintenance due to technical quality issues). Our approach is based on an empirical assessment method of software quality developed at the Software Improvement Group (SIG). The core part of the technical debt calculation is constructed on the basis of empirical data of 44 systems that are currently being monitored by SIG. \nIn a case study, we apply the approach to a real system, and discuss how the results provide useful insights on important questions related to IT investment such as the return on investment (ROI) in software quality improvement."
},
{
  "Title": "Monitoring Code Quality and Development Activity by Software Maps",
  "Type": "Paper",
  "Key": "8ddabe363556cbd4a2e0e32a767a85",
  "Authors": ["Johannes Bohnet", "Jürgen Döllner"],
  "Affiliations": ["Hasso-Plattner-Institute at the University of Potsdam, Germany"],
  "Abstract": "Software development projects are difficult to manage, in general, due to the friction between completing system features and, at the same time, obtaining a high degree of code quality to ensure maintainability of the system in the future. A major challenge of this optimization problem is that code quality is less visible to stakeholders in the development process, particularly, to the management. In this paper, we describe an approach for automated software analysis and monitoring of both quality-related code metrics and development activities by means of software maps. A software map represents an adaptive, hierarchical representation of software implementation artifacts such as source code files being organized in a modular hierarchy. The maps can express and combine information about software development, software quality, and system dynamics; they can systematically be specified, automatically generated, and organized by templates. The maps aim at supporting decision-making processes. For example, they facilitate to decide where in the code an increase of quality would be beneficial both for speeding up current development activities and for reducing risks of future maintenance problems. Due to their high degree of expressiveness and their instantaneous generation, the maps additionally serve as up-to-date information tools, bridging an essential information gap between management and development, improve awareness, and serve as early risk detection instrument. The software map concept and its tool implementation are evaluated by means of two case studies on large industrially developed software systems."
},
{
  "Title": "Investigating the Impact of Design Debt on Software Quality",
  "Type": "Paper",
  "Key": "60bfdcbeebf29be43c52c1e358350c",
  "Authors": ["Nico Zazworka", "Michele Shaw", "Forrest Shull", "Carolyn B. Seaman"],
  "Affiliations": ["Fraunhofer CESE, USA", "UMBC, USA"],
  "Abstract": "Technical debt is a metaphor describing situations where developers accept sacrifices in one dimension of development (e.g. software quality) in order to optimize another dimension (e.g. implementing necessary features before a deadline). Approaches, such as code smell detection, have been developed to identify particular kinds of debt, e.g. design debt. What has not yet been understood is the impact design debt has on the quality of a software product. Answering this question is important for understanding how growing debt affects a software product and how it slows down development, e.g. though introducing rework such as fixing bugs. In this case study we investigate how design debt, in the form of god classes, affects the maintainability and correctness of software products by studying two sample applications of a small-size software development company. The results show that god classes are changed more often and contain more defects than non-god classes. This result complements findings of earlier research and suggests that technical debt has a negative impact on software quality, and should therefore be identified and managed closely in the development process."
},
{
  "Title": "From Assessment to Reduction: How Cutter Consortium Helps Rein in Millions of Dollars in Technical Debt",
  "Type": "Paper",
  "Key": "450eabf26ed84cf1735ead4a05a314",
  "Authors": ["Israel Gat", "John D. Heintz"],
  "Affiliations": ["Cutter Consortium, USA", "Gist Labs, USA"],
  "Abstract": "Technical debt assessments often follow a similar pattern across engagements. In contrast, technical debt reduction projects can vary significantly from one company to another. In addition to exposing unexpected technical challenges, technical debt reduction projects bring business issues, organizational consideration, and methodical questions to the fore. This paper outlines how all these aspects are being dealt with in a Cutter Consortium engagement with a client that struggles to wrestle technical debt to the ground."
},
{
  "Title": "An Extraction Method to Collect Data on Defects and Effort Evolution in a Constantly Modified System",
  "Type": "Paper",
  "Key": "730fc9acce5bfd6fa6d8ae946f98e1",
  "Authors": ["Rebeka Gomes", "Clauirton Siebra", "Graziela Tonin", "Antonio Cavalcanti", "Fabio Q. B. da Silva", "Andre L. M. Santos", "Rafael Marques"],
  "Affiliations": ["UFPE, Brazil"],
  "Abstract": "This paper describes a data extraction method that was carried out on a set of historical development documentation, related to a commercial software system for mobile platform. This method is part of a major project, which aims to identify evidences of technical debt via the analysis of the defect evolution and effort estimation deviation, verifying if there are relations between these concepts and project decisions during the cycles of development. We intend that a future analysis of such data supports the identification of patterns regarding specific decisions and variations of the defect number/frequency and effort deviation. Thus, such patterns could assist project managers during future project decisions, mainly regarding the maintenance and evolution stages."
},
{
  "Title": "A Portfolio Approach to Technical Debt Management",
  "Type": "Paper",
  "Key": "5b3f1b6a189089a921cc15fcdc0116",
  "Authors": ["Yuepu Guo", "Carolyn B. Seaman"],
  "Affiliations": ["University of Maryland Baltimore County, USA"],
  "Abstract": "Technical debt describes the effect of immature software artifacts on software maintenance – the potential of extra effort required in future as if paying interest for the incurred debt. The uncertainty of interest payment further complicates the problem of what debt should be incurred or repaid and when. To help software managers make informed decisions, a portfolio approach is proposed in this paper. The approach leverages the portfolio management theory in the finance domain to determine the optimal collection of technical debt items that should be incurred or held. We expect this approach could provide a new perspective for technical debt management."
},
{
  "Title": "An Enterprise Perspective on Technical Debt",
  "Type": "Paper",
  "Key": "c2062db158a226591d0c6d6aeb5bd9",
  "Authors": ["Tim Klinger", "Peri Tarr", "Patrick Wagstrom", "Clay Williams"],
  "Affiliations": ["IBM Watson Research, USA"],
  "Abstract": "Technical debt is a term that has been used to describe the increased cost of changing or maintaining a system due to expedient shortcuts taken during its development. Much of the research on technical debt has focused on decisions made by project architects and individual developers who choose to trade off short-term gain for a longer-term cost. However, in the context of enterprise software development, such a model may be too narrow. We explore the premise that technical debt within the enterprise should be viewed as a tool similar to financial leverage, allowing the organization to incur debt to pursue options that it couldn’t otherwise afford. We test this premise by interviewing a set of experienced architects to understand how decisions to acquire technical debt are made within an enterprise, and to what extent the acquisition of technical debt provides leverage. We find that in many cases, the decision to acquire technical debt is not made by technical architects, but rather by non-technical stakeholders who cause the project to acquire new technical debt or discover existing technical debt that wasn’t previously visible. We conclude with some preliminary observations and recommendations for organizations to better manage technical debt in the presence of some enterprise-scale circumstances."
},
{
  "Title": "Prioritizing Design Debt Investment Opportunities",
  "Type": "Paper",
  "Key": "488d431b023eeb11abbb1c84af4c8f",
  "Authors": ["Nico Zazworka", "Carolyn B. Seaman", "Forrest Shull"],
  "Affiliations": ["Fraunhofer CESE, USA", "UMBC, USA"],
  "Abstract": "Technical debt is the technical work developers owe a system, typically caused by speeding up development, e.g. before a software release. Approaches, such as code smell detection, have been developed to identify particular kinds of debt, e.g. design debt. Up until now, code smell detection has been used to help point to components that need to be freed from debt by refactoring. To date, a number of methods have been described for finding code smells in a system. However, typical debt properties, such as the value of the debt and interest rate to be paid, have not been well established. This position paper proposes an approach to using cost/benefit analysis to prioritize technical debt reduction work by ranking the value and interest of design debt caused by god classes. The method is based on metric analysis and software repository mining and is demonstrated on a commercial software application at a mid size development company. The results are promising: the method helps to identify which refactoring activities should be performed first because they are likely to be cheap to make yet have significant effect, and which refactorings should be postponed due to high cost and low payoff."
},
{
  "Title": "Technical Debt from the Stakeholder Perspective",
  "Type": "Paper",
  "Key": "bb5c99bd63693c2a6dff708c921107",
  "Authors": ["Ted Theodoropoulos", "Mark Hofberg", "Daniel Kern"],
  "Affiliations": ["Acrowire, USA"],
  "Abstract": "The concept of technical debt provides an excellent tool for describing technology gaps in terms any stakeholder can understand. The technical debt metaphor was pioneered by the software development community and describes technical challenges in that context wonderfully. However, establishing a definitional framework which describes issues affecting quality more broadly will better align to stakeholder perspectives. Building on the existing concept in this way will enable technology stakeholders by providing a centralized technical debt model. The metaphor can then be used to consistently describe quality challenges anywhere within the technical environment. This paper lays the foundation for this conceptual model by proposing a definitional framework that describes how technology gaps affect all aspects of quality."
},
{
  "Title": "Tweetflows - Flexible Workflows with Twitter",
  "Type": "Paper",
  "Key": "382014c1e7da7ba90d6b7bc36df071",
  "Authors": ["Martin Treiber", "Daniel Schall", "Schahram Dustdar", "Christian Scherling"],
  "Affiliations": ["Vienna University of Technology, Austria", "ikangai solutions, Austria"],
  "Abstract": "We present a lightweight coordination and collaboration platform, intertwining contemporary social networking platforms and SOA principles. The idea of our approach is to use Twitter as a platform for collaborations of human and software services in the context of workflows. We introduce primitives that provide SOA functionality like service discovery or service binding and illustrate how these primitives are embedded in Tweets. By using Tweets, we are able to reuse existing infrastructures and tools (e.g., twitter clients on mo- bile devices) for the communication between services and humans. Simultaneously, we exploit social network structures originating from Twitter follower networks in order to discover (human and software) resources that are required for the execution of a workflow. Finally, we are able to monitor the execution of workflows with Twitter, simply by following Tweets that represent the execution of a workflow."
},
{
  "Title": "From Textual Use Cases to Service Component Models",
  "Type": "Paper",
  "Key": "1bbfc0ae8dfa6d70cc30de9343b7a2",
  "Authors": ["Zuohua Ding", "Mingyue Jiang", "Jens Palsberg"],
  "Affiliations": ["Zhejiang Sci-Tech University, China", "UC Los Angeles, USA"],
  "Abstract": "There is a gap between system requirements described with natural language and system design models described with formal language. In this paper, we present a framework for automatically mapping textual use cases to service component models from a model-based point of view. The generated models capture service component signatures and language independent dynamic behaviors. We have implemented our framework and demonstrated the benefits via a case study."
},
{
  "Title": "Engineering Multi-Tenant Software-as-a-Service Systems",
  "Type": "Paper",
  "Key": "3596837f7bd93c5244079ff56d13d7",
  "Authors": ["Bikram Sengupta", "Abhik Roychoudhury"],
  "Affiliations": ["IBM Research, India", "National University of Singapore, Singapore"],
  "Abstract": "Increasingly, Software-as-a-Service (SaaS) is becoming a dominant mechanism for the consumption of software by end users. From a vendor’s perspective, the benefits of SaaS arise from leveraging economies of scale, by serving a large number of customers (“tenants”) through a shared instance of a centrally hosted software service. Consequently, a SaaS provider would, in general, try to drive commonality amongst the requirements of different tenants, and at best, offer a fixed set of customization options. However, many tenants would also come with custom requirements, which may be a pre-requisite for them to adopt the SaaS system. These requirements should then be addressed by evolving the SaaS system in a controlled manner, while still supporting the needs of existing tenants. This need to balance tenant variability and commonality, and to optimize on development and testing effort, can make the evolution of multi-tenant SaaS systems an interesting engineering challenge; this has strong economic undertones as well, given the “pay-per-use” subscription model of SaaS, and the cost of incremental development and maintenance to cater to new tenant needs. In this paper, we outline a set of research issues in the design, testing and maintenance of multi-tenant SaaS systems, and highlight some of the interesting optimization questions that arise in the process. Presenting specific technical solutions is beyond the scope of this paper – instead, our goal is to help shape a research agenda for multi-tenant SaaS that can provide stimulus for further investigation into this area by the software and service engineering research community."
},
{
  "Title": "Towards Efficient Measuring of Web Services API Coverage",
  "Type": "Paper",
  "Key": "605e67badedd920b4eb32c46fd641c",
  "Authors": ["Waldemar Hummer", "Orna Raz", "Schahram Dustdar"],
  "Affiliations": ["Vienna University of Technology, Austria", "IBM Haifa Research Lab, Israel"],
  "Abstract": "We address the problem of interface-based test coverage for Web services. We suggest an approach to analyze the Application Programming Interface (API) of Web services, calculate the number of possible input combinations and compare it to the number of actual historical invocations. Such API coverage metrics are an indicator to which extent the service has been used. Measuring API coverage is a key concern for assessing the significance of Verification and Validation (V&V) techniques; on the other hand, API coverage metrics can also yield interesting usage reports for a service-based system in production use. The coverage metrics rely on the exact specification of service interfaces, and we provide a mechanism to specify restrictions for data types in the Java Web services framework (JAX-WS). As full enumeration of all possible inputs is often infeasible, we allow the definition of custom coverage metrics by means of domain partitioning: the user divides domain ranges into subsets, and a coverage of 100% means that the logged invocations contain at least one sample for each subset. Based on a prototype implementation, we evaluate different aspects of our approach."
},
{
  "Title": "Evaluating the Compatibility of Conversational Service Interactions",
  "Type": "Paper",
  "Key": "fa0050f0531354cef74ef893f43ee6",
  "Authors": ["Sam Guinea", "Paola Spoletini"],
  "Affiliations": ["Politecnico di Milano, Italy", "Università dell'Insubria, Italy"],
  "Abstract": "Service-oriented systems live in an open world, one in which their functionality and quality of service depend on how the services they interact with evolve. System adaptation has been indicated as a way to cope with the evolution these partner services may have. When a partner does not behave as expected, in an adaptable system we can substitute it with an alternative compatible one. Finding a compatible alternative, however, is a difficult task if we consider conversational services that impose a specific interaction protocol and specific data-types. In this paper we introduce Interaction Sequence Charts (ISC) as an effective notation for describing the interactions a service has with its partners, and an algorithm that uses these charts to establish a “degree of compatibility” between interacting services. The algorithm considers both interaction protocol requirements and data-type similarity, for which fuzzy techniques are adopted. The expressive power of ISC is validated by using it to describe the complex behaviour that can be defined using BPEL 2.0, while the algorithm is validated on an example in the field of Tele-Radiology, and shown to be advantageous in practice."
},
{
  "Title": "SMaRT: A Workbench for Reporting the Monitorability of Services from SLAs",
  "Type": "Paper",
  "Key": "b995ffd236259581086d2296f2539b",
  "Authors": ["Howard Foster", "George Spanoudakis"],
  "Affiliations": ["City University London, UK"],
  "Abstract": "Service Level Agreements (SLAs) for Software Services aim to clearly identify the service level commitments established between service requesters and providers. A dynamic configuration for the monitoring of these SLAs provides the opportunity for service monitor providers to offer and release monitoring infrastructures for different types of services. Whilst there has been work on automating this monitor matching and configuration, additional support may be needed in the negotiation and provision of monitors for which the current monitoring infrastructure does not provide suitable SLA term monitors. In this paper we describe an approach to effectively report and assist service monitoring support groups in managing this provision. The approach described is illustrated with mechanical support in the form of a SMaRT Workbench Eclipse IDE plug-in for reporting on the monitorability of SLAs for service monitoring infrastructures."
},
{
  "Title": "Identifying, Modifying, Creating, and Removing Monitor Rules for Service Oriented Computing",
  "Type": "Paper",
  "Key": "0167b87f69f47c5a929c3aadcd7313",
  "Authors": ["Ricardo Contreras", "Andrea Zisman"],
  "Affiliations": ["City University London, UK"],
  "Abstract": "Monitoring of service-based systems is considered an important activity to support service-oriented computing. Monitoring can be used to verify the behavior of a service-based system, and the quality and contextual aspects of the services participating in the system. Existing approaches for monitoring service-based systems assume that monitor rules are pre-defined and known in advance, which is not always the case. We present a pattern-based HCI-aware monitor adaptation framework to support identification, modification, creation, and removal of monitor rules based on user’s interaction with a service-based system and different types of user context. A prototype tool has been implemented to demonstrate the framework."
},
{
  "Title": "Business Process Performance Prediction on a Tracked Simulation Model",
  "Type": "Paper",
  "Key": "9e3536e3c68c07d7644c65510e1720",
  "Authors": ["Andrei Solomon", "Marin Litoiu"],
  "Affiliations": ["York University, Canada"],
  "Abstract": "Business processes need to achieve key performance indicators with minimum resources in changing operating conditions. Changes include hardware and software failures, load variation and variations in user interaction with the system. By incorporating simulation in the prediction model it is possible to predict with more confidence system performance degradations. We present our dynamic predictive model which uses forecasting techniques on historical process performance estimates for business process optimization. The parameters of the simulation model are estimates tuned at run-time by tracking the system with a particle filter."
},
{
  "Title": "Architecture-based Reliability Analysis of Web Services in Multilayer Environment",
  "Type": "Paper",
  "Key": "ff6f976815b2c3555ce4735dbb9946",
  "Authors": ["M. Rahmani", "A. Azadmanesh", "Harvey Siy"],
  "Affiliations": ["University of Nebraska-Omaha, USA"],
  "Abstract": "The reliability analysis of web services is often focused on the web service components, ignoring the impact of the middleware located beneath the web services. A service-based software system is a multilayered system that includes the web service (WS), shared resources, and the hosting application server (AS). It is conjectured that the reliability prediction of the web services is improved if the reliability model accounts for such underlying layers. The initial experiment illustrates that the AS and shared resources can impact the overall reliability of web services greatly. This observation is demonstrated by simulating the interaction between a web service and the AS."
},
{
  "Title": "Use of SPLE to Deliver Custom Solutions at Product Cost – Challenges and Way Forward",
  "Type": "Paper",
  "Key": "883f1bc423901e1683e5a9b93e5ae4",
  "Authors": ["Vinay Kulkarni"],
  "Affiliations": ["Tata Research Development and Design Centre, India"],
  "Abstract": "Need for adaptiveness of business applications is on the rise with continued increase in business dynamics. Ground-up development techniques neither deliver nor can scale in this dynamic situation. Software Product Line Engineering (SPLE) aims to increase adaptiveness by capturing commonality and variability up front to suitably configure the application from its parts. Code-centric SPLE techniques show unacceptable responsiveness when business applications are subjected to changes along multiple simultaneously evolving dimensions. Using clear separation of functional concerns from implementation platform, model driven approaches enable easy delivery of the same functionality into multiple technology platforms. However, business applications exhibit variability in several dimensions such as functionality, business process, design decisions, architecture, and technology platform. We argue that SPLE techniques need to be elevated to a higher level of abstraction to enable them to work in unison with model-driven techniques in order to realize the desired adaptiveness along all these dimensions. We have been delivering large business applications using model-driven techniques for past 15 years. In this paper, we have outlined several key challenges that we faced in adopting SPLE and presented tenets of a solution that is likely to have greater acceptance by industry practice."
},
{
  "Title": "Issues in Software Product Line Evolution: Complex Changes in Variability Models",
  "Type": "Paper",
  "Key": "c62633b1b27f050ec7da47dc480c26",
  "Authors": ["Steve Livengood"],
  "Affiliations": ["Samsung Electronics, USA"],
  "Abstract": "This paper describes industrial experience with the evolution of a software product line of multifunction printers, specifically, experience with modification of the variability model in ways that alter constraints and other relationships between variation points. Evaluating the impact of such changes has proven to be difficult in practice and is an unsolved problem for the organization described."
},
{
  "Title": "NASA Goddard Space Flight Center Core Flight System Product Line",
  "Type": "Paper",
  "Key": "72a05c4161ee5895837181ba7ca242",
  "Authors": ["David McComas", "Dharmalingam Ganesan", "Mikael Lindvall"],
  "Affiliations": ["NASA Goddard Space Flight Center, USA", "Fraunhofer CESE, USA"],
  "Abstract": "In this paper, we describe the NASA Goddard Space Flight Center's Core Flight System software product line. A brief technical description and business context is provided followed by a description of some of the current product line challenges. The intent of this paper is to highlight real-life problems associated with the adoption of the CFS product line. Current CFS research collaborations and potential future product line strategies related to the challenges are also provided."
},
{
  "Title": "On the Problems with Evolving Egemin's Software Product Line",
  "Type": "Paper",
  "Key": "804eec556f8180d58cae78d0fb30aa",
  "Authors": ["Bartosz Michalik", "Danny Weyns", "Wim Van Betsbrugge"],
  "Affiliations": ["Katholieke Universiteit Leuven, Belgium", "Egemin Automation, Belgium"],
  "Abstract": "Egemin, an industrial manufacturer of logistic systems is adopting a Software Product Line (SPL) approach to manage the development of their product portfolio. However, due to the intrinsic complexity of the logistic systems and lack of explicitly documented architectural knowledge evolution of the products is error-prone. Faulty updates increase maintenance costs and harm the company’s reputation. Therefore, Egemin searches for a systematic solution that can improve their SPL evolution strategy."
},
{
  "Title": "Migrating Towards Evolving Software Product Lines: Challenges of an SME in a Core Customer-driven Industrial Systems Engineering Context",
  "Type": "Paper",
  "Key": "285f9e978535a37159cf04ecc33a89",
  "Authors": ["Fritz Stallinger", "Robert Neumann", "Robert Schossleitner", "Stephan Kriener"],
  "Affiliations": ["Software Competence Center Hagenberg, Austria", "STIWA Automation GmbH, Austria"],
  "Abstract": "In this paper we identify key challenges a medium-sized software organization is facing in migrating towards Software Product Line Engineering (SPLE). The software engineering context of the company is characterized by a two-fold access to the market – core customer driven product enhancement and product development for a broader, anonymous market – and the embedding of software engineering in multi-disciplinary systems and solutions engineering. Based on a characterization of the business, the software product subject to migration towards SPLE, and the goals and background of the SPLE initiative, seven key challenges with respect to the migration are identified. These challenges relate to process diversity in the face of multiple reuse approaches; the management of requirements and variability; the integration of requirements traceability and variability management; legacy software and discipline- vs. software-specific modularization; integration with systems engineering; costing and pricing models; and project vs. product documentation."
},
{
  "Title": "Design and Validation of Variability in Product Lines",
  "Type": "Paper",
  "Key": "fe06e32733d537652f2277b761dfe6",
  "Authors": ["Patrizia Asirelli", "Maurice H. ter Beek", "Alessandro Fantechi", "Stefania Gnesi", "Franco Mazzanti"],
  "Affiliations": ["ISTI-CNR, Italy", "Università di Firenze, Italy"],
  "Abstract": "We propose an emerging solution technique, pushing the application of model-checking techniques to the design and validation of variability in a product line (PL), mainly aimed at those industrial domains where model-based development is adopted for the development of safety-critical systems."
},
{
  "Title": "Commonality and Variability Analysis for Resource Constrained Organizations",
  "Type": "Paper",
  "Key": "695dde743d5b3d41660f8af4d18060",
  "Authors": ["Gary Chastek", "Patrick Donohoe", "John D. McGregor"],
  "Affiliations": ["SEI/CMU, USA", "Clemson University, USA"],
  "Abstract": "This position paper describes our current work in adapting a software product line technique to the constraints of a development organization. We report on applying a commonality and variability analysis with an organization adopting a software product line approach while facing sever resource constraints because of current product development commitments. The immediate focus of the paper is on blending commonality and variability analysis into the organization’s existing requirements development process. The longer-term goal of this work is to facilitate the transition to product lines in a minimally intrusive way. The paper describes how the approach was introduced and implemented, and summarizes the benefits achieved and the issues arising from the work to date."
},
{
  "Title": "On-Demand Integration of Product Lines: A Study of Reuse and Stability",
  "Type": "Paper",
  "Key": "632c4fa2f1b0a5a34589adfef38fe3",
  "Authors": ["Alessandro Gurgel", "Francisco Dantas", "Alessandro Garcia"],
  "Affiliations": ["PUC-Rio, Brazil"],
  "Abstract": "The integration of multiple SPLs is increasingly becoming a trend to enable on-demand derivation of new products and accelerate their time-to-market. Integration of SPLs often implies the reuse of a previously-implemented feature across other SPLs. The reuse of a SPL feature is only viable if the underlying programming mechanisms enable its smooth composition within the code of other SPLs. If the required modifications are significant, the design of the target SPLs are likely to be destabilized. This paper presents an exploratory study on the integration of three product lines from the board game domain. We investigate how aspect-oriented and feature-oriented programming impact on the reuse and stability of those product lines."
},
{
  "Title": "Identifying Best Practice by Analyzing the Evolution of the FISCAN MTMSIS Software Product Line",
  "Type": "Paper",
  "Key": "3e42301008c1ad19df93d7cdc36700",
  "Authors": ["Dong Li"],
  "Affiliations": ["FISCAN and First Research Institute of Ministry of Public Security, China"],
  "Abstract": "In the face of development of a software-intensive products family in a domain, Software Product Line Engineering (SPLE) itself is a best practice that has been representatively demonstrated by the cases inducted into Software Product Line Hall of Fame. SPLE best practice is supported by various and vivid meta best practices generated from concrete activities through solution finding processes. This paper describes the evolution of the FISCAN MTMSIS software product line, nominated to the Hall of Fame at SPLC 2010. FISCAN applies SPLE in the security inspection domain and has identified SPLE best practices through years of experience in the past. This paper discusses some representative best practices and the lessons we learned during the process. More work, both development and research, are still in progress on the platform of the MTMSIS software product line and FISCAN will continue to apply SPLE in the future."
},
{
  "Title": "Smart Composition of Reusable Software Components in Mobile Application Product Lines",
  "Type": "Paper",
  "Key": "8f1989f84ef8b05ac647bcfcc71e35",
  "Authors": ["Ricardo Erikson V. S. Rosa", "Vicente F. Lucena", "Jr."],
  "Affiliations": ["Federal University of Amazonas, Brazil"],
  "Abstract": "Mobile application development opens up several challenges to developers. Among these challenges, possibly the most important one is the porting of applications to the heterogeneous devices available on the market. This requires mobile developers to create and maintain several versions of their applications in order to deal with particular features of each platform, including display size, development libraries, sensors, keypad layout, etc. The Software Product Lines (SPL) approach seems to be an useful technique to support mobile application development. A way to make SPL more effective is automating the software components composition for building mobile applications. We present a software infrastructure called AppSpotter that enables the dynamic and automated composition of software components of mobile applications taking into account the particular features of each mobile device. By means of the devices features, AppSpotter determines the components selection and composition of them to build customized mobile applications."
},
{
  "Title": "Ontology-Based Product Line Modeling and Generation",
  "Type": "Paper",
  "Key": "73509b2e3aec634379c0879cd765cf",
  "Authors": ["Harvey Siy", "Aaron Wolfson", "Mansour Zand"],
  "Affiliations": ["University of Nebraska at Omaha, USA"],
  "Abstract": "Software product line engineering defines a family of related software products. Every software product line engineering method has two essential elements, a set of models representing the product family, and a process for instantiating product members from those models. In this paper, we investigate the use of ontologies to model product lines. We also show how product members can be instantiated from an ontology-based model. We discuss our early experiences using ontologies to specify a family of workflows for a large insurance company."
},
{
  "Title": "Supporting Feature Model Configuration using a Demonstration-based Approach",
  "Type": "Paper",
  "Key": "7eb619def0933f5cf5553fa1310095",
  "Authors": ["Yu Sun", "Hyun Cho", "Jeff Gray", "Jules White"],
  "Affiliations": ["University of Alabama at Birmingham, USA", "University of Alabama, USA", "Virginia Tech, USA"],
  "Abstract": "Configuration of feature models in software product-lines typically involves manipulating a model to modify the feature selections and analyzing the model to ensure that no configuration constraints are violated. In order to capture and reuse configuration knowledge from different users, model transformation and constraint languages can be used to specify and automate the constraint checking and model manipulation processes. However, this approach presents challenges to general end-users (e.g., domain experts who may not be programmers) who do not have experience using these languages. This paper presents an end-user technique to support capture and reuse of feature model configurations."
},
{
  "Title": "Flexible Support for Managing Evolving Software Product Lines",
  "Type": "Paper",
  "Key": "16390fbdce2284d0f389de382f2e02",
  "Authors": ["Cheng Thao", "Ethan V. Munson"],
  "Affiliations": ["University of Wisconsin-Milwaukee, USA"],
  "Abstract": "In software product line engineering, core assets are shared among multiple products. Core assets and products generally evolve independently. Developers need to capture evolution in both contexts and to propagate changes in both directions between the core assets and the products. Current version control systems have no support for these tasks and this may be one reason for the slow adoption of the product line approach. We address these issues with a prototype version control system that is designed to support product line engineering, but without imposing a strong process model. The prototype is being tested on the DITA documentation standard. It supports evolution of core assets and of products, as well as propagation of changes from core assets to products and vice versa."
},
{
  "Title": "Demonstration of LMMP (Lunar Mapping and Modeling) Using Amazon's Elastic Compute Cloud",
  "Type": "Paper",
  "Key": "64e1c26a863e56a4ea165f0d4c456a",
  "Authors": ["Bach Bui", "George Chang", "Richard Kim", "Emily Law", "Shan Malhotra"],
  "Affiliations": ["Jet Propulsion Laboratory, USA"],
  "Abstract": "The Lunar Mapping and Modeling Project (LMMP) is currently being built by NASA. The goal is to provide a single point of access to the best state of knowledge of the moon’s terrain, rock and crater fields, resource maps, lighting conditions and thermal conditions. The architecture and design employ a variety of technologies, allowing for execution of complex models, the processing of large data sets and the distribution of the information, over the internet, to both authenticated users and the general public. The architecture supports a variety of light-weight clients including a Flash based display, an iPAD/iPhone interface and a set of programmatic APIs that allow rich clients to interact with the LMMP system."
},
{
  "Title": "Demonstration of LMMP Workflow System Using Cloud Computing Architecture",
  "Type": "Paper",
  "Key": "df000fbe6bd8f903e76415248af76e",
  "Authors": ["George Chang", "Emily Law", "Shan Malhotra"],
  "Affiliations": ["Jet Propulsion Laboratory, USA"],
  "Abstract": "The Lunar Mapping and Modeling Project (LMMP) is currently being built by NASA. The goal is to provide a single point of access to the best state of knowledge of the moon’s terrain, rock and crater fields, resource maps, lighting conditions and thermal conditions. The LMMP contains a workflow system that allows us to allocate jobs to remote computing resources. We will demonstrate this workflow capability."
},
{
  "Title": "Demonstration of LMMP Automated Performance Testing Using Cloud Computing Architecture",
  "Type": "Paper",
  "Key": "958e2d3419ff9c4b1186deeb013467",
  "Authors": ["George Chang", "Emily Law", "Shan Malhotra"],
  "Affiliations": ["Jet Propulsion Laboratory, USA"],
  "Abstract": "The Lunar Mapping and Modeling Project (LMMP) is currently being built by NASA. The goal is to provide a single point of access to the best state of knowledge of the moon’s terrain, rock and crater fields, resource maps, lighting conditions and thermal conditions. The project uses cloud computing scalable infrastructure to support users. This demonstration will show how cloud computing can be used to support large scale automated testing, simulating users from around the world, in a cost effective manner."
},
{
  "Title": "Building Climatological Services on the Cloud",
  "Type": "Paper",
  "Key": "5aedfd98fa708d33e40b5769159f48",
  "Authors": ["Thomas Huang", "Michael E. Gangl", "Andrew W. Bingham"],
  "Affiliations": ["Jet Propulsion Laboratory, USA"],
  "Abstract": "The NASA Physical Oceanographic Distributed Active Archive Center (PO.DAAC) at Jet Propulsion Laboratory is funded by the NASA Earth Science Data and Information System (ESDIS) project to conduct a study of cloud services for data management, data access and data processing. The study is to improve our understanding and articulate the cost/benefit of cloud technologies for the NASA Distributed Active Archive Centers (DAACs) and Science Investigator- led Production Systems (SIPs). This demonstration focuses on our experience in developing climatology services using Apache Hadoop to store and analyze temporal and spatial characteristics of scatterometer data over Antarctica."
},
{
  "Title": "edubase Cloud: An Open-source Cloud Platform for Cloud Engineers",
  "Type": "Paper",
  "Key": "d7a1d89085e26c593e1b61bcfdc86e",
  "Authors": ["Nobukazu Yoshioka", "Shigetoshi Yokoyama", "Yoshionori Tanabe", "Shinichi Honiden"],
  "Affiliations": ["National Institute of Informatics, Japan"],
  "Abstract": "Education for cloud engineers is crucial in terms of innovation in the development of cloud technologies. We propose a new cloud platform based on open-source software that uses multi-clouds for the education."
},
{
  "Title": "An Internationally Distributed Cloud for Science: The Cloud-Enabled Space Weather Platform",
  "Type": "Paper",
  "Key": "5f30465c10e03d83038877bc0cbd7e",
  "Authors": ["Everett Toews", "Barton Satchwill", "Robert Rankin", "John Shillington", "Todd King"],
  "Affiliations": ["Cybera Inc., Canada", "University of Alberta, Canada", "UC Los Angeles, USA"],
  "Abstract": "The purpose of the Cloud-Enabled Space Weather Platform (CESWP) project is to bring the power and flexibility of cloud computing to space weather physicists. The goal is to lower the barriers for the physicists to conduct their science. That is, to make it easier to collaborate with other scientists, develop space weather models, run simulations, produce visualizations and enable provenance. Success of the project is measured by the broad acceptance and use of the platform by the space weather science community. \nTo deliver cloud computing and storage, infrastructure as a service, the project has built an internationally distributed cloud based on Eucalyptus [1]. To provide a graphical user interface for the physicists to interact with we selected the Groovy programming language and the Grails web framework. To construct the software we followed the Scrum agile software development methodology. This paper will report on the motivation and risks of such an undertaking. It will also report on the suitability of Eucalyptus as a cloud framework and the utility of the tools used to build an application on top of it."
},
{
  "Title": "Modeling Cloud Failure Data: A Case Study of the Virtual Computing Lab",
  "Type": "Paper",
  "Key": "7b4ab958d7d7c214e2cb170e6af5a3",
  "Authors": ["Meiyappan Nagappan", "Aaron Peeler", "Mladen Vouk"],
  "Affiliations": ["North Carolina State University, USA"],
  "Abstract": "Virtual Computing Lab is a higher education cloud computing environment that on demand, allocates a chosen software stack on the required hardware and gives access to the customers, in this case NCSU students, faculty and staff. VCL has been in operation since 2004. An important component of the quality of the services provided by a cloud is the reliability and availability. For example, typical availability of the system exceeds 0.999, and reservation reliability is in the 0.99 range. VCL provides comprehensive information (provenance, logs, etc.) about its execution, its resources, and its performance. We mined the VCL log files to find out more about its reliability and availability, and the character of its faults and failures. This paper presents some of these results."
},
{
  "Title": "Uni4Cloud: An Approach based on Open Standards for Deployment and Management of Multi-cloud Applications",
  "Type": "Paper",
  "Key": "c6eca7bf04954798ae95d886e28121",
  "Authors": ["Americo Sampaio", "Nabor Mendonça"],
  "Affiliations": ["Universidade de Fortaleza, Brazil"],
  "Abstract": "Cloud computing is changing the way applications are being developed, deployed and managed. Application developers can focus on business and functionality and leverage infrastructure clouds (IaaS) to provide them low cost resources (e.g., computation, storage, and networking) that can be controlled based on application needs. However, current IaaS cloud developers have to deal with daunting tasks to configure and deploy their applications in different cloud providers. This paper presents the Uni4Cloud approach that facilitates to model, deploy and configure complex applications in multiple infrastructure clouds. We demonstrate through an enterprise application case study how Uni4Cloud facilitates to deploy components (e.g., application server, database, load balancer) in multiple clouds using a model-based approach that helps to automatically configure and deploy applications independent of IaaS cloud provider. Moreover, the approach is based on cloud computing standards such as the Open Virtualization Format (OVF) and Open Cloud Computing Interface (OCCI) to favor interoperability and to avoid being locked in to specific cloud providers."
},
{
  "Title": "Application Migration to Cloud: A Taxonomy of Critical Factors",
  "Type": "Paper",
  "Key": "7da7d3af49b051e77c72a96680623a",
  "Authors": ["Van Tran", "Jacky Keung", "Anna Liu", "Alan Fekete"],
  "Affiliations": ["University of New South Wales, Australia", "The Hong Kong Polytechnic University, China", "NICTA, Australia", "The University of Sydney, Australia"],
  "Abstract": "Cloud computing has attracted attention as an important platform for software deployment, with perceived benefits such as elasticity to fluctuating load, and reduced operational costs compared to running in enterprise data centers. While some software is written from scratch specially for the cloud, many organizations also wish to migrate existing applications to a cloud platform. Such a migration exercise to a cloud platform is not easy: some changes need to be made to deal with differences in software environment, such as programming model and data storage APIs, as well as varying performance qualities. We report here on experiences in doing a number of sample migrations. We propose a taxonomy of the migration tasks involved, and we show the breakdown of costs among categories of task, for a case-study which migrated a .NET n-tier application to run on Windows Azure. We also indicate important factors that impact on the cost of various migration tasks. This work contributes towards our future direction of building a framework for cost-benefit trade-off analysis that would apply to migrating applications to cloud platforms, and could help decision-makers evaluate proposals for using cloud computing."
},
{
  "Title": "Cloud Adoption: A Goal-Oriented Requirements Engineering Approach",
  "Type": "Paper",
  "Key": "960b195911a31f2157cea0710d0921",
  "Authors": ["Shehnila Zardari", "Rami Bahsoon"],
  "Affiliations": ["University of Birmingham, UK"],
  "Abstract": "We motivate the need for a new requirements engineering methodology for systematically helping businesses and users to adopt cloud services and for mitigating risks in such transition. The methodology is grounded in goal oriented approaches for requirements engineering. We argue that Goal Oriented Requirements Engineering (GORE) is a promising paradigm to adopt for goals that are generic and flexible statements of users’ requirements, which could be refined, elaborated, negotiated, mitigated for risks and analysed for economics considerations. We describe the steps of the proposed process and exemplify the use of the methodology through an example. The methodology can be used by small to large scale organisations to inform crucial decisions related to cloud adoption."
},
{
  "Title": "Evaluating Cloud Computing in the NASA DESDynI Ground Data System",
  "Type": "Paper",
  "Key": "6219143c1961b54c20f889838a42e7",
  "Authors": ["John J. Tran", "Luca Cinquini", "Chris A. Mattmann", "Paul A. Zimdars", "David T. Cuddy", "Kon S. Leung", "Oh-Ig Kwoun", "Dan Crichton", "Dana Freeborn"],
  "Affiliations": ["Jet Propulsion Laboratory, USA", "University of Southern California, USA"],
  "Abstract": "The proposed NASA Deformation, Ecosystem Structure and Dynamics of Ice (DESDynI) mission would be a first-of-breed endeavor that would fundamentally change the paradigm by which Earth Science data systems at NASA are built. DESDynI is evaluating a distributed architecture where expert science nodes around the country all engage in some form of mission processing and data archiving. This is compared to the traditional NASA Earth Science missions where the science processing is typically centralized. What's more, DESDynI is poised to profoundly increase the amount of data collection and processing well into the 5 terabyte/day and tens of thousands of job range, both of which comprise a tremendous challenge to DESDynI's proposed distributed data system architecture. In this paper, we report on a set of architectural trade studies and benchmarks meant to inform the DESDynI mission and the broader community of the impacts of these unprecedented requirements. In particular, we evaluate the benefits of cloud computing and its integration with our existing NASA ground data system software called Apache Object Oriented Data Technology (OODT). The preliminary conclusions of our study suggest that the use of the cloud and OODT together synergistically form an effective, efficient and extensible combination that could meet the challenges of NASA science missions requiring DESDynI-like data collection and processing volumes at reduced costs."
},
{
  "Title": "A Cloud-Enabled Regional Climate Model Evaluation System",
  "Type": "Paper",
  "Key": "f3749c9f96288883927409c514de1c",
  "Authors": ["Andrew F. Hart", "Cameron E. Goodale", "Chris A. Mattmann", "Paul A. Zimdars", "Dan Crichton", "Peter Lean", "Jinwon Kim", "Duane Waliser"],
  "Affiliations": ["Jet Propulsion Laboratory, USA", "University of Southern California, USA", "UC Los Angeles, USA"],
  "Abstract": "The climate research community is increasingly interested in utilizing direct, observational measurements to validate model output in an effort to tune those models to better approximate our planet’s dynamic climate. The current emphasis on performing these comparisons at regional, as opposed to global, scales presents challenges both scientific and technical, since regional ecosystems are highly heterogeneous and the available data is not readily consumed on a regional basis. If provided with a common approach for efficiently accessing and utilizing the existing observational datasets, climate researchers have the potential to effect lasting societal, economic and political benefits. A key challenge, however, is that model-to-observational comparison requires massive quantities of data and significant computational capabilities. Further complicating matters is the fact that, currently, observational data and model outputs exist in a variety of data formats, utilize varying degrees of specificity and resolution, and reside in disparate, highly heterogeneous data systems. In this paper we present a soft- ware architectural approach that leverages the advantages of cloud computing and modern open-source software technologies to address the regional climate modeling problem. Our system, dubbed RCMES, is highly scalable and elastic, allows for both local and distributed management of the satellite observations and generated model outputs, and delivers this information to climate researchers in a way that is easily integrated into existing climate simulations and statistical tools."
},
{
  "Title": "A Tale of Migration to Cloud Computing for Sharing Experiences and Observations",
  "Type": "Paper",
  "Key": "43649b73e948c8ab5a242b53dc9f22",
  "Authors": ["Muhammad Ali Babar", "Muhammad Aufeef Chauhan"],
  "Affiliations": ["IT University of Copenhagen, Denmark", "Mälardalen University, Sweden"],
  "Abstract": "Cloud computing is an emerging paradigm, which promises to make the utility computing model comprehensively implemented by using virtualization technologies. An increasing number of enterprises have started providing and using Cloud-enabled infrastructures and services. However, the advancement of cloud computing poses several new challenges to existing methods and approaches to develop and evolve software intensive systems. This paper reports our experiences and observations gained from migrating an Open Source Software (OSS), Hackystat, to cloud computing. We expect that our description of Hackystat’s architecture prior and after migration and design decisions can provide some guidance about modifying architecture of a service-based system for cloud computing. Moreover, we also hope that our experiences reported in this paper can contribute to the identification of some research questions for improving software engineering support for developing and evolving cloud-enabled systems."
},
{
  "Title": "A MapReduce Workflow System for Architecting Scientific Data Intensive Applications",
  "Type": "Paper",
  "Key": "fa97060e943e93308f2e73f661f488",
  "Authors": ["Phuong Nguyen", "Milton Halem"],
  "Affiliations": ["University of Maryland Baltimore County, USA"],
  "Abstract": "MapReduce is promising for developing both scalable business and scientific data intensive applications. However, there are few existing scientific workflow systems which can benefit from the MapReduce programming model. We propose a workflow system for integrating structure, and orchestrating MapReduce jobs for scientific data intensive workflows. The system consists of a simple workflow design C++ API, a job scheduler, and a runtime support system for Hadoop or Sector/Sphere frameworks. A climate satellite data intensive processing and analysis application is developed as a use case and an evaluation for the workflow system. The evaluation shows that it is possible to make the steps in the climate data intensive application automatically from data gridding to complex data analysis using the workflow system. The performance of the climate analysis application is significantly improved by the enabled MapReduce workflow system compared with the sequential embarrassing parallel methods. The overhead of the workflow system is negligible. However, the graphic user interface is still under development for the workflow system."
},
{
  "Title": "An Application Architecture to Facilitate Multi-Site Clinical Trial Collaboration in the Cloud",
  "Type": "Paper",
  "Key": "326f486842f29265ca56ad29ede21c",
  "Authors": ["Jonathan Sharp"],
  "Affiliations": ["City of Hope, USA"],
  "Abstract": "The regulatory environment in the U.S. healthcare sector and the privacy concerns surrounding personal health information complicate research collaborations between investigators, especially collaborations across healthcare organizational boundaries. This paper examines software systems traditionally employed by healthcare providers to utilize clinical data for research purposes within and between organizations. A conceptual software architecture utilizing cloud-based services is then proposed that, it is suggested, may facilitate collaboration between researchers in multi-site clinical trials. Several related challenge areas are then identified."
},
{
  "Title": "A Literature Review of Agile Practices and Their Effects in Scientific Software Development",
  "Type": "Paper",
  "Key": "fe044764b6f30b977205314141b987",
  "Authors": ["Magnus Thorstein Sletholt", "Jo E. Hannay", "Dietmar Pfahl", "Hans Christian Benestad", "Hans Petter Langtangen"],
  "Affiliations": ["University of Oslo, Norway", "Simula Research Laboratory, Norway", "Lund University, Sweden"],
  "Abstract": "The nature of scientific research and the development of scientific software have similarities with processes that follow the agile manifesto: responsiveness to change and collaboration are of the utmost importance. But how well do current scientific software development processes match the practices found in agile development methods, and what are the effects of using agile practices in such processes? In order to investigate this, we conduct a literature review, focusing on evaluating the agility present in a selection of scientific software projects. Both projects with intentionally agile practices and projects with a certain degree of agile elements are taken into consideration. In the agility assessment, we define and utilize an agile mapping chart. The elements of the mapping chart are based on Scrum and XP, thus covering two of the most prominent agile reference models. We compared the findings of the literature review to results of a previously conducted survey. The comparison indicates that scientific software development projects adopting agile practices perceive their testing to be better than average. No difference to average projects was perceived regarding requirements-related activities. Future work includes an in-depth case study to further investigate the existence and impact of agility in three large scientific software projects, ultimately aiming at a better understanding of the particularities involved in developing scientific software."
},
{
  "Title": "Supporting the Testing of Scientific Frameworks with Software Product Line Engineering - A Proposed Approach",
  "Type": "Paper",
  "Key": "a6b0b7d762b5bc53d2cf751ea0c5f6",
  "Authors": ["Hanna Remmel", "Barbara Paech", "Christian Engwer", "Peter Bastian"],
  "Affiliations": ["University of Heidelberg, Germany"],
  "Abstract": "Testing scientific software involves dealing with special challenges like missing test oracle and different possible sources of a problem. When testing scientific frameworks, additionally a large variety of mathematical algorithms and possible applications for the framework has to be handled. We propose to use concepts of software product line engineering to handle this variability. The contribution of this paper is a two-step process for reengineering a variability model out of a framework for scientific software. This process is explained with a real case study. Furthermore, we sketch how the variability model can be used to systematically derive system test applications for the framework."
},
{
  "Title": "On the Object-oriented Design of Reference-counted Shadow Objects",
  "Type": "Paper",
  "Key": "e0be6bd69a8c0a0c3201d13181af95",
  "Authors": ["Karla Morris", "Damian W. I. Rouson", "Jim Xia"],
  "Affiliations": ["Sandia National Laboratories, USA", "IBM Toronto Lab, Canada"],
  "Abstract": "The object-oriented programming (OOP) constructs of Fortran 2003 facilitate an elegant memory management solution of particular value when Fortran drives a second language that does not provide automatic garbage collection. More specifically, when Fortran derived types shadow a companion language’s objects, safe and economical execution requires destroying the companion objects when and only when all corresponding shadows have expired. This paper focuses on the object-oriented design (OOD) of reference-counted shadow objects. The presented class structure automatically controls the lifetimes of any shadow objects that extend a universal parent class. The paper also discusses a relevant use case in ForTrilinos, a set of object-oriented Fortran interfaces to C++ packages in the Trilinos parallel numerical solver library."
},
{
  "Title": "Improving CSE Software through Reproducibility Requirements",
  "Type": "Paper",
  "Key": "fdd6e2acfa11219b0dcd5c364a38f0",
  "Authors": ["Michael A. Heroux"],
  "Affiliations": ["Sandia National Laboratories, USA"],
  "Abstract": "It is often observed that software engineering (SE) processes and practices for computational science and engineering (CSE) lag behind other SE areas. This issue has been a concern for funding agencies, since new research increasingly relies upon and produces computational tools. At the same time, CSE research organizations find it difficult to prescribe formal SE practices for funded projects. \nTheoretical and experimental science rely heavily on independent verification of results as part of the scientific process. Computational science should have the same regard for independent verification but it does not. \nIn this paper, we present an argument for using reproducibility and independent verification requirements as a driver to improve SE processes and practices. We describe existing efforts that support our argument, how these requirements can impact SE, challenges we face, and new opportunities for using reproducibility requirements as a driver for higher quality CSE software."
},
{
  "Title": "Velo: Riding the Knowledge Management Wave for Simulation and Modeling",
  "Type": "Paper",
  "Key": "59ba8fbb0273c42c929cd32bb99a15",
  "Authors": ["Ian Gorton", "Chandrika Sivaramakrishnan", "Gary Black", "Signe White", "Sumit Purohit", "Michael Madison", "Karen Schuchardt"],
  "Affiliations": ["PNNL, USA"],
  "Abstract": "Modern scientific enterprises are inherently knowledge-intensive. In general, scientific studies in domains such as geosciences, climate, and biology require the acquisition and manipulation of large amounts of experimental and field data in order to create inputs for large-scale computational simulations. The results of these simulations must then be analyzed, leading to refinements of inputs and models and additional simulations. Further, these results must be managed and archived to provide justifications for regulatory decisions and publications that are based on these models. In this paper we introduce our Velo framework that is designed as a reusable, domain independent knowledge management infrastructure for modeling and simulation. Velo leverages, integrates, and extends open source collaborative and content management technologies to create a scalable and flexible core platform that can be tailored to specific scientific domains. We describe the architecture of Velo for managing and associating the various types of data that are used and created in modeling and simulation projects, as well as the framework for integrating domain-specific tools. To demonstrate a realization of Velo, we describe the Geologic Sequestration Software Suite (GS3) that has been developed to support geologic sequestration modeling. This provides a concrete example of the inherent extensibility and utility of our approach."
},
{
  "Title": "Reengineering a Scientific Software and Lessons Learned",
  "Type": "Paper",
  "Key": "9190db92f6bcb5dcffadc91d77d674",
  "Authors": ["Yang Li"],
  "Affiliations": ["TU München, Germany"],
  "Abstract": "SeisSol is a scientific software for the numerical simulation of seismic wave phenomena. However, there are three main problems in the SeisSol project. First, the project documentation is incomplete. Second, the source code comprehensibility is low. Third, the dependencies between the modules in the system are complicated. To solve the problems and to enhance the software quality, we perform a reengineering process on SeisSol. The process contains four steps, reverse engineering, requirements reengineering, redesign and source code refactoring. In the requirements reengineering step, we employ a novel approach to elicit requirements efficiently for such a scientific computing project. \nThrough the reengineering process, the documentation of source code, requirements and the improved design is generated, the system is more modularized and easier to be extended, as well as the source code are more comprehensible. We also discuss the lessons learned during the reengineering process."
},
{
  "Title": "Mind the Gap! Bridging the Dichotomy of Design and Implementation",
  "Type": "Paper",
  "Key": "a92476630c29bd1a5fe50046390622",
  "Authors": ["Donna Kaminskyj Long", "Liam Kiemele", "Celina Gibbs", "Andrew Brownsword", "Yvonne Coady"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "This paper presents a revamping of a sparse linear algebra design pattern, targeting parallelization within scienti c and engineering applications. A proof of concept implementation is developed to compare actual software practices and optimizations with those described in the original design pattern. The case study reveals that the design pattern did not tightly coincide with the design decisions in the implementation. The proposed revised pattern takes these decisions into account more explicitly, re ning the structural representation of the pattern to make it more accessible to scientifi c developers attempting to achieve the bene ts of parallelization now available in commodity systems."
},
{
  "Title": "Towards Production Monitoring of Application Progress",
  "Type": "Paper",
  "Key": "bc22c88e53fe4c456bbd92a87e97be",
  "Authors": ["Jonathan Cook", "Hadi Sharifi", "Amir Farrahi"],
  "Affiliations": ["New Mexico State University, USA"],
  "Abstract": "We present here a vision for better information-based management of high-performance computing resources and the long-running scientific applications that use them."
},
{
  "Title": "Evolving Requirements in Patient-Centered Software",
  "Type": "Paper",
  "Key": "9488cf1669e1870ecb298cac9127ab",
  "Authors": ["Kristina Winbladh", "Hadar Ziv", "Debra J. Richardson"],
  "Affiliations": ["University of Delaware, USA", "UC Irvine, USA"],
  "Abstract": "The implications of an aging U.S. population indicate that a large portion of the population will receive limited access to the healthcare they need, unless clinical preventive services are provided. Patient-centered healthcare, in which patients gain more access to and control over their own health, is becoming an important part of clinical preventive services and so is software. Healthcare entails highly complex processes that require substantial communication between different healthcare professionals. A major concern for patient-centered software is that it must adapt to changing needs to support long-term wellbeing, i.e., new knowledge must be considered continuously as part of the software lifecycle. This position paper contends that research efforts should be directed toward software engineering solutions that consider evolution as a part of the software lifecycle and use a variety of feedback channels to direct evolution, and presents a research agenda integrated with an approach that addresses evolving needs through a continuous data-driven requirements engineering (RE) technique."
},
{
  "Title": "A Preliminary Study of Apparent Causes and Outcomes of Reported Failures with Patient Management Software",
  "Type": "Paper",
  "Key": "3c2219002903356b6ce700c4eae048",
  "Authors": ["Jens H. Weber-Jahnke"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "Patient management software (PMS) plays an increasingly critical role in the operation of modern health care systems. While more PMS systems are coming online, many practitioners have raised concerns regarding the quality of such systems, in particular with respect to safety. Recently, the software engineering research community has stepped up efforts investigating the specific challenges in health care in order to translate best engineering practices to this domain. While incidental reports about failures of PMS have been published, little information is available about their outcome and severity. Such evidence would be useful for focusing research efforts. In this paper, we report on a study that seeks to contribute to building up such evidence. We analyzed FDA records of safety incidents with PMS that were submitted over a period of two years. This paper reports on our findings and shows that patient misidentification, end-user customization, and HCI issues are among the most frequent causes for failure."
},
{
  "Title": "A Personalized Medical Information System",
  "Type": "Paper",
  "Key": "660898c77c0350c4ce9144aa653a57",
  "Authors": ["Sebastian Klenk", "Jürgen Dippon", "Peter Fritz", "Gunther Heidemann"],
  "Affiliations": ["Stuttgart University, Germany", "IDM-Foundation, Germany"],
  "Abstract": "Shared decision making is not just a question of collecting enough information, but mostly of gathering the right information and evaluating it correctly. The ever growing amount of available data and the constantly increasing specialization in medicine makes it almost impossible for a patient to get personalized medical information even though this is crucial for a self determined decision. We therefore propose a system that combines epidemiological data, personal medical data, personal data and publicly available data to form one central source of information. We argue that, with currently available methods and data, a patient adapted information system is attainable."
},
{
  "Title": "Precise Process Definitions for Activities of Daily Living: A Basis for Real-time Monitoring and Hazard Detection",
  "Type": "Paper",
  "Key": "19f51d07ad22e4235eee4261f06cc9",
  "Authors": ["Zongfang Lin", "Allen R. Hanson", "Leon J. Osterweil", "Alexander Wise"],
  "Affiliations": ["University of Massachusetts at Amherst, USA"],
  "Abstract": "The dramatically increasing population of disabled people and adults who are 65 years old and over will increase financial burdens for assisted living care in the United States and more generally on a global basis. To mitigate these costs, increasing numbers of disabled and elderly people (our clientele) will live alone at home. This paper suggests that the safety of such disabled and elderly people might be increased by using precise process definitions of Activities of Daily Living (ADLs) as the basis for guiding and monitoring their activities. We propose to model ADLs using Little-JIL, a language that supports ADL definitions that are distinguished from other ADL definitions in the literature by their use of such features as concurrency, exception handling, reaction control, and channel communication, all of which are important for monitoring ADLs at appropriately low levels of detail. \nThis paper uses making tea, making a sandwich and answering a phone, as example ADLs for process definition. It suggests how a client can be monitored in real-time to detect unsafe ADL performance deviations that may lead to hazards. It also suggests how monitoring histories can be used for automated assessments that can provide care providers/specialists with key information about trends."
},
{
  "Title": "HCMS: Conceptual Description of a Health Content Management System",
  "Type": "Paper",
  "Key": "c2f4214ae2fe2887b476e679cb7a09",
  "Authors": ["Hamman W. Samuel", "Osmar R. Zaïane"],
  "Affiliations": ["University of Alberta, Canada"],
  "Abstract": "Health is a hot topic on the Internet. Health websites are unique from other websites because they require a more acute awareness of ethical issues due to potential life threatening risks from misuse of information. We propose and give a high-level description of a Health Content Management System (HCMS) that addresses these issues and other functional needs found in most health websites. In addition, we suggest unique features that are not available in most existing health websites. Surveys of existing health websites and content management systems demonstrate the need for the proposed system. Moreover, the novelty of the proposed HCMS is appraised and asserted in comparison with similar health framework concepts. Our contributions include survey results of more than 50 health websites, a taxonomy of health websites' characteristics, and a blueprint for typical and novel features for health websites."
},
{
  "Title": "Reducing the Footprint of Certifiable Health Software during Early Stage Development",
  "Type": "Paper",
  "Key": "fa02b61470373f1b9d720c1f0f2bc9",
  "Authors": ["Jens H. Weber-Jahnke", "James B. Williams", "Craig E. Kuziemsky"],
  "Affiliations": ["University of Victoria, Canada", "University of Ottawa, Canada"],
  "Abstract": "Due to its critical nature with respect to patient safety, privacy and other quality attribute, healthcare software is often subject to third party certification. To take a solitary example, Health Canada deems patient management systems (such as Electronic Medical Records) to be subject to mandatory certification and licensing. One strategy to reduce the cost of certifying large software products is to dislocate the implementation of functions that require certification from those functions that do not require certification. While this strategy has been recommended, no systematic method has been proposed on how to integrate this notion into the overall system design process. This paper addresses this gap. We present a process that takes a requirements specification and pertinent certification criteria as input and produces a set of constraints on potential architecture designs, ensuring that the size of software that must undergo certification is reduced. We demonstrate the approach using a real-world case study involving Electronic Health Record software in the context of the Canadian regulatory framework."
},
{
  "Title": "Towards Electronic Health Record Support for Collaborative Processes",
  "Type": "Paper",
  "Key": "7a36348a90df98997c0e472aef8562",
  "Authors": ["Craig E. Kuziemsky", "James B. Williams", "Jens H. Weber-Jahnke"],
  "Affiliations": ["University of Ottawa, Canada", "University of Victoria, Canada"],
  "Abstract": "As more healthcare delivery is provided by collaborative care teams, there is a need to design tools such as electronic health records to support teams. Much of the existing EHR work has focused on semantic interoperability. While that work is important, collaborative care delivery is largely process driven, meaning that process interoperability must also be considered. This paper takes a first step towards engineering EHR requirements to support collaborative care delivery by defining a set of collaborative competencies. These competencies emphasize process interoperability through separation of data and processes. The findings from this paper can help inform EHR design to support collaborative care delivery."
},
{
  "Title": "On Effective Testing of Health Care Simulation Software",
  "Type": "Paper",
  "Key": "710082fef1e63703436a977f6b7843",
  "Authors": ["Christian Murphy", "M. S. Raunak", "Andrew King", "Sanjian Chen", "Christopher Imbriano", "Gail Kaiser", "Insup Lee", "Oleg Sokolsky", "Lori Clarke", "Leon J. Osterweil"],
  "Affiliations": ["University of Pennsylvania, USA", "Loyola University, USA", "Columbia University, USA", "University of Massachusetts at Amherst, USA"],
  "Abstract": "Health care professionals rely on software to simulate anatomical and physiological elements of the human body for purposes of training, prototyping, and decision making. Software can also be used to simulate medical processes and protocols to measure cost effectiveness and resource utilization. Whereas much of the software engineering research into simulation software focuses on validation (determining that the simulation accurately models real-world activity), to date there has been little investigation into the testing of simulation software itself, that is, the ability to effectively search for errors in the implementation. This is particularly challenging because often there is no test oracle to indicate whether the results of the simulation are correct. In this paper, we present an approach to systematically testing simulation software in the absence of test oracles, and evaluate the effectiveness of the technique."
},
{
  "Title": "Investigating the Influence of Personal Values on Requirements for Health Care Information Systems",
  "Type": "Paper",
  "Key": "adb92ec3d1715caa69dd34ea320a80",
  "Authors": ["Rumyana Proynova", "Barbara Paech", "Sven H. Koch", "Andreas Wicht", "Thomas Wetter"],
  "Affiliations": ["University of Heidelberg, Germany"],
  "Abstract": "Stakeholder requirements for health care information systems cannot be defined purely objectively. Instead they are influenced by personal and social factors. In this paper, we present preliminary insights into one such factor, namely personal values. Based on work from psychology, we have developed first instruments to elicit personal values and their relationships to software requirements by interviewing nurses and physicians. We report on these instruments and the results of applying them in two small case studies."
},
{
  "Title": "Leveraging Performance Analytics to Improve Integration of Care",
  "Type": "Paper",
  "Key": "a9b7462dc8750a64a6651297f91973",
  "Authors": ["Alain Mouttham", "Liam Peyton", "Craig E. Kuziemsky"],
  "Affiliations": ["University of Ottawa, Canada"],
  "Abstract": "The need for healthcare systems to provide efficient, effective and integrated care has put an emphasis on performance analytics. However while performance analytics can measure outcomes and suggest policy and protocol for achieving efficiency; it does not drive the actual integration of care processes. There is a need for research that develops fine-grained metrics and illustrates how to link them into the underlying clinical care processes in order to drive and support integration of care. An integrated case study of cardiac care processes and performance analytics we have been developing at a community hospital in Ontario is used to illustrate our approach. We analyze how fine-grained metrics can be linked into cardiac care processes to address high level performance objectives, and present a technology assessment to identify how software engineering support for the collection and communication of these fine-grained metrics can be provided."
},
{
  "Title": "Evaluating Access Control of Open Source Electronic Health Record Systems",
  "Type": "Paper",
  "Key": "9f24908e8f4c395ce70e316f182e65",
  "Authors": ["Eric Helms", "Laurie Williams"],
  "Affiliations": ["North Carolina State University, USA"],
  "Abstract": "Incentives and penalties for healthcare providers as laid out in the American Recovery and Reinvestment Act of 2009 have caused tremendous growth in the development and installation of electronic health record (EHR) systems in the US. For the benefit of protecting patient privacy, regulations and certification criteria related to EHR systems stipulate the use of access control of protected health information. The goal of this research is to guide development teams, regulators, and certification bodies by assessing the state of the practice in EHR access control. In this paper, we present a compilation of 25 criteria relative to access control in EHR systems found in the Health Insurance Portability and Accountability Act (HIPAA) regulation, meaningful use certification criteria, best practices embodied in the National Institute for Standards and Technology (NIST) role-based access control standard, and other best practices found in the literature. We then examine the state of the practice in access control by evaluating four open source EHR systems using these 25 evaluation criteria. Our research indicates that the NIST Meaningful Use criteria provide HIPAA compliance, but none of the regulatory and certification criteria address the implementation standards, and best practices related to access control. Additionally, our results indicate that open source EHR system designers are not implementing robust access control mechanisms for the adequate protection of patient data."
},
{
  "Title": "Developing a Virtual-World Simulation",
  "Type": "Paper",
  "Key": "c237d8179853105b6cec994309ca85",
  "Authors": ["David Chodos", "Eleni Stroulia", "Sharla King"],
  "Affiliations": ["University of Alberta, Canada"],
  "Abstract": "Simulation-based training has been an integral part of health-sciences education for many years, and is becoming increasingly important with the shift towards competency-based education. Virtual worlds have emerged as an effective way to deliver realistic, collaborative training in complex processes, which is consistent with competency-based training and assessment. We have developed MeRiTS, a virtual world-based platform for creating training simulations, to provide students in a wide range of disciplines with this kind of training. Furthermore, through these student training experiences, we will be able to provide a rigorous, comprehensive evaluation of the effectiveness of conducting scenario-based training in virtual worlds. In this paper, we briefly present the MeRiTS architecture, and the underlying theories, components and models that support the system. We then present a detailed description of our most mature scenario, which trains paramedics in proper rescue and patient handoff procedures. We also provide an in-depth discussion of the development process for this scenario, and conclude with some lessons learned from the experience."
},
{
  "Title": "COPAL-ML: A Macro Language for Rapid Development of Context-Aware Applications in Wireless Sensor Networks",
  "Type": "Paper",
  "Key": "58869c7ee74752419b2fdb5cb253ff",
  "Authors": ["Sanjin Sehic", "Fei Li", "Schahram Dustdar"],
  "Affiliations": ["Vienna University of Technology, Austria"],
  "Abstract": "Application development on wireless sensor networks is becoming more and more challenging due to increasing complexity of applications and lack of dedicated programming models. Developers should concentrate on the application logic, while network designers should ensure the network and sensor performance. However, in reality, these two roles often overlap because the architectural and programming abstraction between the network and application is missing. Research on middleware and language that bridges these two abstraction levels is still in a preliminary stage. \nThis paper proposes a macro language based on our previous work COPAL (COntext Provisioning for ALl). COPAL is a runtime context provisioning middleware that, via a loosely-coupled and composable architecture, ensures context information from wireless sensor networks and other sources can be processed for the needs of context-aware applications. COPAL-ML is a macro language that extends Java programming language and is tailored for the application development using COPAL. Its main task is to reduce development efforts, hide the inherent complexity of COPAL API, and separate concerns of the context-aware application from underlining wireless sensor network."
},
{
  "Title": "Cross-Platform Protocol Development for Sensor Networks: Lessons Learned",
  "Type": "Paper",
  "Key": "f7881b8287e13ba00cfa44d81489f0",
  "Authors": ["Marcin Brzozowski", "Hendrik Salomon", "Krzysztof Piotrowski", "Peter Langendoerfer"],
  "Affiliations": ["IHP, Germany"],
  "Abstract": "Protocols for sensor networks are commonly coupled to the specific operating system (OS), for instance TinyOS, involv- ing some drawbacks. First, programmers must learn OS architecture, programming guidelines, sometimes a new pro- gramming language, etc. Second, protocols run on the spe- cific OS only, i.e. on the hardware supported by the OS. Third, only selected network simulators, mostly provided by OS, can execute the OS-coupled code. To tackle the problems with interoperability we examined the idea of cross-platform protocol development for sensor networks, i.e. software running on several OSs, and share our experience in this paper. Our primary cross-platform MAC protocol runs on two OSs and on a hardware platform supporting the C program- ming language. To achieve interoperability we decoupled MAC from OS calls and provided a hardware abstraction layer (HAL) for each OS. We discovered that such an ap- proach does not result in a significant penalty in terms of occupied memory (e.g. smaller by 3 kB than the TinyOS- dedicated version) and consumed energy (additional over- head smaller by three orders of magnitude from total tx energy)."
},
{
  "Title": "Modeling and Analyzing Performance of Software for Wireless Sensor Networks",
  "Type": "Paper",
  "Key": "2882a657d493cab2ad572c22c5bddb",
  "Authors": ["Luca Berardinelli", "Vittorio Cortellessa", "Stefano Pace"],
  "Affiliations": ["University of L'Aquila, Italy"],
  "Abstract": "The development of software for Wireless Sensor Networks (WSN) is mostly based on code-and-fix techniques. Up today model-driven engineering techniques have only been limitedly considered in this domain, although they would enable a set of activities aimed at improving the quality of software. In this paper we investigate the possibility to adapt an existing model-based approach that exploits such techniques to combine the modeling and performance analysis of software for WSN. We introduce a UML-based framework where a system model (i) is extended with a new profile for representing NesC application along with the supporting hardware platform, and (ii) is annotated with performance parameters defined in the standard UML MARTE profile. Thereafter we apply a set of transformations to this enhanced UML model that targets a Queueing Network performance model. Finally we illustrate our approach at work on a case study in the agricultural domain."
},
{
  "Title": "On Coordination Tools in the PicOS Tuples System",
  "Type": "Paper",
  "Key": "8cae3d724afe5aa03f7f122d971a85",
  "Authors": ["Benny Shimony", "Ioanis Nikolaidis", "Pawel Gburzynski", "Eleni Stroulia"],
  "Affiliations": ["University of Alberta, Canada", "Olsonet Communications Corporation, Canada"],
  "Abstract": "In this paper, we discuss the most recent coordination extension to the PicOS-tuples environment, inspired, to a degree, by B-Threads and FACTS. We illustrate the extensions with two design patterns, highly useful in WSN computations, known as regulative superimposition and distributed detection. Those patterns are employed in a debugging protocol that retrieves snapshots of node states. We demonstrate how our new idioms can be propitious for separating concerns in WSN programming using tuples."
},
{
  "Title": "On Mining Sensor Network Software Repositories",
  "Type": "Paper",
  "Key": "345280cd8bdae988e704a14795467e",
  "Authors": ["Andreas Loukas", "Matthias Woehrle", "Koen Langendoen"],
  "Affiliations": ["Delft University of Technology, Netherlands"],
  "Abstract": "Wireless Sensor Network (WSN) software is typically developed in one of the two prominent WSN operating systems: TinyOS or Contiki. Both of these operating systems are open-source projects and basically frameworks for WSN developers. In this paper, we study the software repositories of these two projects. Software repositories provide a wealth of information on software projects and their development. Based on the mined information, we explore the TinyOS and Contiki commit history and compare them to an open-source embedded operating system, Ethernut. As a second step, we explore WSN-specific artifacts and mine TinyOS software for cross-cutting concerns. Most of the relations we find are not cross-cutting. Nevertheless, we do find cross-cutting concerns that are resource-related."
},
{
  "Title": "Model Driven Development for Rapid Prototyping and Optimization of Wireless Sensor Network Applications",
  "Type": "Paper",
  "Key": "b931c14a2bf65c3f5da6fa9b9bc749",
  "Authors": ["Ryo Shimizu", "Kenji Tei", "Yoshiaki Fukazawa", "Shinichi Honiden"],
  "Affiliations": ["Waseda University, Japan", "National Institute of Informatics, Japan", "The University of Tokyo, Japan"],
  "Abstract": "In order to develop Wireless Sensor Network (WSN) applications, it is necessary to develop prototypes in a low-cost way and to optimize application performance. Existing development approaches enable to develop a low-cost prototype by concealing the detail of WSN from the developers. However, there is a trade-off between the development cost of prototype and the description capability needed to optimize the application performance. We propose a Model-Driven Development (MDD) process to enable a low-cost prototyping and detailed optimization. To enable such a development process, we define modeling languages, which describe an application at three abstraction levels, and transformation rules, which transform models described by our modeling language to concrete one. Using our process, in prototyping, the developer describes a model by using the modeling language at the highest abstraction level and automatically obtains an executable model by using transformation rules. In addition, in optimizing, the developer can automatically obtain the models at more concrete abstraction level than the prototype by using transformation rules and modifies them in greater detail by using each modeling language."
},
{
  "Title": "sMapReduce: A Programming Pattern for Wireless Sensor Networks",
  "Type": "Paper",
  "Key": "558189bd75cf4c4f61d417687ada17",
  "Authors": ["Vikram Gupta", "Eduardo Tovar", "Luis Miguel Pinho", "Junsung Kim", "Karthik Lakshmanan", "Ragunathan Rajkumar"],
  "Affiliations": ["Polytechnic Institute of Porto, Portugal", "CMU, USA"],
  "Abstract": "Wireless Sensor Networks (WSNs) are increasingly used in various application domains like home-automation, agriculture, industries and infrastructure monitoring. As applications tend to leverage larger geographical deployments of sensor networks, the availability of an intuitive and user- friendly programming abstraction becomes a crucial factor in enabling faster and more efficient development, and re- programming of applications. Therefore, in this paper, we propose a programming pattern named sMapReduce, inspired by the Google MapReduce framework, for mapping application behaviors on to a sensor network and enabling complex data aggregation. The proposed pattern requires a user to create a network-level application in two functions: sMap and Reduce, in order to abstract away from the low- level details without sacrificing the freedom to develop complex logic. Such a two-fold division of programming logic is a natural-fit to typical sensor networking operation which makes sensing and topological modalities accessible to the user."
},
{
  "Title": "Lazy Preemption to Enable Path-Based Analysis of Interrupt-Driven Code",
  "Type": "Paper",
  "Key": "e3a932610ec67d8786b2af0bed5421",
  "Authors": ["Wei Le", "Jing Yang", "Mary Lou Soffa", "Kamin Whitehouse"],
  "Affiliations": ["University of Virginia, USA"],
  "Abstract": "One of the important factors in ensuring the correct functionality of wireless sensor networks (WSNs) is the reliability of the software running on individual sensor nodes. Research has shown that path-sensitive static analysis is effective for bug detection and fault diagnosis; however, path-sensitive analysis is prohibitively expensive when applied to a WSN application due to the large state space caused by arbitrary interrupt preemptions. In this paper, we propose a new execution model called lazy preemption that reduces this state space by restricting interrupt handlers to a set of pre-determined preemption points, if possible. This execution model allows us to represent the program with an inter-interrupt control flow graph (IICFG), which is easier to analyze than the original CFGs with arbitrary interrupt preemptions."
},
{
  "Title": "The Smart Condo: Integrating Sensor Networks and Virtual Worlds",
  "Type": "Paper",
  "Key": "aee2783ecbd449b25261db6ebbb482",
  "Authors": ["Veselin Ganev", "David Chodos", "Ioanis Nikolaidis", "Eleni Stroulia"],
  "Affiliations": ["University of Alberta, Canada"],
  "Abstract": "The term “Smart Home” refers to a home that has both a set of sensors to observe the environment and a set of actuators to automatically control home devices to improve the occupants’ experience. A Smart Home has the potential to provide a variety of services based on information gleaned by data recorded by a multiplicity of sensor types, but also requires a systematic means (a) of abstracting differences among sensor types and (b) of supporting services and their rapid development. In this paper, we describe our approach towards meeting those needs, including a component of sensor adapters and a language for modeling person activities that can guide the development of health-care services."
},
{
  "Title": "Challenges of Satisfying Multiple Stakeholders: Quality of Service in the Internet of Things",
  "Type": "Paper",
  "Key": "3aba5a9273a1fecdad4836932ad7a9",
  "Authors": ["Chien-Liang Fok", "Christine Julien", "Gruia-Catalin Roman", "Chenyang Lu"],
  "Affiliations": ["University of Texas at Austin, USA", "Washington University at St. Louis, USA"],
  "Abstract": "As wireless sensor networks become increasingly integrated with the Internet, the Internet of Things (IOT) is beginning to emerge. The IOT is a massive cyber-physical system that presents many software engineering challenges. One fundamental challenge is the need for multi-dimensional QoS that can satisfy the individual constraints of the many participants in the system. In this paper, we investigate the challenges of providing such a mechanism via a simple abstraction consisting of a general QoS function provided by each application. This function distills the multi-dimensional QoS specifications from each stakeholder into a single value that is used to determine the best configuration of interactions. We prototype our approach in a real wireless sensor network using a pervasive healthcare fall-detection application and highlight the many challenges it unveils."
},
{
  "Title": "Firm Firmware and Apps for the Internet of Things",
  "Type": "Paper",
  "Key": "a36831037ac5fd39900f110a8850d9",
  "Authors": ["Matthias Kovatsch"],
  "Affiliations": ["ETH Zurich, Switzerland"],
  "Abstract": "Among the challenges for the Internet of Things, two stand out: a scalable application layer with wide interoperability and a common, reliable programming model. We propose to strip all application logic from the firmware and only provide a RESTful interface to the hardware functionality. Critical parts such as the network stack remain in the immutable firmware that is maintained by experts. Applications are developed atop the resource abstraction and run in the cloud. Leaving the embedded domain, application development is eased while sharing and customizing applications helps to cope with the vast number of diversified device types."
},
{
  "Title": "Model-Driven Design plus Artificial Intelligence for Wireless Sensor Networks Software Development",
  "Type": "Paper",
  "Key": "689bf6ea59394df65d1b3ac64e92f7",
  "Authors": ["Peter Mülders", "Stefan Gruner", "Nguyen Xuan Thang"],
  "Affiliations": ["University of Pretoria, South Africa", "University of Kassel, Germany"],
  "Abstract": "To date, software development for wireless sensor network nodes is still characterized by low-level ad-hoc programming to a large extent. This short-paper argues for a methodologically more systematic approach on the basis of Model-Driven Engineering with CASE-Tool support, in combination with AI-based design choice optimizations during the MDE-steps of model-refinement and information-enrichment. Our work in this project is ongoing."
},
{
  "Title": "Toward a Unified Object Model for Cyber-Physical Systems",
  "Type": "Paper",
  "Key": "8ca3cc9c6d29173b379d1079d5c0c1",
  "Authors": ["Yu David Liu"],
  "Affiliations": ["SUNY Binghamton, USA"],
  "Abstract": "Cyber-physical systems are a coordinated combination of computational and physical elements. This position paper calls for the design of a unified object model that blends the boundary of the two. The unified object model has the benefits of bringing classic software engineering technologies and tools -- such as UML -- to the new application domain of cyber-physical systems, and further equipping programs written for these systems with the traditional strengths of object-oriented languages, such as encapsulation, code reuse and customization, and strong guarantees for avoiding run-time errors."
},
{
  "Title": "An Initial Study on the Use of Execution Complexity Metrics as Indicators of Software Vulnerabilities",
  "Type": "Paper",
  "Key": "e8b1602433a1a562c8532199790a39",
  "Authors": ["Yonghee Shin", "Laurie Williams"],
  "Affiliations": ["DePaul University, USA", "North Carolina State University, USA"],
  "Abstract": "Allocating code inspection and testing resources to the most problematic code areas is important to reduce development time and cost. While complexity metrics collected statically from software artifacts are known to be helpful in finding vulnerable code locations, some complex code is rarely executed in practice and has less chance of its vulnerabilities being detected. To augment the use of static complexity metrics, this study examines execution complexity metrics that are collected during code execution as indicators of vulnerable code locations. We conducted case studies on two large size, widely-used open source projects, the Mozilla Firefox web browser and the Wireshark network protocol analyzer. Our results indicate that execution complexity metrics are better indicators of vulnerable code locations than the most commonly-used static complexity metric, lines of source code. The ability of execution complexity metrics to discriminate vulnerable code locations from neutral code locations and to predict vulnerable code locations vary depending on projects. However, the vulnerability prediction models using execution complexity metrics are superior to the models using static complexity metrics in reducing inspection effort."
},
{
  "Title": "Security Policy Foundations in Context UNITY",
  "Type": "Paper",
  "Key": "23e91a4adbc3d196c76e0a88debe47",
  "Authors": ["M. Todd Gamble", "Rose F. Gamble", "Matthew L. Hale"],
  "Affiliations": ["University of Tulsa, USA"],
  "Abstract": "Security certification includes assessing an information system to verify its compliance with diverse, pre-selected security controls. The goal of certification is to identify where controls are implemented correctly and where they are violated, creating potential vulnerability risks. Certification complexity is magnified in software composed of systems of systems where there are limited formal methodologies to express management policies, given a set of security control properties, and verify them against the interaction of the participating components and their individual security policy implementations. In this paper, we extend Context UNITY, a formal, distributed, and context aware coordination language to support policy controls. The new language features enforce security controls and provide a means to declare policy specifics in a manner similar to declaring variable types. We use these features in a specification to show how verifying system compliance with selected security controls, such as those found in the NIST SP800-53 document, can be accomplished."
},
{
  "Title": "Preserving Security Properties under Refinement",
  "Type": "Paper",
  "Key": "2d77a427ac03fc180ab86ed9745d93",
  "Authors": ["Fabio Martinelli", "Ilaria Matteucci"],
  "Affiliations": ["IIT-CNR, Italy"],
  "Abstract": "Communication is one of the cornerstone of our everyday life. Guaranteeing the security of a communication is a very important challenge. In this paper, we propose a formal top-down approach for assuring that security properties are preserved during the development of a complex and concurrent system, i.e., within passage from specification to implementation of the components of the system. Indeed, we investigate on the set of requirements a refinement function has to satisfy for preserving a class of properties that can be formalized as specific instances of a general scheme, called Generalized Non Deducibility on Composition (GNDC). Hence, we show that it is possible to guarantee that the refinement of a considered system that is verified to be GNDC at a high level of abstraction, is GNDC also at a lower one without checking it again."
},
{
  "Title": "A Conceptual Meta-Model for Secured Information Systems",
  "Type": "Paper",
  "Key": "a01f54b621d17f2c72b724d4f4cb9b",
  "Authors": ["Nadira Lammari", "Jean-Sylvain Bucumi", "Jacky Akoka", "Isabelle Comyn-Wattiau"],
  "Affiliations": ["CNAM, France"],
  "Abstract": "Over the past years, research on specifying, designing and developing secured information systems (IS) has been very active. Some contributions have focused on integrating security aspects, mainly access control mechanisms, at the implementation phase. Others pay a particular attention to the capture and analysis of security requirements. However, to our knowledge, no method addresses the whole problem of the specification of security requirements and their transformation through all the phases of the IS development life cycle. We argue that better Secured IS can be obtained if security issues are taken into account at an earlier phase of the system life cycle and integrated with functional aspects along the whole life cycle. This paper is a step forward to a comprehensive security conceptual meta-model encompassing the main security properties such as availability, integrity, confidentiality, and accountability. It integrates functional and non-functional requirements. It includes social, organizational as well as informational aspects. This meta-model is the backbone of our approach."
},
{
  "Title": "Composition of Least Privilege Analysis Results in Software Architectures (Position Paper)",
  "Type": "Paper",
  "Key": "de5e2a14122966d344945f708af1ec",
  "Authors": ["Koen Buyens", "Riccardo Scandariato", "Wouter Joosen"],
  "Affiliations": ["Katholieke Universiteit Leuven, Belgium"],
  "Abstract": "Security principles are often neglected by software architects, due to the lack of precise definitions. This results in potentially high-risk threats to systems. Our own previous work tackled this by introducing formal foundations for the least privilege (LP) principle in software architectures and providing a technique to identify violations to this principle. This work shows that this technique can scale by composing the results obtained from the analysis of the sub-parts of a larger system. The technique decomposes the system into independently described subsystems and a description listing the interactions between these subsystems. These descriptions are thence analyzed to obtain LP violations and subsequently composed to obtain the violations of the overall system."
},
{
  "Title": "Towards Transformation Guidelines from Secure Tropos to Misuse Cases (Position Paper)",
  "Type": "Paper",
  "Key": "ecd6791584724b6485aec35b5a6ae4",
  "Authors": ["Naved Ahmed", "Raimundas Matulevičius"],
  "Affiliations": ["University of Tartu, Estonia"],
  "Abstract": "Progressive increase in developing secure information systems (IS) requires that the security concerns should be properly articulated well ahead in early requirement engineering (RE) along with other functional and non-functional requirements. In this paper, based on the domain model for IS security risk management (SRM) we propose a set of transformation guidelines to translate Secure Tropos models to the misuse case diagrams. We believe that such a model translation would help developers to elicit real security needs by integrating the security analysis starting from early requirement stages to all the stages of development process. The translation aligns the IS security concerns with functional requirements and maintains traceability of the security decisions to their origin."
},
{
  "Title": "PEASOUP: Preventing Exploits Against Software of Uncertain Provenance (Position Paper)",
  "Type": "Paper",
  "Key": "a96b2d047ea8cbcd9f5c23787298e9",
  "Authors": ["Michele Co", "Jack W. Davidson", "Jason D. Hiser", "John C. Knight", "Anh Nguyen-Tuong", "David Cok", "Denis Gopan", "David Melski", "Wenke Lee", "Chengyu Song", "Thomas Bracewell", "David Hyde", "Brian Mastropietro"],
  "Affiliations": ["University of Virginia, USA", "Grammatech Inc., USA", "Georgia Institute of Technology, USA", "Raytheon Inc., USA"],
  "Abstract": "Because software provides much of the critical services for modern society, it is vitally important to provide methodologies and tools for building and deploying reliable software. While there have been many advances towards this goal, much research remains to be done. For example, a recent evaluation of five state-of-the-art C/C++ static analysis tools applied to a corpus of code containing common weaknesses revealed that 41% of the potential vulnerabilities were detected by no tool. The problem of deploying resilient software is further complicated because modern software is often assembled from components from many sources. Consequently, it is difficult to know who built a particular component and what processes were used in its construction. Our research goal is to develop and demonstrate technology that provides comprehensive, automated techniques that allow end users to safely execute new software of uncertain provenance. This paper presents an overview of our vision for realizing these goals and outlines some of the challenging research problems that must be addressed to realize our vision. We call our vision PEASOUP and have begun implementing and evaluating these ideas."
},
{
  "Title": "Power Analysis Attack and Countermeasure on the Rabbit Stream Cipher (Position Paper)",
  "Type": "Paper",
  "Key": "f7380590a14088392eba164d08b4a2",
  "Authors": ["KiSeok Bae", "MahnKi Ahn", "HoonJae Lee", "JaeCheol Ha", "SangJae Moon"],
  "Affiliations": ["Kyungpook National University, Korea", "Defence agency for Technology and Quality, Korea", "Dongseo University, Korea", "Hoseo University, Korea"],
  "Abstract": "Recently, there has been extensive research on mobile devices and stream cipher to increase security. The Rabbit stream cipher was selected for the final eSTREAM portfolio organized by EU ECRYPT and as one of algorithms of the ISO/IEC 18033-4 Stream Ciphers on ISO Security Standardization. As the Rabbit evaluated the complexity of side-channel analysis attack as ‘medium’ in a theoretical approach, the method of correlation power analysis attack and the feasibility of a practical power analysis attack in the experiments are described in this paper. We also propose a countermeasure with random masking and hiding schemes for linear operation. We construct the algorithm of the countermeasure with an additional operating time of 24% with 12.3% increased memory requirements to maintain high-speed performance. We use an eight-bit RISC AVR microprocessor (ATmega 128L) to implement our methods to show that the proposed method is secure against correlation power analysis attacks in practical experiments."
},
{
  "Title": "Architecting with Just Enough Information",
  "Type": "Paper",
  "Key": "b5b287466fd8e704b6ced10a763d6f",
  "Authors": ["Robert L. Nord", "Nanette Brown", "Ipek Ozkaya"],
  "Affiliations": ["SEI/CMU, USA"],
  "Abstract": "We learned an important lesson recently about breaking down barriers among architects, developers, and other stakeholders when we were engaged on a project and were challenged to deliver the architecture in smaller increments and shorter iterations. We learned how information was used and exchanged among key players participating in the software development process and are seeking to formalize our understanding through principles of workflow from lean software development and how architecture knowledge management can influence defining an appropriate architecture batch size for effective incremental development."
},
{
  "Title": "Building Roadmaps: A Knowledge Sharing Perspective",
  "Type": "Paper",
  "Key": "b065b6c910af50618c004522a72959",
  "Authors": ["Antony Tang", "Taco de Boer", "Hans van Vliet"],
  "Affiliations": ["VU University Amsterdam, Netherlands", "Océ Technologies, Netherlands"],
  "Abstract": "Roadmapping is a process that involves many stakeholders and architects. In an industry case, we have found that a major challenge is to exchange timely knowledge between these people. We report a number of knowledge sharing scenarios in the roadmapping process. In order to address these issues, we propose a codification mechanism that makes use of a semantic wiki to facilitate knowledge sharing."
},
{
  "Title": "Goals, Questions and Metrics for Architectural Decision Models",
  "Type": "Paper",
  "Key": "5384bea40f7579ac60c6ca51944a2b",
  "Authors": ["Marcin Nowak", "Cesare Pautasso"],
  "Affiliations": ["University of Lugano, Switzerland"],
  "Abstract": "Architectural decisions are the key element behind the design process leading to a software architecture. Making software architects aware of the implications of their decisions is only the beginning of what can be achieved by capturing the rationale and the constraints influencing the decision making process in a reusable body of architectural knowledge. In this paper we propose a metric-based approach to the analysis of architectural decision models. Using a hierarchically-structured approach we identify a number of useful goals and stakeholders involved in the architectural design process. Next, we sketch a set of metrics to provide data for the evaluation of the aforementioned goals. Our aim is to stimulate a discussion on how to find indicators relevant for software architects by measuring the intrinsic properties of architectural knowledge."
},
{
  "Title": "Using Rationale to Drive Product Line Architecture Configuration",
  "Type": "Paper",
  "Key": "c1514b9850ecd3ab7a35bbc780f412",
  "Authors": ["Janet E. Burge", "Gerald C. Gannod", "Holly L. Connor"],
  "Affiliations": ["Miami University, USA"],
  "Abstract": "The process of designing and building a software system requires making many decisions. These decisions, the al- ternatives considered, and the reasons behind the choices comprise the rationale for the completed system. The driv- ing force behind many, if not most, of these decisions is the need to meet the stakeholder requirements for the system being developed. Software product line approaches allow developers to design and develop families of products that share a common platform of behaviors and infrastructure. These approaches are based on assembling a con guration of a set of common features (commonalities) along with a set of product specifi c features (variabilities) to form a new product with a low amount of e ort. In this context, these variabilities represent a wide variety of potential design al- ternatives. The goal of our research is to bring the end-user into the process of con guring a software product through the use of system level rationale that maps product line features to system requirements. Speci cally, in our approach we specify rationale at the level of a feature diagram. Accordingly, we are taking advantage of the natural correlation between alternative features in a feature diagram and the alternative structure used in design rationale. This allows the end-user to indicate which requirements apply to their product and to have that selection generate a set of product features that satisfy those requirements."
},
{
  "Title": "Codifying Architecture Knowledge to Support Online Evolution of Software Product Lines",
  "Type": "Paper",
  "Key": "b750e68e9e0560a66d05084f33e894",
  "Authors": ["Danny Weyns", "Bartosz Michalik"],
  "Affiliations": ["Katholieke Universiteit Leuven, Belgium"],
  "Abstract": "A company's architecture knowledge is often personalized across specific people that share experience and knowledge in the field. However, this knowledge may be important for other stakeholders. Omitting the codification of the architecture knowledge may result in ad-hoc practices, which is particularly relevant for software evolution. In a collaboration with Egemin, an industrial manufacturer of logistic systems, we faced the problem with a lack of codified architecture knowledge in the context of the evolution of a software product line (SPL). In particular, maintainers lack the architecture knowledge that is needed to perform the evolution tasks of deployed products correctly and efficiently. Ad-hoc updates increase costs and harm the company's reputation. \nTo address this problem, we developed an automated approach for evolving deployed systems of a SPL. Central in this approach are (1) a meta-model that codifies the architecture knowledge required to support evolution of a SPL, and (2) and algorithm that uses the architecture knowledge harvested from a deployed system based on the meta-model to generate the list of tasks maintainers have to perform to evolve the system. Evaluation of the approach demonstrates a significant improvement of the quality of system updates with respect to the correct execution of updates and the availability of services during the updates."
},
{
  "Title": "Transforming Trace Information in Architectural Documents into Re-usable and Effective Traceability Links",
  "Type": "Paper",
  "Key": "a15e3c4604de39c3fb7aca1070bb26",
  "Authors": ["Mehdi Mirakhorli", "Jane Cleland-Huang"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "Architectural analysis processes, such as the Architecture Trade-o and Analysis Method (ATAM), utilize a scenario based approach to evaluate the extent to which an architectural solution meets a potentially competing set of quality goals. The resulting architectural documents contain a rich set of trace relationships between quality goals, decisions, and architectural elements. Unfortunately this information is not readily accessible for supporting tasks other than initial architectural assessments. In this paper we describe a technique and supporting tools for extracting and generating traceability links from the architectural documents. A specialized Traceability Information Model is used to guide the user through the task of establishing traceability links from design decisions to the architectural elements in which the decision is realized. The retrieved and generated traceability links can then be used to support a far broader set of activities including visualization of design rationale and architectural preservation. We evaluate our approach using a case study of the NASA Crew Exploration Vehicle."
},
{
  "Title": "Software Designers, Are You Biased?",
  "Type": "Paper",
  "Key": "c9ff829909e8e75c3073700cc0b860",
  "Authors": ["Antony Tang"],
  "Affiliations": ["VU University Amsterdam, Netherlands"],
  "Abstract": "Methods of representing and capturing design rationale have been studied in past years. Many meta-models, methods and techniques have been proposed. Are these software engineering methods sufficient to help designers make logical and appropriate design decisions? Studies have shown that people make biased decisions, software designers may also be subjected to such cognitive biases. In this paper, I give an overview of how cognitive biases and reasoning failures may lead to unsound design decisions. I conjecture that in order to improve the overall quality of software design, we as a community need to improve our understanding and teaching of software design reasoning."
},
{
  "Title": "Using Code Analysis Tools for Architectural Conformance Checking",
  "Type": "Paper",
  "Key": "b52485d9fd19818a07a110aed8ec8c",
  "Authors": ["Jo Van Eyck", "Nelis Boucké", "Alexander Helleboogh", "Tom Holvoet"],
  "Affiliations": ["Katholieke Universiteit Leuven, Belgium"],
  "Abstract": "Architectural conformance checking verifies whether a system conforms to its intended architecture, which is essential to safeguard the quality attributes of the system. Due to the size of many systems, performing conformance checking by means of manual code inspections is often practically infeasible. Code analysis tools can be used to automatically check architectural conformance. \nIn this paper, we investigate several code analysis tools that offer support for Java and compare them on their usefulness for architectural conformance checking: Architecture Rules, Macker, Lattix DSM, SonarJ, Structure101 and XDepend."
},
{
  "Title": "Recommending API Methods Based on Identifier Contexts",
  "Type": "Paper",
  "Key": "828c5cb2f666e0b8f54bca7be9e1d3",
  "Authors": ["Lars Heinemann", "Benjamin Hummel"],
  "Affiliations": ["Technische Universität München, Germany"],
  "Abstract": "Reuse recommendation systems suggest functions or code snippets that are useful for the programming task at hand within the IDE. These systems utilize different aspects from the context of the cursor position within the source file being edited for inferring which functionality is needed next. Current approaches are based on structural information like inheritance relations or type/method usages. We propose a novel method that utilizes the knowledge embodied in the identifiers as a basis for the recommendation of API methods. This approach has the advantage that relevant recommendations can also be made in cases where no methods are called in the context or if contexts use distinct but semantically similar types or methods. First experiments show, that the correct method is recommended in about one quarter to one third of the cases."
},
{
  "Title": "Content-based Search of Model Repositories with Graph Matching Techniques",
  "Type": "Paper",
  "Key": "ce55c281f9a6c156f526ef081ce238",
  "Authors": ["Bojana Bislimovska", "Alessandro Bozzon", "Marco Brambilla", "Piero Fraternali"],
  "Affiliations": ["Politecnico di Milano, Italy"],
  "Abstract": "Modern software project repositories provide support for both source code and design models that describe in details the data structure, behavior, and components of an application. We propose a graph matching-based technique between software models to address content-based query (a.k.a., query by example) on project repositories so as to retrieve significant model fragments for reuse. This can be extremely valuable in a scenario where the designer has a rough idea of the model or pattern he needs, he quickly sketches a coarse schema, and wants to retrieve projects that contain matching patterns (with all the details in place). Our approach encompasses the transformation of models into suitable graphs, the definition of a similarity function and an implementation within a search engine platform. In this paper we present the graph matching approach of the query model against the model repository and we evaluate different configurations of the similarity function."
},
{
  "Title": "Finding Web Services via BPEL Fragment Search",
  "Type": "Paper",
  "Key": "9b5bc245d60b74824edb3beafe8801",
  "Authors": ["Shingo Takada"],
  "Affiliations": ["Keio University, Japan"],
  "Abstract": "The development of service-oriented systems (SOS) is based on searching for services that are to be used. Much work has been done on finding individual services, and recently, work has also been done on searching for services by first searching for similar SOS, i.e., those having similar processes. But such work has focused on finding the entire process of an SOS. The developer may only want part of a process, but current work do not explicitly support it. This paper takes an approach of finding services by first finding process fragments. We take BPEL as an example of a behavioral process model that describes an SOS. We describe our approach to searching for BPEL fragments."
},
{
  "Title": "An Algorithm Search Engine for Software Developers",
  "Type": "Paper",
  "Key": "db4ed4ed0c3aa0ff657e5d529e9fb1",
  "Authors": ["Sumit Bhatia", "Suppawong Tuarob", "Prasenjit Mitra", "C. Lee Giles"],
  "Affiliations": ["Pennsylvania State University, USA"],
  "Abstract": "Efficient algorithms are extremely important and can be crucial for certain software projects. Even though many source code search engines have been proposed in the literature to help software developers find source code related to their needs, to our knowledge there has been no effort to develop systems that keep abreast of the latest algorithmic developments. In this paper, we describe our initial effort towards developing such an algorithm search engine. The proposed system extracts and indexes algorithms discussed in academic literature and their associated metadata. Users can search the index through a \\emph{free text} query interface. The source code of proposed system, being developed as a part of a larger open source toolkit, SeerSuite, will be released in due course. We also provide directions for further research and improvements of the current system."
},
{
  "Title": "A Spontaneous Code Recommendation Tool Based on Associative Search",
  "Type": "Paper",
  "Key": "50fb974754f0918b1fe59ce35630b0",
  "Authors": ["Watanabe Takuya", "Hidehiko Masuhara"],
  "Affiliations": ["University of Tokyo, Japan"],
  "Abstract": "We present Selene, a source code recommendation tool based on an associative search engine. It spontaneously searches and displays example programs while the developer is editing a program text. By using an associative search engine, it can search a repository of two million example programs within a few seconds. This paper discusses issues that are revealed by our ongoing implementation of Selene, in particular those of performance, similarity measures and user interface."
},
{
  "Title": "Discrepancy Discovery in Search-Enhanced Testing",
  "Type": "Paper",
  "Key": "e5c9a2efb28c09bf8f4dd11845a06e",
  "Authors": ["Werner Janjic", "Florian Barth", "Oliver Hummel", "Colin Atkinson"],
  "Affiliations": ["University of Mannheim, Germany"],
  "Abstract": "Automating software testing can significantly reduce the time and effort required to assure the quality of software systems, and over recent years significant strides have been made in test automation techniques. However, one aspect of software testing that has always resisted full automation is the determination of the expected results for given system states and input values -- the so called ``oracle problem''. Fortunately, the recent advent of a new generation of software search engines containing millions of reusable software artifacts offers an elegant solution to this dilemma. Once a search engine is able to deliver multiple results that conform to a given specification (by searching for and adapting preexisting components), multi-version testing of software with ``harvested'' oracles becomes a feasible alternative to manual oracle definition. In this paper we present an approach to Search-Enhanced Testing with a focus on the discovery of discrepancies between the results returned by harvested test oracles and a Component Under Test for randomly generated test invocations. Our current research focuses on validating the hypothesis that human test engineers will find more defects when analyzing such automatically discovered discrepancies than when developing test cases using traditional coverage criteria."
},
{
  "Title": "Towards Sharing Source Code Facts Using Linked Data",
  "Type": "Paper",
  "Key": "8de303a8d2e0c51b3043da0e3c9521",
  "Authors": ["Iman Keivanloo", "Christopher Forbes", "Juergen Rilling", "Philippe Charland"],
  "Affiliations": ["Concordia University, Canada", "Defence R&D, Canada"],
  "Abstract": "Linked Data is designed to support interoperability and sharing of open datasets by allowing on the fly inter-linking of data using the basic layers of the Semantic Web and the HTTP protocol. In our research, we focus on providing a Uniform Resource Locator (URL) generation schema and a supporting ontological representation for the inter-linking of data extracted from source code ecosystems. As a result, we created the Source code ECOsystem Linked Data (SECOLD) framework that adheres to the Linked Data publication standard. The framework provides not only source code and facts that are usable by both humans and machines for browsing or querying, but it will also assist the research community at large in sharing and utilizing a standardized source code representation. The dataset has been submitted and registered to ckan.net, under the SECOLD project name, as the first source code Linked Data repository. In order to maintain its relevance to the research community, we plan to update the data set every four months."
},
{
  "Title": "A Prolog-based Framework for Search, Integration and Empirical Analysis on Software Evolution Data",
  "Type": "Paper",
  "Key": "942192c8e2603bb2b4b06fb418f642",
  "Authors": ["Pamela Bhattacharya", "Iulian Neamtiu"],
  "Affiliations": ["UC Riverside, USA"],
  "Abstract": "Software projects use different repositories for storing project and evolution information such as source code, bugs and patches. An integrated system that combines these multiple repositories and can answer a broad range of queries regarding the project’s evolution history would be beneficial to both software developers and researchers. For example, the list of source code changes or the list of developers associated with a bug fix are frequent queries for both developers and researchers. Integrating and gathering this information is a tedious, cumbersome, error-prone process when done manually, especially for large projects. Previous approaches to this problem use frameworks that limit the user to a set of pre-defined query templates, or use query languages with limited power. In this paper, we argue the need for a framework built with recursively enumerable languages, that can answer temporal queries, and sup- ports negation and recursion. As a first step toward such a frame- work, we present a Prolog-based system that we built, along with an evaluation of real-world integrated data from the Firefox project. Our system allows for elegant and concise, yet powerful queries, and can be used by developers and researchers for frequent development and empirical analysis tasks."
},
{
  "Title": "What Do Developers Search for in Source Code and Why",
  "Type": "Paper",
  "Key": "1f23e4502822a905aaafed365d647e",
  "Authors": ["Oleksandr Panchenko", "Hasso Plattner", "Alexander Zeier"],
  "Affiliations": ["Hasso Plattner Institute for Software Systems Engineering, Germany"],
  "Abstract": "Source code search is an important tool used by software engineers. However, until now relatively little is known about what developers search for in source code and why. This paper addresses this knowledge gap. We present the results of a log file analysis of a source code search engine. The data from the log file was analyzed together with the change history of four development and maintenance systems. The results show that most of the search targets were not changed after being downloaded, thus we concluded that the developers conducted searches to find reusable components, to obtain coding examples or to perform impact analysis. In contrast, maintainers often change the code they have downloaded. Moreover, we automatically categorized the search queries. The most popular categories were: method name, structural pattern, and keyword. The major search target was a statement. Although the selected data set was small, the deviations between the systems were negligible, therefore we conclude that our results are valid."
},
{
  "Title": "Investigating How to Effectively Combine Static Concern Location Techniques",
  "Type": "Paper",
  "Key": "5621f8140a80438e73f2fcf06de2b0",
  "Authors": ["Emily Hill", "Lori Pollock", "K. Vijay-Shanker"],
  "Affiliations": ["Montclair State University, USA", "University of Delaware, USA"],
  "Abstract": "As software systems continue to grow and evolve, locating code for maintenance tasks becomes increasingly difficult. Studies have shown that combining static global concern location techniques like search with more structure-based local techniques can improve effectiveness. However, no studies have yet investigated why this occurs. In this paper, we investigate why combining global and local techniques improves effectiveness, and under what conditions. We explore such questions as: “What are the limits of lexical information in locating concerns?”, “How far away does a local technique have to go to locate the remaining relevant elements?”, and “How sensitive are these results to the query or scoring thresholds of the techniques?”. The results of our study can inform design decisions to maximize effective global and local combinations in future concern location techniques."
},
{
  "Title": "What Kinds of Development Problems Can Be Solved by Searching the Web?: A Field Study",
  "Type": "Paper",
  "Key": "623539daea8bba9a23fab57c7d47ba",
  "Authors": ["Rosalva E. Gallardo-Valencia", "Susan Elliott Sim"],
  "Affiliations": ["UC Irvine, USA"],
  "Abstract": "Developers use the Web as a tool to find information to help them solve their software development problems. However, little is known about what kinds of problems motivate developers to do searches on the Web. We asked twenty-five developers to record their Web searches at a medium-sized software company. We also observed twelve developers. In our analysis, we found that there are six main kinds of problems. One, developers want to find procedural information, such as quick references about how to solve some programming problems. Two, developers use the Web as a memory aid to remember details about how to solve a problem they solved before. Three, developers want to solve a knowledge problem, for example, learning some new concepts. Four, developers want to find information to understand and solve errors. Five, developers need information to judge the suitability of software components being evaluated. Finally, in the “Other” category, we included problems such as searching for software to download or translating functions."
},
{
  "Title": "Traceability Research: Taking the Next Steps",
  "Type": "Paper",
  "Key": "9a098ef49040837eeade3d9c4d2fcc",
  "Authors": ["Jane Cleland-Huang"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "This keynote highlights areas of significant accomplishments in traceability research and asks the question of ``where next?'' It describes forward looking projects of the Center of Excellence for Software Traceability (CoEST) and raises some of the difficult questions related to building a shared research infrastructure in the traceability community."
},
{
  "Title": "Source Code Indexing for Automated Tracing",
  "Type": "Paper",
  "Key": "5c60d51d948215b400d3ad90a7bd38",
  "Authors": ["Anas Mahmoud", "Nan Niu"],
  "Affiliations": ["Mississippi State University, USA"],
  "Abstract": "Requirements-to-source-code traceability employs information retrieval (IR) methods to automatically link requirements to the source code that implements them. A crucial step in this process is indexing, where partial and important information from the software artifacts is converted into a representation that is compatible with the underlying IR model. Source code demands special attention in the indexing process. In this paper, we investigate source code indexing for supporting automatic traceability. We introduce a feature diagram that captures the key components and their relationships in the domain of source code indexing. We then present an experiment to examine the features of the diagram and their dependencies. Results show that utilizing comments has a significant effect on traceability link generation, and stemming is required when comments are considered."
},
{
  "Title": "Traceability between Function Point and Source Code",
  "Type": "Paper",
  "Key": "a246a56fd24b431081f0d5bf88b67a",
  "Authors": ["Paulo José Azevedo Vianna Ferreira", "Márcio De Oliveira Barros"],
  "Affiliations": ["UNIRIO, Brazil"],
  "Abstract": "Software development can achieve interesting benefits through the use of requirements traceability, including improved program comprehension, easier maintenance, component reuse, impact analysis, and measure of project progress and completeness. On the other hand, while the cost of a new IS can be estimated by applying Function Point Analysis, this technique has limited application on maintenance. By determining the impact of changing a given set of features, IS development organizations can build a clear understanding of the effort that these changes will require. In this paper, we propose a technique which uses traceability to build a bridge between function points and source code. We believe that this technique can support negotiations between IS development organizations and their clients regarding changes to Information Systems."
},
{
  "Title": "Grand Challenges, Benchmarks, and TraceLab: Developing Infrastructure for the Software Traceability Research Community",
  "Type": "Paper",
  "Key": "6c0f3d9368d18d3a182a4218551585",
  "Authors": ["Jane Cleland-Huang", "Adam Czauderna", "Alex Dekhtyar", "Olly Gotel", "Jane Huffman Hayes", "Ed Keenan", "Greg Leach", "Jonathan Maletic", "Denys Poshyvanyk", "Yonghee Shin", "Andrea Zisman", "Giuliano Antoniol", "Brian Berenbach", "Alexander Egyed", "Patrick Maeder"],
  "Affiliations": ["DePaul University, USA", "Cal Poly, USA", "University of Kentucky, USA", "Kent State University, USA", "College of William and Mary, USA", "City University London, UK", "École Polytechnique Montréal, Canada", "Siemens Corporate Research, USA", "Johannes Kepler University, Austria"],
  "Abstract": "The challenges of implementing successful and cost-effective traceability have created a compelling research agenda that has addressed a broad range of traceability related issues, ranging from qualitative studies of traceability users in industry to very technical and quantitative studies. Unfortunately, advances are hampered by the significant time and effort needed to establish a traceability research environment and to perform comparative evaluations of new results against existing baselines. In this panel we discuss ongoing efforts by members of the Center of Excellence for Software Traceability (CoEST) to define the Grand Challenges of Traceability, develop benchmarks, and to construct TraceLab, an extensible and scalable visual environment for designing and executing a broad range of traceability experiments."
},
{
  "Title": "Traceclipse: An Eclipse Plug-in for Traceability Link Recovery and Management",
  "Type": "Paper",
  "Key": "17084583edbe62efd8ad5637368307",
  "Authors": ["Samuel Klock", "Malcom Gethers", "Bogdan Dit", "Denys Poshyvanyk"],
  "Affiliations": ["College of William and Mary, USA"],
  "Abstract": "Traceability link recovery is an active research area in software engineering with a number of open research questions and challenges, due to the substantial costs and challenges associated with software maintenance. We propose Traceclipse, an Eclipse plug-in that integrates some similar characteristics of traceability link recovery techniques in one easy-to-use suite. The tool enables software developers to specify, view, and manipulate traceability links within Eclipse and it provides an API through which recovery techniques may be added, specified, and run within an integrated development environment. The paper also presents initial case studies aimed at evaluating the proposed plug-in."
},
{
  "Title": "Recovering Traceability Links between Source Code and Fixed Bugs via Patch Analysis",
  "Type": "Paper",
  "Key": "fc1dcffdcc513af8b7c91197b688e8",
  "Authors": ["Christopher S. Corley", "Nicholas A. Kraft", "Letha H. Etzkorn", "Stacy K. Lukins"],
  "Affiliations": ["University of North Alabama, USA", "The University of Alabama, USA", "The University of Alabama at Huntsville, USA"],
  "Abstract": "Traceability links can be recovered using data mined from a revision control system, such as CVS, and an issue tracking system, such as Bugzilla. Existing approaches to recover links between a bug and the methods changed to fix the bug rely on the presence of the bug's identifier in a CVS log message. In this paper we present an approach that relies instead on the presence of a patch in the issue report for the bug. That is, rather than analyzing deltas retrieved from CVS to recover links, our approach analyzes patches retrieved from Bugzilla. We use BugTrace, the tool implementing our approach, to conduct a case study in which we compare the links recovered by our approach to links recovered by manual inspection. The results of the case study support the efficacy of our approach. After describing the limitations of our case study, we conclude by reviewing closely related work and suggesting possible future work."
},
{
  "Title": "Tracing Requirements for Adaptive Systems using Claims",
  "Type": "Paper",
  "Key": "0634917c79c03a437819dfc0dc2157",
  "Authors": ["Kristopher Welsh", "Nelly Bencomo", "Peter Sawyer"],
  "Affiliations": ["Lancaster University, UK", "INRIA Paris, France"],
  "Abstract": "The complexity of environments faced by dynamically adaptive systems means that the RE process will often be iterative with analysts revisiting the system specifications based on new environmental understanding product of experiences with experimental deployments, or even after final deployments. An ability to trace backwards to an identified environmental assumption, and to trace forwards to find the areas of a DAS's specification that are affected by a change in environmental understanding aids in supporting this necessarily iterative RE process. This paper demonstrates how claims, a record in an i* SR model of an assumption made, can be used as markers for areas of uncertainty in a DAS specification. The paper demonstrates backward tracing using claims to identify faulty environmental understanding, and forward tracing to allow generation of new behaviour in the form of policy adaptations and models for transitioning the running system."
},
{
  "Title": "Formalizing Traceability Relations for Product Lines",
  "Type": "Paper",
  "Key": "18cb4a491f58ae514a6e3cd3d8459b",
  "Authors": ["Luis C. Lamb", "Waraporn Jirapanthong", "Andrea Zisman"],
  "Affiliations": ["Federal University of Rio Grande do Sul, Brazil", "Dhurakij Pundit University, Thailand", "City University London, UK"],
  "Abstract": "Traceability is considered an important activity during the development of software systems. Despite the various classifications that have been proposed for different types of traceability relations, there is still a lack of standard semantic definitions for traceability relations. In this paper, we present an ontology-based formalism for semantic representation of various types of traceability relations for product line systems and associations between these various types of traceability relations."
},
{
  "Title": "Improving Traceability Link Recovery Methods through Software Artifact Summarization",
  "Type": "Paper",
  "Key": "92b602b22b1eaec6e1ada77013f1a8",
  "Authors": ["Jairo Aponte", "Andrian Marcus"],
  "Affiliations": ["Universidad Nacional de Colombia, Colombia", "Wayne State University, USA"],
  "Abstract": "Analyzing candidate traceability links is a difficult, time consuming and error prone task, as it usually requires a detailed study of a long list of software artifacts of various kinds. One option to alleviate this problem is to select the most important features of the software artifacts that the developers would investigate. We discuss in this position paper how text summarization techniques could be used to address this problem. The potential gains in using summaries are both in terms of time and correctness of the traceability link recovery process."
},
{
  "Title": "Software Verification and Validation Research Laboratory (SVVRL) of the University of Kentucky: Traceability Challenge 2011: Language Translation",
  "Type": "Paper",
  "Key": "267bcf197f037655e602596746c8d3",
  "Authors": ["Jane Huffman Hayes", "Hakim Sultanov", "Wei-Keat Kong", "Wenbin Li"],
  "Affiliations": ["University of Kentucky, USA"],
  "Abstract": "We present the process and methods applied in undertaking the Traceability Challenge in addressing Grand Challenge C-GC1 – Trace recovery. The Information Retrieval methods implemented in REquirements TRacing On target .NET (RETRO.NET) were applied to the tracing of the eTour and EasyClinic datasets. Our work focused on the nuances of native language (Italian, English). Datasets were augmented with additional terms derived from splitting function and variable names with Camel-Back notation and using the Google Translate API to translate Italian terms into English. Results based on the provided answer set show that the augmented datasets significantly improved recall and precision for one of the datasets."
},
{
  "Title": "Creating Operational Profiles of Software Systems by Transforming their Log Files to Directed Cyclic Graphs",
  "Type": "Paper",
  "Key": "46c9e55bfd0ac6a7ccfcc509ddb48e",
  "Authors": ["Meiyappan Nagappan", "Brian Robinson"],
  "Affiliations": ["North Carolina State University, USA", "ABB Corporate Research, USA"],
  "Abstract": "Most log files are of one format - a flat file with the events of execution recorded one after the other. Each line in the file contains at least a timestamp, a combination of one or more event identifiers, and the actual log message with information of which event was executed and what the values for the dynamic parameters of that event are. Since log files have this trace information, we can use it for many purposes, such as operational profiling and anomalous execution path detection. However the current flat file format of a log file is very unintuitive to detect the existence of a repeating pattern. In this paper we propose a transformation of the current serial order format of a log file to a directed cyclic graph (such as a non-finite state machine) format and how the operational profile of a system can be built from this representation of the log file. We built a tool (in C++), that transforms a log file with a set of log events in a serial order to an adjacency matrix for the resulting graphical representation. We can then easily apply existing graph theory based algorithms on the adjacency matrix to analyze the log file of the system. The directed cyclic graph and the analysis of it can be visualized by rendering the adjacency matrix with graph visualization tools, like Graphviz."
},
{
  "Title": "Towards a Model of Analyst Effort for Traceability Research",
  "Type": "Paper",
  "Key": "a71df86368925972b370640e72483f",
  "Authors": ["Alex Dekhtyar", "Jane Huffman Hayes", "Matt Smith"],
  "Affiliations": ["California Polytechnic State University, USA", "University of Kentucky, USA"],
  "Abstract": "This paper posits that a theoretical model of analyst effort in tracing tasks is necessary to assist with study of the analyst. Specifically, it is clear from prior work by numerous research groups that the important factors in such a model are: the amount of time it takes for an analyst to vet a given candidate link and the amount of time it takes an analyst to find a missing link. This paper introduces a theoretical model of analyst effort as well as a simplified model. A number of simulations were undertaken in order to build effort curves to assist in evaluating numerous tracing scenarios, such as determining at what point in time an analyst should switch from vetting candidate links to manually searching for links not in the candidate list."
},
{
  "Title": "A Rich Traceability Model for Social Interactions",
  "Type": "Paper",
  "Key": "746bf65c2ab3cfa4c4cf09be00fb7d",
  "Authors": ["Maurício Serrano", "Julio Cesar Sampaio do Prado Leite"],
  "Affiliations": ["Pontifícia Universidade Católica do Rio de Janeiro, Brazil"],
  "Abstract": "In 1993, Goguen published a research note addressing the social issues in Requirements Engineering. He identified in the requirements process three major social groups: the client organization; the requirements team; and the development team. However, nowadays there is a lack of technological support that traces requirements to social issues on the requirements team or development team. From early published traceability metamodels to current requirements traceability literature, the client organization and the stakeholders are first-class citizens, but the software engineers and the interactions between these groups are not. In this paper we present a partially formalized RichPicture traceability model to fill this gap. ITrace is a flexible model to weave together the social network graph, the information sources graph, the social interactions graph, and the Requirements Engineering artifacts evolution graph. We empirically developed our traceability model tracking a Transparency catalogue evolution. We also compare our model structure to Contribution Structures."
},
{
  "Title": "On the Use of Eye Tracking in Software Traceability",
  "Type": "Paper",
  "Key": "b5c2406783f6368d66757e4c8ee7c8",
  "Authors": ["Bonita Sharif", "Huzefa Kagdi"],
  "Affiliations": ["Ohio University, USA", "Winston Salem State University, USA"],
  "Abstract": "The paper advocates for the induction of eye tracking technology in software traceability and takes a position that the use of eye tracking metrics can contribute to several software traceability tasks. The authors posit that the role of eye tracking is not simply restricted to an instrument for empirical studies, but also could extend to providing a foundation of a new software traceability methodology. Several scenarios where eye-tracking metrics could be meaningful are presented. The specific research directions include conducting empirical studies with eye-tracking metrics and replicating previously reported empirical studies, eye-tracking enabled traceability link recovery and management methodology, and visualization support."
},
{
  "Title": "Analyzing the Role of Tags as Lightweight Traceability Links",
  "Type": "Paper",
  "Key": "fc1efacd42774c4838aa3039db281a",
  "Authors": ["Matthew L. Hale", "Noah M. Jorgenson", "Rose F. Gamble"],
  "Affiliations": ["University of Tulsa, USA"],
  "Abstract": "Tagging offers a traceability mechanism for software development by connecting artifacts in a meaningful way. Our integrated courseware, SEREBRO, provides a framework of tools that capture conversation and artifact creation and modification throughout the software development lifecycle by student team members developing non-trivial software products in a Software Engineering course. Using a data driven approach, we investigate the use of lightweight tagging mechanisms applied by student software project teams and present some preliminary results of this investigation."
},
{
  "Title": "Traceability Challenge 2011: Using TraceLab to Evaluate the Impact of Local versus Global IDF on Trace Retrieval",
  "Type": "Paper",
  "Key": "cf0dd743b243efa8aebcb3c1b09c03",
  "Authors": ["Adam Czauderna", "Marek Gibiec", "Greg Leach", "Yubin Li", "Yonghee Shin", "Ed Keenan", "Jane Cleland-Huang"],
  "Affiliations": ["DePaul University, USA"],
  "Abstract": "Numerous trace retrieval algorithms incorporate the standard tf-idf (term frequency, inverse document frequency) technique to weight various terms. In this TEFSE challenge report we address Grand Challenge C-GC1 by comparing the effectiveness of computing idf based only on the local terms in the query, versus computing it based on general term usage as documented in the American National Corpus. We also address Grand Challenges L-GC1 and L-GC2 by setting ourselves the additional task of designing and conducting the experiments using the alpha-release of TraceLab. TraceLab is an experimental workbench which allows researchers to graphically model and execute a traceability experiment as a workflow of components. Results of the experiment show that the local idf approach exceeds or matches the global approach in all of the cases studied."
},
{
  "Title": "Supporting Plug-in Mashes to Ease Tool Integration",
  "Type": "Paper",
  "Key": "0ab56946d6190b2b7dd760854bf8cf",
  "Authors": ["Leonardo Mariani", "Fabrizio Pastore"],
  "Affiliations": ["University of Milano Bicocca, Italy"],
  "Abstract": "The majority of IDEs implement a concept of plug-in that nicely supports the integration of tools within the IDEs. Plug-ins dramatically simplify the structural integration of multiple tools, but provide little support to the design of the dynamic of the integration, which must be entirely coded by programmers from plug-ins' API. Manually integrating plug-ins is costly, complex and requires a deep understanding of the underlying environment. The implementation of tools as plug-ins and the integration of the results produced by di erent plug-ins are still difficult, expensive and error-prone activities. This paper presents the concepts of Task Based Plug-in (TB-plug-in) and workflow of TB-plug-ins. In our vision, IDE users must be able to execute plug-ins and integrate their results by designing workflows that can be persisted, executed and re-used in other workflows. We validated our idea by refactoring a set of Eclipse plug-ins for log- file analysis into TB-plug-ins, and designing several workflows that integrate plug-in tasks. We compared the e ffort necessary to implement these analyses from plug-ins with the eff ort necessary to design the workflows from TB-plug-ins. We discovered that workflows can be easily designed with little knowledge about the IDE and the plug-ins' API, save signi cant eff ort otherwise devoted to the implementation of additional plug-ins and glue-code, and produce analyses that can be quickly modified and reused."
},
{
  "Title": "Pest: From the Lab to the Classroom",
  "Type": "Paper",
  "Key": "d5ce22521b5ad7b75ec5b133f8e978",
  "Authors": ["Guido de Caso", "Diego Garbervetsky", "Daniel Gorin"],
  "Affiliations": ["Universidad de Buenos Aires, Argentina"],
  "Abstract": "Automated software verification is an active field of research which has made enormous progress both in theoretical and practical aspects. In recent years, an important effort has been put into applying these techniques on top of mainstream programming languages. These languages typically provide powerful features such as reflection, aliasing and polymorphism which are handy for practitioners but, in contrast, make verification a real challenge. The Pest programming language, on the other and, was conceived with verifiability as one of its main design drivers. Although its main purpose is to serve as a test bed for new language features, its bare-bones syntax and strong support for annotations suggested early on in its development that it could also serve as a teaching tool for first-year undergraduate students. Developing an Eclipse plug-in for Pest proved to be both cost-effective and a key part to its adoption in the classroom. In this paper, we report on this experience."
},
{
  "Title": "Four Generic Issues for Tools-as-Plugins Illustrated by the Distributed Editor Saros",
  "Type": "Paper",
  "Key": "40a593638dc7047f8e7168d6d94e98",
  "Authors": ["Lutz Prechelt", "Karl Beecher"],
  "Affiliations": ["Freie Universität Berlin, Germany"],
  "Abstract": "Saros is an Eclipse plugin for multi-writer, real-time, distributed collaborative text editing that also includes VoIP, chat, whiteboard, and screen sharing functionality. We present four problematic issues we encountered in the development of Saros: Providing portability, choosing a metaphor, handling clashes in display markups, and attributing incompatibilities correctly to their source. These issues will apply to many other plugins similarly. For three of them, no generic solution approach yet exists but should be worked out."
},
{
  "Title": "Ginga-NCL Architecture for Plug-ins",
  "Type": "Paper",
  "Key": "aef68bf29fea4d8f4bddc216b4817e",
  "Authors": ["Marcio Ferreira Moreno", "Rafael Savignon Marinho", "Luiz Fernando Gomes Soares"],
  "Affiliations": ["PUC-Rio, Brazil"],
  "Abstract": "Ginga-NCL is the declarative environment of the Ginga middleware, an ITU-T Recommendation for IPTV services and ITU-R Recommendation for terrestrial digital TV. This paper discusses the two-way solution Ginga proposes for its plug-ins. Ginga defines an API that allows third party tools as NCL (the declarative language of Ginga) player’s plug-ins for specific media-object type exhibition that composes a DTV application presentation in its whole. As NCL allows nested NCL applications, an NCL application itself acts as a plug-in of another parent NCL application, therefore obeying the same plug-in API previously mentioned. In general, the same NCL plug-in API can be used to allow applications specified in other languages to be embedded in NCL applications, as well as to allow NCL applications to be embedded in other presentation environments, in particular the Ginga-NCL environment. This two-way bridge is exemplified in this paper between NCL and HTML applications."
},
{
  "Title": "Platform Support for Developing Testing and Analysis Plug-ins",
  "Type": "Paper",
  "Key": "d581f019d28f702ba66dd9b8a9e074",
  "Authors": ["Shauvik Roy Choudhary", "Jeremy Duvall", "Wei Jin", "Dan Zhao", "Alessandro Orso"],
  "Affiliations": ["Georgia Institute of Technology, USA"],
  "Abstract": "Plug-ins have become an important part of today's integrated development environments (IDEs). They are useful for extending the functionality of these environments and customizing them for different types of projects. In this paper, we discuss some features that should be provided by IDEs to support the development of a specific kind of plug-ins---plug-ins that support program analysis and software testing techniques. To guide the discussion, we leverage our experience in building a plug-in for two different platforms and generalize from that experience."
},
{
  "Title": "Reconciling the 3-layer Architectural Style with the Eclipse Plug-in-based Architecture",
  "Type": "Paper",
  "Key": "f736d9e45495c51a43172227d67c7a",
  "Authors": ["David Ameller", "Oriol Collell", "Xavier Franch"],
  "Affiliations": ["Universistat Politècnica de Catalunya, Spain"],
  "Abstract": "Software architecture construction is the result of a complex decision-making process, in which competing alternatives need to be compared. For example, deciding between a web-based application or a plug-in-based application has a significant impact on the architecture, therefore in order to make the right choice all possible tradeoffs between them must be considered. Decisions need to be made in all architectural views, from the logical view in which architectural styles are chosen, to the development view in which types of modules are decided, to the deployment view where physical allocation is determined. In this paper we analyze the interactions between a 3-layer architecture at the logical view, and a plug-in-based development view implemented in Eclipse, focusing on the difficulties we overcome in a research project in order to make it work."
},
{
  "Title": "AODVis: Leveraging Eclipse Plugins to Reverse Engineer and Visualize AspectJ/Java Source Code",
  "Type": "Paper",
  "Key": "32721ff4880e376268150ecf596591",
  "Authors": ["Jeffrey Koch", "Sunil Bohra", "Rohit Goel", "Sonali Pagade", "Kendra M. L. Cooper"],
  "Affiliations": ["University of Texas at Dallas, USA"],
  "Abstract": "AspectJ reverse engineering and visualization remains a challenge at the architectural and design levels, with fewer tools available for reverse engineers compared to other languages such as Java. As part of our AODVis (Aspect-Oriented Development Visualization) framework, we are developing Eclipse plugins to reverse-engineer and visualize AspectJ projects as 3D UML-based detailed design, architecture, and analysis level models. \nThe AODVis plugins leverage several existing Eclipse plugins to extract program facts, create and manipulate UML, transform program facts into models, and generate visualizations of the models. Although integration can be challenging, the broad range of plugins are invaluable in providing solutions for extending and integrating existing tools such as compilers and IDEs. The plugins and the Eclipse plugin architecture also allowed us to concentrate on our specific research problem instead of developing our own tool support from scratch."
},
{
  "Title": "RDB2RDF Plugin: Relational Databases to RDF Plugin for Eclipse",
  "Type": "Paper",
  "Key": "1decf61e280dca2e78628ed3876b02",
  "Authors": ["Percy E. Salas", "Edgard Marx", "Alexander Mera", "José Viterbo"],
  "Affiliations": ["PUC-Rio, Brazil", "Universidade Federal Fluminense, Brazil"],
  "Abstract": "RDB2RDF is the process by which a relational database schema (RDB) is transformed into a set of RDF triples. A major step in this process is deciding how to represent database schema concepts in terms of RDF classes and properties. This correlation is described in the RDB2RDF mapping file, which is used as the base for the generation of RDF triples. Most RDB2RDF engines today provide support to the mechanical process of transforming RDB to RDF, each with its own mapping language. Due to this fact, the W3C RDB2RDF Working Group has been working to standardize a language to map relational data to RDF called R2RML. Part of their efforts is directed to fostering the development of methods, tools and techniques to support standard RDB2RDF mapping strategies. In this paper, we introduce an Eclipse plug-in that supports the standard RDB to RDF Mapping Language (R2RML) to produce Direct Mappings in RDF."
},
{
  "Title": "Application Management Plug-ins through Dynamically Pluggable Probes",
  "Type": "Paper",
  "Key": "cc6e7eb09ca67d77463951b02370a1",
  "Authors": ["Kiev Gama", "Gabriel Pedraza", "Thomas Lévêque", "Didier Donsez"],
  "Affiliations": ["University of Grenoble, France", "Mälardalen University, Sweden"],
  "Abstract": "It is widely recognized that applications need to be administered remotely. In general, application management and monitoring is supported by textual management consoles while graphical user interfaces specialized for their tasks are preferred by average users. Defining what must be monitored and what are the admin actions one wants to perform on an application cannot be defined during the application development due to the fact that these needs evolve after the application deployment as we cannot completely predict the execution environment such as available devices. This paper presents an architecture and the corresponding infrastructure that allow administrators to define what they want to monitor and manage and automate the discovery and deployment of corresponding probes and related management console graphical plug-ins. This work has been validated on two different application domains."
},
{
  "Title": "SRP-Plugin: A Strategic Release Planning Plug-in for Visual Studio 2010",
  "Type": "Paper",
  "Key": "f3723606f9b92d35e9f5cbd69a220d",
  "Authors": ["Jamshaid G. Mohebzada", "Guenther Ruhe", "Armin Eberlein"],
  "Affiliations": ["University of Calgary, Canada", "American University of Sharjah, United Arab Emirates"],
  "Abstract": "Strategic release planning (SRP) is a critical step in iterative software development. SRP involves the assignment of features or requirements to releases while considering hard and soft constraints, such as time, effort, quality or resources. ReleasePlanner™ is a web-based decision support tool that is based on a sound and rigorous formal strategic release planning model. In this paper we describe the integration of ReleasePlanner™ with Visual Studio in the form of a Visual Studio plug-in called SRP-Plugin. The SRP-Plugin is used to demonstrate our hypothesis that tools implemented as plug-ins for widely used development platform (such as Visual Studio) help to increase efficiency of the development process. The plug-in augments the rich Visual Studio environment with advanced release planning capabilities which result in better release planning quality, increased productivity and enhanced communication among project stakeholders. The added value of providing systematic release planning as part of a development is not limited to release planning alone. We have outlined the future work to extend SRP-Plugin to include proactive decision making features such as utilization of sensitivity analysis and machine learning techniques to provide recommendations for the project manager during the challenging task of deciding which sets of features should be offered to whom, when, and why."
},
{
  "Title": "IDE Support to Facilitate the Transition from Rapid Prototyping to Robust Software Production",
  "Type": "Paper",
  "Key": "d424ec4401b1d20d9762ff21607e15",
  "Authors": ["Francisco Ortin", "Anton Morant"],
  "Affiliations": ["University of Oviedo, Spain", "University of Oxford, UK"],
  "Abstract": "Dynamic languages are becoming increasingly popular for different software development scenarios such as rapid prototyping because of the flexibility and agile interactive development they offer. The benefits of dynamic languages are, however, counteracted by many limitations produced by the lack of static typing. In order to obtain the benefits of both approaches, some programming languages offer a hybrid dynamic and static type system. The existing IDEs for these hybrid typing languages do not provide any type-based feature when dynamic typing is used, lacking important IDE facilities offered for statically typed code. We have implemented a constraint-based type inference system that gathers type information of dynamic references at compile time. Using this type information, we have extended a professional IDE to offer those type-based features missed for dynamically typed code. Following the Separation of Concerns principle, the IDE has also been customized to facilitate the conversion of dy-namically typed code into statically typed one, and vice versa."
},
{
  "Title": "Contractor.NET: Inferring Typestate Properties to Enrich Code Contracts",
  "Type": "Paper",
  "Key": "95839523c7d24b99f9a53b03f0b297",
  "Authors": ["Edgardo Zoppi", "Víctor Braberman", "Guido de Caso", "Diego Garbervetsky", "Sebastián Uchitel"],
  "Affiliations": ["Universidad de Buenos Aires, Argentina"],
  "Abstract": "In this work we present Contractor.NET, a Visual Studio extension that supports the construction of contract specifications with typestate information which can be used for verification of client code. Contractor.NET uses and extends Code Contracts to provide stronger contract specifications. It features a two step process. First, a class source code is analyzed to extract a finite state behavior model (in the form of a typestate) that is amenable to human-in-the-loop validation and refinement. The second step is to augment the original contract specification for the input class with the inferred typestate information, therefore enabling the verification of client code. The inferred typestates are enabledness preserving: a level of abstraction that has been successfully used to validate software artifacts, assisting in the detection of a number of concerns in various case studies including specifications of Microsoft Server protocols."
},
{
  "Title": "Fishtail: From Task Context to Source Code Examples",
  "Type": "Paper",
  "Key": "9cd1b11d509628952de96f26a06b3d",
  "Authors": ["Nicholas Sawadsky", "Gail C. Murphy"],
  "Affiliations": ["University of British Columbia, Canada"],
  "Abstract": "Implementing software development tools as integrated development environment (IDE) plugins gives tools direct access to a range of useful representations of the program being created and can improve programmer efficiency. These benefits must be weighed against the effort to integrate the tool into the IDE, effort which may need to be repeated for each IDE targeted. In this paper, we introduce Fishtail, a prototype plugin for the Eclipse IDE, which assists programmers in discovering code examples and documentation on the web relevant to their current task. Fishtail uses a detailed history of programmer interactions with the source code to automatically determine relevant web resources. We describe the key factors that make it attractive to implement Fishtail as a plugin, and the requirements Fishtail imposes on the plugin/IDE interface. To reach a broader user base and understand how well our tool supports different programming styles and IDE architectures, we have recently begun investigating how to make a version of Fishtail available in the Visual Studio IDE. We outline some of the challenges we face in trying to reuse code from the original Eclipse plugin."
},
{
  "Title": "A Cloud-aware API for Semi-structured BLOB Databases Addressing Data Overflow",
  "Type": "Paper",
  "Key": "3f074fcf75921d6ac27d6fe7ae28d8",
  "Authors": ["Jaumir V. da Silveira", "Jr.", "Karin K. Breitman"],
  "Affiliations": ["PUC-Rio, Brazil"],
  "Abstract": "Cloud computing is rapidly becoming an important platform for research in Software Engineering. Despite the vibe and huge literature on commercial cloud environments, there is, however, very little research on how to capture, model, design and implement new software applications that can make intelligent use of the cloud. In this paper we propose a new abstraction that explores a fundamental aspect of cloud systems – data elasticity. The Container Database (CDB) abstraction provides a cloud-based solution for scenarios where device local storage is not sufficient for manipulating data. To demonstrate the viability of the proposed approach we present an implementation of the CDB abstraction as an API designed to work with a Visual Studio 2010 plug-in, using Windows Azure Cloud services."
},
{
  "Title": "Resource Usage Contracts for .NET",
  "Type": "Paper",
  "Key": "6edfc59ab349d9caa13fbaa7292111",
  "Authors": ["Jonathan Tapicer", "Diego Garbervetsky", "Martin Rouaux"],
  "Affiliations": ["UBA, Argentina"],
  "Abstract": "Code Contracts is a tool that allows the specification and verification of contracts (pre, post-condition, invariants) in all .NET based programming languages. Resource Contracts is an extension of this language to specify resource usage in .NET programs. The new annotations, initially focussed on dynamic memory, enable modular analysis of both memory consumption and lifetime properties. They are checked by relying on the own Code Contracts static verifier and a points-to analysis. This approach is implemented as a Visual Studio extension, providing facilities such us autocompletion and verification at build time."
},
{
  "Title": "Architecting a Plug-in Based Steam Turbine Design Tool",
  "Type": "Paper",
  "Key": "2c29c3592632fe8a06d8fd94b15205",
  "Authors": ["Stefanos Zachariadis", "Tim Cianchi"],
  "Affiliations": ["Zuhlke Engineering, UK"],
  "Abstract": "At a leading manufacturer of equipment for power generation, the engineers currently design a steam turbine, a key component of a power plant, using a large number of disjoint legacy tools written mostly in Fortran; These tools encapsulate significant engineering know how and are vital to the successful operation of the company. Their age and state pose a number of challenges, including difficulty in adapting to new methods, maintenance costs and lack of integration; the cost of replacing them all in one go however, has been deemed to be prohibitively expensive. In this experience report we describe the Turbine Design Tool (TDT), our approach in developing a plug-in based design tool that encapsulates and integrates the legacy tools into a single, component-based, extendable environment that offers the advantages of an integrated solution while minimising the cost and disruption to the business and that allows for the gradual replacement of the tools."
},
{
  "Title": "Towards Subtyped Program Generation in F#",
  "Type": "Paper",
  "Key": "cd9a3a61436178cd4e809c86048b62",
  "Authors": ["Baris Aktemur"],
  "Affiliations": ["Özyeğin University, Turkey"],
  "Abstract": "Program Generation is the technique of combining code fragments to construct a program. In this work we report on our progress to extend F# with program generation constructs. Our prototype implementation uses a translation that allows simulating program generators by regular programs. The translation enables fast implementation and experimentation. We state how a further extension with subtyping can be integrated by benefiting from the translation."
},
{
  "Title": "OthelloPlay– A Plug-in Based Tool for Requirement Formalization and Validation",
  "Type": "Paper",
  "Key": "f0b3e9b2ff2b369184048e6fa322fe",
  "Authors": ["Roberto Cavada", "Alessandro Cimatti", "Andrea Micheli", "Marco Roveri", "Angelo Susi", "Stefano Tonetta"],
  "Affiliations": ["FBK, Italy"],
  "Abstract": "Requirement engineering is one of the most important phases in the development process of software and systems. In safety-critical applications, it is important to support the validation of the requirements with formal techniques to identify and remove flaws. However, requirements are often written in textual documents and their formalization and validation is not trivial for non-experts in formal methods. The goal of the OthelloPlay tool is to support formalization of textual requirements and to simplify the use of formal techniques for requirements validation. The tool combines a formal verification engine and the Microsoft Word(R) editor in a single and consistent environment. A fundamental key in our design approach is a plug-in-based architecture, which uses the Python language in conjunction with a Microsoft Word(R) Add-In. The user can jump between textual requirements in the Microsoft Word(R) editor and the corresponding formal requirements model."
},
{
  "Title": "An OpenGL-based Eclipse Plug-in for Visual Debugging",
  "Type": "Paper",
  "Key": "9c54887964c1470d40aeb7886228d7",
  "Authors": ["André Riboira", "Rui Abreu", "Rui Rodrigues"],
  "Affiliations": ["University of Porto, Portugal"],
  "Abstract": "Locating components which are responsible for observed failures is the most expensive, error-prone phase in the software development life cycle. We present an Eclipse Plug-in that aims to fill some of the automatic debugging tools gaps: the lack of a visualization tool that provides intuitive feedback about the defect distribution over the code base, and easy access to the faulty locations."
},
{
  "Title": "eCLAIM - An Eclipse Plug-in for Mobile MAS Applications",
  "Type": "Paper",
  "Key": "d3dfb238b3f69950daacb07cac7791",
  "Authors": ["Diego Salomone Bruno", "Karin K. Breitman", "Amal El Fallah Seghrouchni"],
  "Affiliations": ["PUC-RIO, Brazil", "LIP6, France"],
  "Abstract": "The eCLAIM plug-in provides a solid environement for design and implementation of Multi-Agent Systems (MAS). Based on the agent oriented language CLAIM \\cite{SF07}, it eases the development of software agents that are at the same time mobile and intelligent."
},
{
  "Title": "ICE: Circumventing Meltdown with an Advanced Binary Analysis Framework",
  "Type": "Paper",
  "Key": "134bcb750d029655f95d2f2a060291",
  "Authors": ["Dean Pucsek", "Jonah Wall", "Celina Gibbs", "Jennifer Baldwin", "Martin Salois", "Yvonne Coady"],
  "Affiliations": ["University of Victoria, Canada", "Defence Research and Development, Canada"],
  "Abstract": "In this paper we propose ICE, an Integrated Comprehension Environment, designed to facilitate advanced binary analysis through an extensible framework. ICE makes extensive use of modules and a flexible intermediate representation to enable seamless integration of instruction set architectures, platforms, and analysis techniques."
},
{
  "Title": "Leveraging Social Media to Gather User Feedback for Software Development",
  "Type": "Paper",
  "Key": "e038297cec28170683ed26e2d82224",
  "Authors": ["Dejana Bajic", "Kelly Lyons"],
  "Affiliations": ["UserVoice, USA", "University of Toronto, Canada"],
  "Abstract": "Social media is impacting the way service offerings are deployed and delivered. Several social media service offerings have emerged in the past few years (e.g., Facebook, Twitter, LinkedIn). At the same time, more traditional kinds of service offerings are finding creative ways of making use of social media techniques (e.g., Dell Community). In this research, we analyze how software companies are finding ways to use social media techniques to gather feedback from users collectively. The following four factors and the way they influence the use of social media are analyzed in depth: company size, transparency, software deployment, and number of social media tools in use. Results of our analysis of software vendors that gather collective user feedback in this way are presented, concluding with a discussion on which attributes influence the way social media is used for collecting user feedback."
},
{
  "Title": "Wikigramming: A Wiki-based Training Environment for Programming",
  "Type": "Paper",
  "Key": "9760073eeb538fb21e5df2e1e16b8b",
  "Authors": ["Takashi Hattori"],
  "Affiliations": ["Keio University, Japan"],
  "Abstract": "Wiki is one of the most successful technologies in Web 2.0 because it is so simple that anyone can start using it instantly. The main aim of this research is to realize a collaborative programming environment that is as simple as Wiki. Each Wiki page contains source code of a Scheme function which is executed on the server. Users can edit any function at any time without complicated procedure, and see the results of their changes instantly. In order to avoid intentional or unintentional destruction of working programs, when users attempt to modify existing functions, the modified version must pass unit tests written by other users. Though changes are made anonymously, we can have some confidence if test cases are written by many users."
},
{
  "Title": "Supporting the Cooperation of End-User Programmers through Social Development Environments",
  "Type": "Paper",
  "Key": "70cac7deff1dc7dda11a6c09cc1416",
  "Authors": ["Leif Singer", "Kurt Schneider"],
  "Affiliations": ["Leibniz Universität Hannover, Germany"],
  "Abstract": "Many programs are being created by end-users without formal training in programming. Spreadsheets are the most popular environment for this, but mashups which combine public services into new, albeit small applications are also becoming more and more popular. Research shows that end-user programmers make potentially costly mistakes. Yet initiatives that aim at bringing software engineering principles to end-users are still rudimentary. In particular, we see much unused potential in approaches that foster and support the cooperation among end-user programmers. Whereas the application of mechanisms from social software to software engineering problems is gaining traction, this has not yet been investigated sufficiently for end-user software engineering. This paper discusses how insights from Communities of Practice research may be implemented using mechanisms from recent developments in social software. From the implementation of the presented social mechanisms, we expect an improvement in cooperation and mutual help in communities of end-user programmers. We plan to combine this approach with lightweight variations of software engineering methods targeted at end-user programmers. This should lead to higher quality in the programs developed by these end-users, as good practices are more likely to spread."
},
{
  "Title": "Automatic Status Updates in Distributed Software Development",
  "Type": "Paper",
  "Key": "c8444885d736d11d783ad475188fb8",
  "Authors": ["Abayomi King", "Kelly Lyons"],
  "Affiliations": ["University of Toronto, Canada"],
  "Abstract": "This study investigates how automatic, real-time, user-centered awareness information can help distributed software development teams. We created an Eclipse plugin that automatically determines a user’s activity in their Eclipse IDE and publishes the activity information as the status of their instant messenger client. The status is updated in real-time every time the user changes his or her activities in their IDE. We evaluated this tool by demonstrating it to eighty-one academics and industry workers in the field of computer science and interviewing them about the perceived benefits and usefulness of the tool. The results reveal various factors that can impact a participant’s desire for increased awareness information. Despite these factors there was a general desire to improve awareness of users’ activities via the tool. There was also some indication that the tool might help with interruption management."
},
{
  "Title": "Measuring API Documentation on the Web",
  "Type": "Paper",
  "Key": "aaa8bd5709201e4aea516d4e0b34ff",
  "Authors": ["Chris Parnin", "Christoph Treude"],
  "Affiliations": ["Georgia Institute of Technology, USA", "University of Victoria, Canada"],
  "Abstract": "Software development blogs, developer forums and Q&A websites are changing the way software is documented. With these tools, developers can create and communicate knowledge and experiences without relying on a central authority to provide official documentation. Instead, any content created by a developer is just a web search away. To understand whether documentation via social media can replace or augment more traditional forms of documentation, we study the extent to which the methods of one particular API -- jQuery -- are documented on the Web. We analyze 1,730 search results and show that software development blogs in particular cover 87.9% of the API methods, mainly featuring tutorials and personal experiences about using the methods. Further, this effort is shared by a large group of developers contributing just a few blog posts. Our findings indicate that social media is more than a niche in software documentation, that it can provide high levels of coverage and that it gives readers a chance to engage with authors."
},
{
  "Title": "Towards Understanding Twitter Use in Software Engineering: Preliminary Findings, Ongoing Challenges and Future Questions",
  "Type": "Paper",
  "Key": "b3c3a7fe04b63d093b1d80078a10a5",
  "Authors": ["Gargi Bougie", "Jamie Starke", "Margaret-Anne Storey", "Daniel M. German"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "There has been some research conducted around the motivation for the use of Twitter and the value brought by micro-blogging tools to individuals and business environments. This paper builds on our understanding of how the phenomenon affects the population which birthed the technology: Software Engineers. We find that the Software Engineering community extensively leverages Twitter's capabilities for conversation and information sharing and that use of the tool is notably different between distinct Software Engineering groups. Our work exposes topics for future research and outlines some of the challenges in exploring this type of data."
},
{
  "Title": "Data Quality: Cinderella at the Software Metrics Ball?",
  "Type": "Paper",
  "Key": "bd29abb14db917d99d7c4d745a5f0e",
  "Authors": ["Martin Shepperd"],
  "Affiliations": ["Brunel University, UK"],
  "Abstract": "In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since — whether we like it or not — our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers."
},
{
  "Title": "Integrating Quality Models and Static Analysis for Comprehensive Quality Assessment",
  "Type": "Paper",
  "Key": "042f7b532240444dffc309ddfdc629",
  "Authors": ["Klaus Lochmann", "Lars Heinemann"],
  "Affiliations": ["Technische Universität München, Germany"],
  "Abstract": "To assess the quality of software, two ingredients are available today: (1) quality models defining abstract quality characteristics and (2) code analysis tools providing a large variety of metrics. However, there exists a gap between these two worlds. The quality attributes defined in quality models are too abstract to be operationalized. On the other side, the aggregation of the results of static code analysis tools remains a challenge. We address these problems by defining a quality model based on an explicit meta-model. It allows to operationalize quality models by defining how metrics calculated by tools are aggregated. Furthermore, we propose a new approach for normalizing the results of rule-based code analysis tools, which uses the information on the structure of the source code in the quality model. We evaluate the quality model by providing tool support for both developing quality models and conducting automatic quality assessments. Our results indicate that large quality models can be built based on our meta-model. The automatic assessment shows a high correlation between the automatic assessment and an expert-based ranking."
},
{
  "Title": "Is My Project's Truck Factor Low? Theoretical and Empirical Considerations About the Truck Factor Threshold",
  "Type": "Paper",
  "Key": "8f5ce25445ec0b8fb6d7ffa5d8f3f9",
  "Authors": ["Marco Torchiano", "Filippo Ricca", "Alessandro Marchetto"],
  "Affiliations": ["Politecnico di Torino, Italy", "Università di Genova, Italy", "Fondazione Bruno Kessler, Italy"],
  "Abstract": "The Truck Factor is a simple way, proposed by the agile community, to measure the system’s knowledge distribution in a team of developers. It can be used to highlight potential project problems due to the inadequate distribution of the system knowledge. Notwithstanding its relevance, only few studies investigated the Truck Factor and proposed ways to efficiently measure, evaluate and use it. In particular, the effective use of the Truck Factor is limited by the lack of reliable thresholds. In this preliminary paper, we present a theoretical model concerning the Truck Factor and, in particular, we investigate its use to define the maximum achievable Truck Factor value in a project. The relevance of such a value concerns the definition of a reliable threshold for the Truck Factor. Furthermore in the paper, we document an experiment in which we apply the proposed model to real software projects with the aim of comparing the maximum achievable value of the Truck Factor with the unique threshold proposed in literature. The preliminary outcome we achieved shows that the existing threshold has some limitations and problems."
},
{
  "Title": "Analyzing Tool Usage to Understand to What Extent Experts Change their Activities when Mentoring",
  "Type": "Paper",
  "Key": "51df3dee541b83001e0393fde8730f",
  "Authors": ["Pekka Abrahamsson", "Ilenia Fronza", "Jelena Vlasenko"],
  "Affiliations": ["Free University of Bozen, Italy"],
  "Abstract": "Automated In-Process Software Engineering Measurement and Analysis (AISEMA) systems represent a major advancement in tracking non-invasively the activities of developers. We have built on the top of an AISEMA system a model that enables to understand better how the tools are used in practical real-life development settings. In this work we evaluate to what extent experienced developers change their activities during mentoring activities in Pair Programming (PP) and, in this case, how long this effect can be observed. We compare how the experienced developers use the tools when working with other experts and when with new developers. The results indicate that there is a notable difference in the way the tools are used between the experts working together and the experts mentoring the new developers that have just joined the team. Moreover, over time the difference between pairs of experts and mixed pairs (experts and novices) working together becomes almost unnoticeable."
},
{
  "Title": "By No Means: A Study on Aggregating Software Metrics",
  "Type": "Paper",
  "Key": "82b15988d17ffe72fe103d804b2818",
  "Authors": ["Bogdan Vasilescu", "Alexander Serebrenik", "Mark van den Brand"],
  "Affiliations": ["Technische Universiteit Eindhoven, Netherlands"],
  "Abstract": "Fault prediction models usually employ software metrics which were previously shown to be a strong predictor for defects, e.g., SLOC. However, metrics are usually defined on a micro-level (method, class, package), and should therefore be aggregated in order to provide insights in the evolution at the macro-level (system). In addition to traditional aggregation techniques such as the mean, median, or sum, recently econometric aggregation techniques, such as the Gini, Theil, and Hoover indices have been proposed. \nIn this paper we wish to understand whether the aggregation technique influences the presence and strength of the relation between SLOC and defects. Our results indicate that correlation is not strong, and is influenced by the aggregation technique."
},
{
  "Title": "Insights into Component Testing Process",
  "Type": "Paper",
  "Key": "46263bf7316e8b5790a9617e98014a",
  "Authors": ["Vikrant Kaulgud", "Vibhu Saujanya Sharma"],
  "Affiliations": ["Accenture Technology Labs, India"],
  "Abstract": "Effective component testing (or commonly termed as Unit Testing) is important to control defect slippage into the testing stage. Often testing teams lack in-process visibility into the effectiveness of ongoing component testing. Using project data such as code coverage and schedule and effort estimates, we generate temporal and rate-based insights into component testing effectiveness. A simple composite metric is used for measuring and forecasting the health of the component testing process. The early warning signals, based on the forecast and associated insights, lead teams to take proactive actions for improving component testing. In our ongoing experimental studies, we have observed that use of these insights cause a substantial reduction in defect slippage."
},
{
  "Title": "Linking Software Design Metrics to Component Change-Proneness",
  "Type": "Paper",
  "Key": "f4c3c31c8524cdb5f9b71d93d2065d",
  "Authors": ["Claire Ingram", "Steve Riddle"],
  "Affiliations": ["Newcastle University, UK"],
  "Abstract": "One technique from value-based software engineering involves prioritising the system and selectively applying time-consuming techniques (such as traceability) in order to maximise return on investment. This prioritisation could be based on predicted change-proneness of code modules, if a sufficiently accurate prediction can be achieved. Several previous studies have examined links between software change-proneness and software metrics such as size and complexity. However, conclusions differ as to the strength of the relationships. We present here a new case study project, extracting a range of complexity values from the code modules and testing for the existence of a significant link between change-proneness and complexity. We find only limited evidence of a linear relationship, but analysis using other statistical techniques does reveal some other significant links."
},
{
  "Title": "Stability of Java Interfaces: A Preliminary Investigation",
  "Type": "Paper",
  "Key": "3d99ece0a4db5c2ec73d6303ccad4f",
  "Authors": ["Jonathan Chow", "Ewan Tempero"],
  "Affiliations": ["The University of Auckland, New Zealand"],
  "Abstract": "The attribute of stability is regarded by some as an important attribute of software. Some claims regarding software design quality imply that what are called interfaces in Java are stable. This paper introduces some new metrics for investigating such claims, and presents some preliminary measurements from these metrics, which indicate that developers do not consistently develop stable interfaces."
},
{
  "Title": "Different Strokes for Different Folks: A Case Study on Software Metrics for Different Defect Categories",
  "Type": "Paper",
  "Key": "52b3b635ffc3b789e066a46a55655e",
  "Authors": ["Ayse Tosun Mısırlı", "Bora Çağlayan", "Andriy V. Miranskyy", "Ayşe Başar Bener", "Nuzio Ruffolo"],
  "Affiliations": ["Bogazici University, Turkey", "IBM Canada Ltd., Canada", "Ryerson University, Canada"],
  "Abstract": "Defect prediction has been evolved with variety of metric sets, and defect types. Researchers found code, churn, and network metrics as significant indicators of defects. However, all metric sets may not be informative for all defect categories such that only one metric type may represent majority of a defect category. Our previous study showed that defect category sensitive prediction models are more successful than general models, since each category has different characteristics in terms of metrics. We extend our previous work, and propose specialized prediction models using churn, code, and network metrics with respect to three defect categories. Results show that churn metrics are the best for predicting all defects. The strength of correlation for code and network metrics varies with defect category: Network metrics have higher correlations than code metrics for defects reported during functional testing and in the field, and vice versa for defects reported during system testing."
},
{
  "Title": "Concern-Based Cohesion as Change Proneness Indicator: An Initial Empirical Study",
  "Type": "Paper",
  "Key": "31d5bcf61758556c9f4124969d5eb0",
  "Authors": ["Bruno C. da Silva", "Cláudio Sant'Anna", "Christina Chavez"],
  "Affiliations": ["Federal University of Bahia, Brazil"],
  "Abstract": "Structure-based cohesion metrics, such as the well-known Chidamber and Kemerer’s Lack of Cohesion in Methods (LCOM), fail to capture the semantic notion of a software component’s cohesion. Some researchers claim that it is one of the reasons they are not good indicators of change proneness. The Lack of Concern-based Cohesion metric (LCC) is an alternative cohesion metric which is centered on counting the number of concerns a component implements. A concern is any important concept, feature, property or area of interest of a system that we want to treat in a modular way. In this way, LCC focus on what really matters for assessing a component’s cohesion - the amount of responsibilities placed on them. Our aim in this paper is to present an initial investigation about the applicability of this concern-based cohesion metric as a change proneness indicator. We also checked if this metric has a correlation with efferent coupling. An initial empirical assessment work was done with two small to medium-sized systems. Our results indicated a moderate to storng correlation between LCC and change proneness, and also a strong correlation between LCC and efferent coupling."
},
{
  "Title": "A Revised Web Objects Method to Estimate Web Application Development Effort",
  "Type": "Paper",
  "Key": "a73b48c8a4360a3a3ea0b80022e2bc",
  "Authors": ["Raffaella Folgieri", "Giulio Barabino", "Giulio Concas", "Erika Corona", "Roberto De Lorenzi", "Michele L. Marchesi", "Andrea Segni"],
  "Affiliations": ["University of Genova, Italy", "University of Milan, Italy", "University of Cagliari, Italy", "Datasiel spa, Italy"],
  "Abstract": "We present a study of the effectiveness of estimating web application development effort using Function Points and Web Objects methods, and a method we propose – the Revised Web Objects (RWO). RWO is an upgrading of WO method, aimed to account for new web development styles and technologies. It also introduces an up-front classification of web applications according to their size, scope and technology, to further refine their effort estimation. These methods were applied to a data-set of 24 projects obtained by Datasiel spa, a mid-sized Italian company, focused on web application projects, showing that RWO performs statistically better than WO, and roughly in the same way as FP."
},
{
  "Title": "Which Code Construct Metrics are Symptoms of Post Release Failures?",
  "Type": "Paper",
  "Key": "e05899fb27fd7a3d94aaa1014c5585",
  "Authors": ["Meiyappan Nagappan", "Brendan Murphy", "Mladen Vouk"],
  "Affiliations": ["North Carolina State University, USA", "Microsoft Research, USA"],
  "Abstract": "Software metrics, such as code complexity metrics and code churn metrics, are used to predict failures. In this paper we study a specific set of metrics called code construct metrics and relate them to post release failures. We use the values of the code construct metrics for each file to characterize that file. We analyze the code construct metrics along with the post release failure data on the files (that splits the files into two classes: files with post release failures and files without post release failures). In our analysis we compare a file with post release failure to a set of files without post release failures, that have similar characteristics. In our comparison we identify which code construct metric, more often than the others, differs the most between these two classes of files. The goal of our research is to find out which code construct metrics can perhaps be used as symptoms of post release failures. In this paper we analyzed the code construct metrics of Eclipse 2.0, 2.1, and 3.0. Our results indicate that MethodInvocation, QualifiedName, and SimpleName, are the code constructs that differentiates the two classes of files the most and hence are the key symptoms/indicators of a file with post release failures in these versions of Eclipse."
},
{
  "Title": "The Fractal Dimension Metric and Its Use to Assess Object-Oriented Software Quality",
  "Type": "Paper",
  "Key": "6029e48dda9524e5fa79c7c2b4bce2",
  "Authors": ["Ivana Turnu", "Giulio Concas", "Michele L. Marchesi", "Roberto Tonelli"],
  "Affiliations": ["University of Cagliari, Italy"],
  "Abstract": "We present a study were software systems are considered as complex networks which have a self-similar structure under a length-scale transformation. On such complex software networks we computed a self-similar coefficient, also known as fractal dimension, using “the box counting method ”. \nWe analyzed various releases of the publically available Eclipse software systems, calculating the fractal dimension for twenty sub-projects, randomly chosen, for every release, as well as for each release as a whole. Our results display an over- all consistency among the sub-projects and among all the analyzed releases. We found a very good correlation between the fractal di- mension and the number of bugs for Eclipse and for twenty sub-projects. Since the fractal dimension is just a scalar number that characterizes a whole system, while complex- ity and quality metrics are in general computed on every system module, this result suggests that the fractal dimen- sion could be considered as a global quality metric for large software systems. Our results need however to be confirmed for other large software systems."
},
{
  "Title": "Program Slicing-Based Cohesion Measurement: The Challenges of Replicating Studies Using Metrics",
  "Type": "Paper",
  "Key": "f48649d2fed8a3256b2ae810721561",
  "Authors": ["David Bowes", "Tracy Hall", "Andrew Kerr"],
  "Affiliations": ["University of Hertfordshire, UK", "Brunel University, UK"],
  "Abstract": "Background: It is important to develop corpuses of data to test out the efficacy of using metrics. Replicated studies are an important contribution to corpuses of metrics data. There are few replicated studies using metrics reported in software engineering. Aim: To contribute more data to the body of evidence on the use of novel program slicing-based cohesion metrics. Method: We replicate a very well regarded study by Meyers and Binkley [15, 16] which analyses the cohesion of open source projects using program slicing-based metrics. Results: Our results are very different from Meyers and Binkley’s original results. This suggests that there are a variety of opportunities for inconsistently to creep into the collection and analysis of metrics data during replicated studies. Conclusion: We conclude that researchers using metrics data must present their work with sufficient detail for replication to be possible. Without this detail it is difficult for subsequent researchers to accurately replicate a study such that consistent and reliable data can be added to a body of evidence."
},
{
  "Title": "Human Judgement and Software Metrics: Vision for the Future",
  "Type": "Paper",
  "Key": "07528ff991d55beccc1cea97215a46",
  "Authors": ["Carolyn Mair", "Martin Shepperd"],
  "Affiliations": ["Southampton Solent University, UK", "Brunel University, UK"],
  "Abstract": "Background: There has been much research into building formal (metrics-based) prediction systems with the aim of improving resource estimation and planning of software projects. However the ‘objectivity’ of such systems is illusory in the sense that many inputs need themselves to be estimated by the software engineer. Method: We review the uptake of past software project prediction research and identify relevant cognitive psychology research on expert behaviour. In particular we explore potential applications of recent metacognition research. Results: We find the human aspect is largely ignored, despite the availability of many important results from cognitive psychology. Conclusions: In order to increase the actual use of our metrics research e.g. effort prediction systems we need to have a more integrated view of how such research might be used and who might be using it. This leads to our belief that future research must be more holistic and inter-disciplinary."
},
{
  "Title": "Code-Motion for API Migration: Fixing SQL Injection Vulnerabilities in Java",
  "Type": "Paper",
  "Key": "d7abb9a050a53c85883d2c6c582f5f",
  "Authors": ["Aharon Abadi", "Yishai A. Feldman", "Mati Shomrat"],
  "Affiliations": ["IBM Research Haifa, Israel", "Tel Aviv University, Israel"],
  "Abstract": "Refactoring often requires the reordering of code fragments; such is the case when migrating from one API to another. Performing such reordering manually is complex and error-prone. A specific example in the security domain involves database query execution, in which some of the parameters come from untrusted sources. In Java, the Statement API provides opportunities for SQL injection attacks. The recommended remedy is to replace it with the secure PreparedStatement API; however, that sometimes requires changing the order in which the query is built. We present an algorithm that performs this migration, moving code as necessary to preserve functionality while changing the structure of the original code as little as possible."
},
{
  "Title": "A Visualization Method of Program Dependency Graph for Identifying Extract Method Opportunity",
  "Type": "Paper",
  "Key": "c55d38ca060d16949dcb69774aa6fc",
  "Authors": ["Tomoko Kanemitsu", "Yoshiki Higo", "Shinji Kusumoto"],
  "Affiliations": ["Osaka University, Japan"],
  "Abstract": "Refactoring is important for efficient software maintenance. However, tools supports are highly required for refactoring because manual operations of refactoring are troublesome and error prone. This paper proposes a technique that suggests Extract Method candidates automatically. Extract Method refactoring is to create a new method from a code fragment in an existing method. Previous research efforts showed that the Extract Method refactoring is often performed prior to other refactorings, so that it is important to support Extract Method refactoring. Previous studies have proposed methods that suggest Extract Method candidates based on linage or complexity. However it is originally desirable to divide methods based on their functionalities. This paper uses the strength of data connection between sentences in the source code. We deem that strongly-connected data expresses a single function. This paper proposes a technique that suggests Extract Method candidates based on strongly-connected data."
},
{
  "Title": "Automated Acceptance Test Refactoring",
  "Type": "Paper",
  "Key": "e51227a53d6fcd42a1f49036000d5e",
  "Authors": ["Rodrick Borg", "Martin Kropp"],
  "Affiliations": ["UAS Northwestern Switzerland"],
  "Abstract": "With the increasing popularity of agile software development and Test-Driven-Development, also maintenance of acceptance test has become an important issue. In this paper, we describe a concept and a tool for automated acceptance test maintenance using a refactoring approach. Acceptance tests are user tests which are used to determine if a system satisfies acceptance criteria and to enable a customer to determine whether or not to accept the system. In agile development acceptance test are also used as a mean for specification, i.e. acceptance tests are written in advance to the production code (called Behavior-Driven-Development – BDD). In an agile project this poses three major challenges with respect to maintenance of acceptance tests: new requirements may cause changes in the acceptance criteria, which require the system under test to be adapted; when the system under test undergoes a major restructuring, even the acceptance test might have to be adapted; with the increasing acceptance test suite in an agile project the tests themselves may undergo a major reorganization. Having a large acceptance test base, doing these refactorings manually is error prone and causes a lot of effort. In this paper we present a concept and tool for executing automated refactoring for Fit acceptance tests, which significantly reduces the effort for test maintenance and makes them much less error prone"
},
{
  "Title": "A Security-Aware Refactoring Tool for Java Programs",
  "Type": "Paper",
  "Key": "9dd1098af26a64c6c688cae76663ba",
  "Authors": ["Katsuhisa Maruyama", "Takayuki Omori"],
  "Affiliations": ["Ritsumeikan University, Japan"],
  "Abstract": "Refactoring is a useful practice in developing and maintaining software since it improves the design of existing code without changing its external behavior. Therefore, contemporary integrated development environments tend to include refactoring tools that support automatic transformations of source code. Unfortunately, some of the popular refactoring transformations make existing code vulnerable although they improve its maintainability. The existence of vulnerable code is still a serious issue for many software systems. This paper describes a tool with support for a new class of refactoring concerning software security, which is built as an Eclipse plug-in. It helps programmers to easily know the adverse impact of code changes on security vulnerabilities in the application of refactoring, and provides them with a chance to determine if they could accept or should cancel the applied refactoring. Consequently, they feel safe to improve the maintainability of existing code without missing security vulnerabilities newly inserted into the code. To evaluate the capability of this tool, we made an experiment with it. The experimental results show the usefulness of the tool and also reveal several remaining issues to be tackled."
},
{
  "Title": "A Refactoring Tool to Extract GPU Kernels",
  "Type": "Paper",
  "Key": "65abc1f139ca0d363ab6cad2248f58",
  "Authors": ["Kostadin Damevski", "Madhan Muralimanohar"],
  "Affiliations": ["Virginia State University, USA"],
  "Abstract": "Significant performance gains can be achieved by using hardware architectures that integrate GPUs with conventional CPUs to form a hybrid and highly parallel computational engine. However, programming these novel architectures is tedious and error prone, reducing their ease of acceptance in an even wider range of computationally intensive applications. In this paper we discuss a refactoring technique, called {\\em Extract Kernel} that transforms a loop written in C into a parallel function that uses NVIDIA's CUDA framework to execute on a GPU. The selected approach and the challenges encountered are described, as well as some early results that demonstrate the potential of this refactoring."
},
{
  "Title": "Understanding the Longevity of Code Smells: Preliminary Results of an Explanatory Survey",
  "Type": "Paper",
  "Key": "cc9370e610270b0e7b31e249f7addb",
  "Authors": ["Roberta Arcoverde", "Alessandro Garcia", "Eduardo Figueiredo"],
  "Affiliations": ["PUC-Rio, Brazil", "UFMG, Brazil"],
  "Abstract": "There is growing empirical evidence that some (patterns of) code smells seem to be, either deliberately or not, ignored. More importantly, there is little knowledge about the factors that are likely to influence the longevity of smell occurrences in software projects. Some of them might be related to limitations of tool support, while others do not. This paper presents the preliminary results of an explanatory survey aimed at understanding better the longevity of code smells in software projects. A questionnaire was elaborated and distributed to developers, and 33 answers were collected up to now. Our initial observations reveal, for instance, that smell removal with refactoring tools is often avoided when maintaining frameworks or product lines."
},
{
  "Title": "Impact of Refactoring on Quality Code Evaluation",
  "Type": "Paper",
  "Key": "207b0d981debf9c709894ecd609e50",
  "Authors": ["Francesca Arcelli Fontana", "Stefano Spinelli"],
  "Affiliations": ["University of Milano Bicocca, Italy"],
  "Abstract": "Code smells are characteristics of the software that may indicate a code or design problem that can make software hard to understand, to evolve and maintain. Detecting code smells in the code and consequently applying the right refactoring steps, when necessary, is very important for improving the quality of the code. In this paper, according to well known metrics proposed to evaluate the code and design quality of a system, we analyze the impact of refactoring, applied to remove code smells, on the quality evaluation of the system."
},
{
  "Title": "Code-Imp: A Tool for Automated Search-Based Refactoring",
  "Type": "Paper",
  "Key": "aa7882b41075e36ef3a68b92e622d6",
  "Authors": ["Iman Hemati Moghadam", "Mel Ó Cinnéide"],
  "Affiliations": ["University College Dublin, Ireland"],
  "Abstract": "Manual refactoring is tedious and error-prone, so it is natural to try to automate this process as much as possible. Fully automated refactoring usually involves using metaheuristic search to determine which refactorings should be applied to improve the program according to some fitness function, expressed in terms of standard software quality metrics. Code-Imp (Combinatorial Optimisation for Design Improvement) is such an automated refactoring platform for the Java language. It can apply a range of refactorings, supports several search types, and implements over 25 software quality metrics which can be combined in various ways to form a fitness function. The original goal of the Code-Imp project was to investigate the use of automated refactoring to improve software quality as expressed by a contemporary metrics suite. In this paper we present a technical overview of the Code-Imp implementation, and summarise three active research strands involving Code-Imp: refactoring for testability, metrics exploration, and multi-level design improvement."
},
{
  "Title": "Fantasy, Farms, and Freemium: What Game Data Mining Teaches Us About Retention, Conversion, and Virality",
  "Type": "Paper",
  "Key": "ea9b26e54a111369371853ee44e797",
  "Authors": ["Jim Whitehead"],
  "Affiliations": ["UC Santa Cruz, USA"],
  "Abstract": "In December of 2010, the new game CityVille achieved 6 million daily active users in just 8 days. Clearly the success of CityVille owes something to the fun gameplay experience it provides. That said, it was far from the best game released in 2010. Why did it grow so fast? In this talk the key factors behind the dramatic success of social network games are explained. Social network games build word-of-mouth player acquisition directly into the gameplay experience via friend invitations and game mechanics that require contributions by friends to succeed. Software analytics (mined data about player sessions) yield detailed models of factors that affect player retention and engagement. Player engagement is directly related to conversion, shifting a free player into a paying player, the critical move in a freemium business model. Analytics also permit tracking of player virality, the degree to which one player invites other players into the game. \nSocial network games offer multiple lessons for software engineers in general, and software mining researchers in particular. Since software is in competition for people’s attention along with a wide range of other media and software, it is important to design software for high engagement and retention. Retention engineering requires constant attention to mined user experience data, and this data is easiest to acquire with web-based software. Building user acquisition directly into software provides powerful benefits, especially when it is integrated deeply into the experience delivered by the software. Since retention engineering and viral user acquisition are much easier with web-based software, the trend of software applications migrating to the web will accelerate."
},
{
  "Title": "Connecting Technology with Real-world Problems - From Copy-paste Detection to Detecting Known Bugs",
  "Type": "Paper",
  "Key": "f5181393804e53d4e86a95bea4c0bc",
  "Authors": ["Yuanyuan Zhou"],
  "Affiliations": ["UC San Diego, USA"],
  "Abstract": "In my talk, I will share with you our experience in applying and deploying our source code mining technology in industry. In particular, the most valuable lesson we have learned is that sometimes there is a bigger problem in the real world that can really benefit from our technology but unfortunately we do not know about it until we closely work with industry. In 2004, motivated from some previous research work that pointed out copy-pasting as a major reason for majority of the bugs in device driver code, my students and I applied data mining technology (specifically frequent subsequence mining algorithms) in identifying copy-pasted code and also detecting forget-to-change bugs introduced during copy-pasting. The benefit of using data mining is that it is highly scalable (20 minutes for 4-5 millions lines of code), and can tolerate changes such as statement insertion/deletion/modification as well as variable name changes. When we released our tool called CP-Miner to the open source community, it attracted some inquiries from industry. These inquiries have motivated us to start a company to commercialize our tools."
},
{
  "Title": "Java Generics Adoption: How New Features are Introduced, Championed, or Ignored",
  "Type": "Paper",
  "Key": "f6d09aab78aedbfa7f19a562b7c302",
  "Authors": ["Chris Parnin", "Christian Bird", "Emerson Murphy-Hill"],
  "Affiliations": ["Georgia Institute of Technology, USA", "Microsoft Research, USA", "North Carolina State University, USA"],
  "Abstract": "Support for generic programming was added to the Java language in 2004, representing perhaps the most significant change to one of the most widely used programming languages today. Researchers and language designers anticipated this addition would relieve many long-standing problems plaguing developers, but surprisingly, no one has yet measured whether generics actually provide such relief. In this paper, we report on the first empirical investigation into how Java generics have been integrated into open source software by automatically mining the history of 20 popular open source Java programs, traversing more than 500 million lines of code in the process. We evaluate five hypotheses, each based on assertions made by prior researchers, about how Java developers use generics. For example, our results suggest that generics do not significantly reduce the number of type casts and that generics are usually adopted by a single champion in a project, rather than all committers."
},
{
  "Title": "A Study of Language Usage Evolution in Open Source Software",
  "Type": "Paper",
  "Key": "abadf565c709a63bc4eb5726b7f8da",
  "Authors": ["Siim Karus", "Harald Gall"],
  "Affiliations": ["University of Tartu, Estonia", "University of Zurich, Switzerland"],
  "Abstract": "The use of programming languages such as Java and C in Open Source Software (OSS) has been well studied. However, many other popular languages such as XSL or XML have received minor attention. In this paper, we discuss some trends in OSS development that we observed when considering multiple programming language evolution of OSS. Based on the revision data of 22 OSS projects, we tracked the evolution of language usage and other artefacts such as documentation files, binaries and graphics files. In these systems several different languages and artefact types including C/C++, Java, XML, XSL, Makefile, Groovy, HTML, Shell scripts, CSS, Graphics files, JavaScript, JSP, Ruby, Phyton, XQuery, OpenDocument files, PHP, etc. have been used. We found that the amount of code written in different languages differs substantially. Some of our findings can be summarized as follows: (1) JavaScript and CSS files most often co-evolve with XSL; (2) Most Java developers but only every second C/C++ developer work with XML; (3) and more generally, we observed a significant increase of usage of XML and XSL during recent years and found that Java or C are hardly ever the only language used by a developer. In fact, a developer works with more than 5 different artefact types (or 4 different languages) in a project on average."
},
{
  "Title": "How Developers Use the Dynamic Features of Programming Languages: The Case of Smalltalk",
  "Type": "Paper",
  "Key": "4451316fee89ab420446e477b981b8",
  "Authors": ["Oscar Callaú", "Romain Robbes", "Éric Tanter", "David Röthlisberger"],
  "Affiliations": ["University of Chile, Chile", "University of Bern, Switzerland"],
  "Abstract": "The dynamic and reflective features of programming languages are powerful constructs that programmers often mention as extremely useful. However, the ability to modify a program at runtime can be both a boon---in terms of flexibility---, and a curse---in terms of tool support. For instance, usage of these features hampers the design of type systems, the accuracy of static analysis techniques, or the introduction of optimizations by compilers. In this paper, we perform an empirical study of a large Smalltalk codebase---often regarded as the poster-child in terms of availability of these features---, in order to assess how much these features are actually used in practice, whether some are used more than others, and in which kinds of projects. These results are useful to make informed decisions about which features to consider when designing language extensions or tool support."
},
{
  "Title": "An Exploratory Study of Identifier Renamings",
  "Type": "Paper",
  "Key": "a0b6faccacc255ba1c12eb8676747e",
  "Authors": ["Laleh M. Eshkevari", "Venera Arnaoudova", "Massimiliano Di Penta", "Rocco Oliveto", "Yann-Gaël Guéhéneuc", "Giuliano Antoniol"],
  "Affiliations": ["École Polytechnique de Montréal, Canada", "University of Sannio, Italy", "University of Molise, Italy"],
  "Abstract": "Identifiers play an important role in source code understandability, maintainability, and fault-proneness. This paper reports a study of identifier renamings in software systems, studying how terms (identifier atomic components) change in source code identifiers. Specifically, the paper (i) proposes a term renaming taxonomy, (ii) presents an approximate lightweight code analysis approach to detect and classify term renamings automatically into the taxonomy dimensions, and (iii) reports an exploratory study of term renamings in two open-source systems, Eclipse-JDT and Tomcat. We thus report evidence that not only synonyms are involved in renamings but also (in a small fraction) more unexpected changes occur: surprisingly, we detected hypernym (a more abstract term, e.g., size vs. length) and hyponym (a more concrete term, e.g., restriction vs. rule) renamings, and antonym renamings (a term replaced with one having the opposite meaning, e.g., closing vs. opening). Despite being only a fraction of all renamings, synonym, hyponym, hypernym, and antonym renamings may hint at some program understanding issues and, thus, could be used in a renaming-recommendation system to improve code quality."
},
{
  "Title": "Retrieval from Software Libraries for Bug Localization: A Comparative Study of Generic and Composite Text Models",
  "Type": "Paper",
  "Key": "10790789b2ba4ae11d0c6a2df2fb79",
  "Authors": ["Shivani Rao", "Avinash Kak"],
  "Affiliations": ["Purdue University, USA"],
  "Abstract": "From the standpoint of retrieval from large software libraries for the purpose of bug localization, we compare five generic text models and certain composite variations thereof. The generic models are: the Unigram Model (UM), the Vector Space Model (VSM), the Latent Semantic Analysis Model (LSA), the Latent Dirichlet Allocation Model (LDA), and the Cluster Based Document Model (CBDM). The task is to locate the files that are relevant to a bug reported in the form of a textual description by a software user/developer. We use for our study iBUGS, a benchmarked bug localization dataset with 75 KLOC and a large number of bugs (291). A major conclusion of our comparative study is that simple text models such as UM and VSM are more effective at correctly retrieving the relevant files from a library as compared to the more sophisticated models such as LDA. The retrieval effectiveness for the various models was measured using the following two metrics: (1) Mean Average Precision; and (2) Rank-based metrics. Using the SCORE metric, we also compare the retrieval effectiveness of the models in our study with some other bug localization tools."
},
{
  "Title": "Comparison of Similarity Metrics for Refactoring Detection",
  "Type": "Paper",
  "Key": "d2c86ee9c342c091604067a0fe96a3",
  "Authors": ["Benjamin Biegel", "Quinten David Soetens", "Willi Hornig", "Stephan Diehl", "Serge Demeyer"],
  "Affiliations": ["University of Trier, Germany", "University of Antwerp, Belgium"],
  "Abstract": "Identifying refactorings in software archives has been an active research topic in the last decade, mainly because it is a prerequisite for various software evolution analyses (e.g., error detection, capturing intent of change, capturing and replaying changes, and relating refactorings and software metrics). Many of these techniques rely on similarity measures to identify structurally equivalent code, however, up until now the effect of this similarity measure on the performance of the refactoring identification algorithm is largely unexplored. In this paper we replicate a well-known experiment from Weissgerber and Diehl, plugging in three different similarity measures (text-based, AST-based, token-based). We look at the overlap of the results obtained by the different metrics, and we compare the results using recall and the computation time. We conclude that the different result sets have a large overlap and that the three metrics perform with a comparable quality."
},
{
  "Title": "Finding Software License Violations Through Binary Code Clone Detection",
  "Type": "Paper",
  "Key": "cb44a5df494900e50e37847e7340b7",
  "Authors": ["Armijn Hemel", "Karl Trygve Kalleberg", "Rob Vermaas", "Eelco Dolstra"],
  "Affiliations": ["gpl-violations.org, Netherlands", "KolibriFX, Norway", "Delft University of Technology, Netherlands"],
  "Abstract": "Software released in binary form frequently uses third-party packages without respecting their licensing terms. For instance, many consumer devices have firmware containing the Linux kernel, without the suppliers following the requirements of the GNU General Public License. Such license violations are often accidental, e.g., when vendors receive binary code from their suppliers with no indication of its provenance. To help find such violations, we have developed the Binary Analysis Tool (BAT), a system for code clone detection in binaries. Given a binary, such as a firmware image, it attempts to detect cloning of code from repositories of packages in source and binary form. We evaluate and compare the effectiveness of three of BAT's clone detection techniques: scanning for string literals, detecting similarity through data compression, and detecting similarity by computing binary deltas."
},
{
  "Title": "A Simpler Model of Software Readability",
  "Type": "Paper",
  "Key": "aad811fb5ef508a86a27d77693c545",
  "Authors": ["Daryl Posnett", "Abram Hindle", "Premkumar Devanbu"],
  "Affiliations": ["UC Davis, USA"],
  "Abstract": "Software readability is a property that influences how easily a given piece of code can be read and understood. Since readability can affect maintainability, quality, etc., programmers are very concerned about the readability of code. If automatic readability checkers could be built, they could be integrated into development tool-chains, and thus continually inform developers about the readability level of the code. Unfortunately, readability is a subjective code property, and not amenable to direct automated measurement. In a recently published study, Buse et al. asked 100 participants to rate code snippets by readability, yielding arguably reliable mean readability scores of each snippet; they then built a fairly complex predictive model for these mean scores using a large, diverse set of directly measurable source code properties. We build on this work: we present a simple, intuitive theory of readability, based on size and code entropy, and show how this theory leads to a much sparser, yet statistically significant, model of the mean readability scores produced in Buse's studies. Our model uses well-known size metrics and Halstead metrics, which are easily extracted using a variety of tools. We argue that this approach provides a more theoretically well-founded, practically usable, approach to readability measurement."
},
{
  "Title": "Comparing Fine-Grained Source Code Changes And Code Churn For Bug Prediction",
  "Type": "Paper",
  "Key": "ded086338a3a0e20604d669b25467d",
  "Authors": ["Emanuel Giger", "Martin Pinzger", "Harald Gall"],
  "Affiliations": ["University of Zurich, Switzerland", "Delft University of Technology, Netherlands"],
  "Abstract": "A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models."
},
{
  "Title": "Security Versus Performance Bugs: A Case Study on Firefox",
  "Type": "Paper",
  "Key": "a8a3d367d3d362055493f4254ca7a8",
  "Authors": ["Shahed Zaman", "Bram Adams", "Ahmed E. Hassan"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "A good understanding of the impact of different types of bugs on various project aspects is essential to improve software quality research and practice. For instance, we would expect that security bugs are fixed faster than other types of bugs due to their critical nature. However, prior research has often treated all bugs as similar when studying various aspects of software quality (e.g., predicting the time to fix a bug), or has focused on one particular type of bug (e.g., security bugs) with little comparison to other types. In this paper, we study how different types of bugs (performance and security bugs) differ from each other and from the rest of the bugs in a software project. Through a case study on the Firefox project, we find that security bugs are fixed and triaged much faster, but are reopened and tossed more frequently. Furthermore, we also find that security bugs involve more developers and impact more files in a project. Our work is the first work to ever empirically study performance bugs and compare it to frequently-studied security bugs. Our findings highlight the importance of considering the different types of bugs in software quality research and practice."
},
{
  "Title": "Empirical Evaluation of Reliability Improvement in an Evolving Software Product Line",
  "Type": "Paper",
  "Key": "37cf47ae19d2b85537af8ce0e249ab",
  "Authors": ["Sandeep Krishnan", "Robyn R. Lutz", "Katerina Goševa-Popstojanova"],
  "Affiliations": ["Iowa State University, USA", "West Virginia University, USA"],
  "Abstract": "Reliability is important to software product-line developers since many product lines require reliable operation. It is typically assumed that as a software product line matures, its reliability improves. Since post-deployment failures impact reliability, we study this claim on an open-source software product line, Eclipse. We investigate the failure trend of common components (reused across all products), high-reuse variation components (reused in five or six products) and low-reuse variation components (reused in one or two products) as Eclipse evolves. We also study how much the common and variation components change over time both in terms of addition of new files and modification of existing files. Quantitative results from mining and analysis of the Eclipse bug and release repositories show that as the product line evolves, fewer serious failures occur in components implementing commonality, and that these components also exhibit less change over time. These results were roughly as expected. However, contrary to expectation, components implementing variations, even when reused in five or more products, continue to evolve fairly rapidly. Perhaps as a result, the number of severe failures in variation components shows no uniform pattern of decrease over time. The paper describes and discusses this and related results."
},
{
  "Title": "Implementing Quality Metrics and Goals at the Corporate Level",
  "Type": "Paper",
  "Key": "3880d0db5bbe27535e9a8e9ef2b674",
  "Authors": ["Pete Rotella", "Sunita Chulani"],
  "Affiliations": ["Cisco Systems Inc., USA"],
  "Abstract": "Over the past eight years, Cisco Systems, Inc., has implemented software quality goals for most groups engaged in software development, including development for both customer and internal use. This corporate implementation has proven to be a long and difficult process for many reasons, including opposition from many groups, uncertainties as to how to proceed with key aspects of the goaling, and the many unanticipated modifications needed to adapt the program to a large and diverse development and test environment. This paper describes what has worked, what has not work so well, and what levels of improvement the Engineering organization has experienced in part as a result of these efforts. Key customer experience metrics have improved 30% to 70% over the past six years, partly as a result of metrics and process standardization, dashboarding, and goaling. As one would expect with such a large endeavor, some of the results shown are not statistically provable, but are nevertheless generally accepted within the corporation as valid. Other important results do have strong statistical substantiation, and we will also describe these. But whether or not the results are statistically provable, Cisco has in fact improved its software quality substantially over the past eight years, and the corporate goaling mechanism is generally recognized as having been a necessary (but of course not sufficient) part of this improvement effort."
},
{
  "Title": "How Do Developers Blog? An Exploratory Study",
  "Type": "Paper",
  "Key": "54413c4325db3bbff985025f083e0d",
  "Authors": ["Dennis Pagano", "Walid Maalej"],
  "Affiliations": ["Technische Universität München, Germany"],
  "Abstract": "We report on an exploratory study, which aims at understanding how software developers use social media compared to conventional development infrastructures. We analyzed the blogging and the committing behavior of 1,100 developers in four large open source communities. We observed that these communities intensively use blogs with one new entry about every 8 hours. A blog entry includes 14 times more words than a commit message. When analyzing the content of the blogs, we found that most popular topics represent high-level concepts such as functional requirements and domain concepts. Source code related topics are covered in less than 15% of the posts. Our results also show that developers are more likely to blog after corrective engineering and management activities than after forward engineering and re-engineering activities. Our findings call for a hypothesis-driven research to further understand the role of social media in software engineering and integrate it into development processes and tools."
},
{
  "Title": "Entering the Circle of Trust: Developer Initiation as Committers in Open-Source Projects",
  "Type": "Paper",
  "Key": "c735abee252b205ec18d86c53d36ef",
  "Authors": ["Vibha Singhal Sinha", "Senthil Mani", "Saurabh Sinha"],
  "Affiliations": ["IBM Research, India"],
  "Abstract": "The success of an open-source project depends to a large degree on the proactive and constructive participation by the developer community. An important role that developers play in a project is that of a code committer. However, code-commit privilege is typically restricted to the core group of a project. In this paper, we study the phenomenon of the induction of external developers as code committers. The trustworthiness of an external developer is one of the key factors that determines the granting of commit privileges. Therefore, we formulate different hypotheses to explain how the trust is established in practice. To investigate our hypotheses, we developed an automated approach based on mining code repositories and bug-tracking systems. We implemented the approach and performed an empirical study, using the Eclipse projects, to test the hypotheses. Our results indicate that, most frequently, developers establish trust and credibility in a project by contributing to the project in a non-committer role. Moreover, the employing organization of a developer is another factor—although a less significant one—that influences trust."
},
{
  "Title": "Social Interactions around Cross-System Bug Fixings: The Case of FreeBSD and OpenBSD",
  "Type": "Paper",
  "Key": "85e0b3681470e06c9c27a6adac9210",
  "Authors": ["Gerardo Canfora", "Luigi Cerulo", "Marta Cimitile", "Massimiliano Di Penta"],
  "Affiliations": ["University of Sannio, Italy", "Unitelma Sapienza, Italy"],
  "Abstract": "Cross-system bug fixing propagation is frequent among systems having similar characteristics, using a common framework, or, in general, systems with cloned source code fragments. While previous studies showed that clones tend to be properly maintained within a single system, very little is known about cross-system bug management. \nThis paper describes an approach to mine explicitly documented cross-system bug fixings, and to relate their occurrences to social characteristics of contributors discussing through the project mailing lists-e.g., degree, betweenness, and brokerage-as well as to the contributors' activity on source code. \nThe paper reports results of an empirical study carried out on FreeBSD and OpenBSD kernels. The study shows that the phenomenon of cross-system bug fixing between these two projects occurs often, despite the limited overlap of contributors. The study also shows that cross-system bug fixings mainly involve contributors with the highest degree, betweenness and brokerage level, as well as contributors that change the source code more than others."
},
{
  "Title": "Do Time of Day and Developer Experience Affect Commit Bugginess?",
  "Type": "Paper",
  "Key": "915cf7a613a6ce74c9b6fc8919aa7f",
  "Authors": ["Jon Eyolfson", "Lin Tan", "Patrick Lam"],
  "Affiliations": ["University of Waterloo, Canada"],
  "Abstract": "Modern software is often developed over many years with hundreds of thousands of commits. Commit metadata is a rich source of social characteristics, including the commit's time of day and the experience and commit frequency of its author. The \"bugginess\" of a commit is also a critical property of that commit. In this paper, we investigate the correlation between a commit's social characteristics and its \"bugginess\"; such results can be very useful for software developers and software engineering researchers. For instance, developers or code reviewers might be well-advised to thoroughly verify commits that are more likely to be buggy. \nIn this paper, we study the correlation between a commit's bugginess and the time of day of the commit, the day of week of the commit, and the experience and commit frequency of the commit authors. We survey two widely-used open source projects: the Linux kernel and PostgreSQL. \nOur main findings include: (1) commits submitted between midnight and 4 AM (referred to as late-night commits) are significantly buggier and commits between 7 AM and noon are less buggy, implying that developers may want to double-check their own late-night commits; (2) daily-committing developers produce less-buggy commits, indicating that we may want to promote the practice of daily-committing developers reviewing other developers' commits; and (3) the bugginess of commits versus day-of-week varies for different software projects."
},
{
  "Title": "Automated Topic Naming to Support Cross-project Analysis of Software Maintenance Activities",
  "Type": "Paper",
  "Key": "02acda2f9c794fef376c3d9aca4176",
  "Authors": ["Abram Hindle", "Neil A. Ernst", "Michael W. Godfrey", "John Mylopoulos"],
  "Affiliations": ["UC Davis, USA", "University of Toronto, Canada", "University of Waterloo, Canada", "University of Trento, Italy"],
  "Abstract": "Researchers have employed a variety of techniques to extract underlying topics that relate to software development artifacts. Typically, these techniques use semi-unsupervised machine-learning algorithms to suggest candidate word-lists. However, word-lists are difficult to interpret in the absence of meaningful summary labels. Current topic modeling techniques assume manual labelling and do not use domain-specific knowledge to improve, contextualize, or describe results for the developers. We propose a solution: automated labelled topic extraction. Topics are extracted using Latent Dirichlet Allocation (LDA) from commit-log comments recovered from source control systems such as CVS and BitKeeper. These topics are given labels from a generalizable cross-project taxonomy, consisting of non-functional requirements. Our approach was evaluated with experiments and case studies on two large-scale RDBMS projects: MySQL and MaxDB. The case studies show that labelled topic extraction can produce appropriate, context-sensitive labels relevant to these projects, which provides fresh insight into their evolving software development activities."
},
{
  "Title": "Modeling the Evolution of Topics in Source Code Histories",
  "Type": "Paper",
  "Key": "f47b2ea7cea93b3d17e13b7ed39dfc",
  "Authors": ["Stephen W. Thomas", "Bram Adams", "Ahmed E. Hassan", "Dorothea Blostein"],
  "Affiliations": ["Queen's University, Canada"],
  "Abstract": "Studying the evolution of topics (collections of co-occurring words) in a software project is an emerging technique to automatically shed light on how the project is changing over time: which topics are becoming more actively developed, which ones are dying down, or which topics are lately more error-prone and hence require more testing. Existing techniques for modeling the evolution of topics in software projects suffer from issues of data duplication, i.e., when the repository contains multiple copies of the same document, as is the case in source code histories. To address this issue, we propose the Diff model, which applies a topic model only to the changes of the documents in each version instead of to the whole document at each version. A comparative study with a state-of-the-art topic evolution model shows that the Diff model can detect more distinct topics as well as more sensitive and accurate topic evolutions, which are both useful for analyzing source code histories."
},
{
  "Title": "Software Bertillonage: Finding the Provenance of an Entity",
  "Type": "Paper",
  "Key": "2beaf37e02a37242e52f852f825dc2",
  "Authors": ["Julius Davies", "Daniel M. German", "Michael W. Godfrey", "Abram Hindle"],
  "Affiliations": ["University of Victoria, Canada", "University of Waterloo, Canada", "UC Davis, USA"],
  "Abstract": "Deployed software systems are typically composed of many pieces, not all of which may have been created by the main development team. Often, the provenance of included components --- such as external libraries or cloned source code --- is not clearly stated, and this uncertainty can introduce technical and ethical concerns that make it difficult for system owners and other stakeholders to manage their software assets. In this work, we motivate the need for the recovery of the provenance of software entities by a broad set of techniques that could include signature matching, source code fact extraction, software clone detection, call flow graph matching, string matching, historical analyses, and other techniques. We liken our provenance goals to that of Bertillonage, a simple and approximate forensic analysis technique based on bio-metrics that was developed in 19th century France before the advent of fingerprints. As an example, we have developed a fast, simple, and approximate technique called anchored signature matching for identifying library version information within a given Java application. This technique involves a type of structured signature matching performed against a database of candidates drawn from the Maven2 repository, a 150GB collection of open source Java libraries. An exploratory case study using a proprietary e-commerce Java application illustrates that the approach is both feasible and effective."
},
{
  "Title": "Supporting Software History Exploration",
  "Type": "Paper",
  "Key": "2834c29fdb43d0ac2a18425b989df1",
  "Authors": ["Alexander W. J. Bradley", "Gail C. Murphy"],
  "Affiliations": ["University of British Columbia, Canada"],
  "Abstract": "Software developers often confront questions such as \"Why was the code implemented this way\"? To answer such questions, developers make use of information in a software system's bug and source repositories. In this paper, we consider two user interfaces for helping a developer explore information from such repositories. One user interface, from Holmes and Begel's Deep Intellisense tool, exposes historical information across several integrated views, favouring exploration from a single code element to all of that element's historical information. The second user interface, in a tool called Rationalizer that we introduce in this paper, integrates historical information into the source code editor, favouring exploration from a particular code line to its immediate history. We introduce a model to express how software repository information is connected and use this model to compare the two interfaces. Through a lab experiment, we found that our model can help predict which interface is helpful for a particular kind of historical question. We also found deficiencies in the interfaces that hindered users in the exploration of historical information. These results can help inform tool developers who are presenting historical information either directly from or mined from software repositories."
},
{
  "Title": "Improving Identifier Informativeness Using Part of Speech Information",
  "Type": "Paper",
  "Key": "65027048902f4bc025b5373bd34613",
  "Authors": ["David Binkley", "Matthew Hearn", "Dawn Lawrie"],
  "Affiliations": ["Loyola University Maryland, USA"],
  "Abstract": "Recent software development tools have exploited the mining of natural language information found within software and its supporting documentation. To make the most of this information, researchers have drawn upon the work of the natural language processing community for tools and techniques. One such tool provides part-of-speech information, which finds application in improving the searching of software repositories and extracting domain information found in identifiers. \nUnfortunately, the natural language found is software differs from that found in standard prose. This difference potentially limits the effectiveness of off-the-shelf tools. An empirical investigation finds that with minimal guidance an existing tagger was correct 88% of the time when tagging the words found in source code identifiers. The investigation then uses the improved part-of-speech information to tag a large corpus of over 145,000 structure-field names. From patterns in the tags several rules emerge that seek to understand past usage and to improve future naming."
},
{
  "Title": "Bug-fix Time Prediction Models: Can We Do Better?",
  "Type": "Paper",
  "Key": "caed91ab726d05539b6e83722e257c",
  "Authors": ["Pamela Bhattacharya", "Iulian Neamtiu"],
  "Affiliations": ["UC Riverside, USA"],
  "Abstract": "Predicting bug-fix time is useful in several areas of software evolution, such as predicting software quality or coordinating development effort during bug triaging. Prior work has proposed bug-fix time prediction models that use various bug report attributes (e.g., number of developers who participated in fixing the bug, bug severity, number of patches, bug-opener's reputation) for estimating the time it will take to fix a newly-reported bug. In this paper we take a step towards constructing more accurate and more general bug-fix time prediction models by showing how existing models fail to validate on large projects widely-used in bug studies. In particular, we used multivariate and univariate regression testing to test the prediction significance of existing models on 512,474 bug reports from five open source projects: Eclipse, Chrome and three products from the Mozilla project (Firefox, Seamonkey and Thunderbird). The results of our regression testing indicate that the predictive power of existing models is between 30% and 49% and that there is a need for more independent variables (attributes) when constructing a prediction model. Additionally, we found that, unlike in prior recent studies on commercial software, in the projects we examined there is no correlation between bug-fix likelihood, bug-opener's reputation and the time it takes to fix a bug. These findings indicate three open research problems: (1) assessing whether prioritizing bugs using bug-opener's reputation is beneficial, (2) identifying attributes which are effective in predicting bug-fix time, and (3) constructing bug-fix time prediction models which can be validated on multiple projects."
},
{
  "Title": "Integrating Software Engineering Data Using Semantic Web Technologies",
  "Type": "Paper",
  "Key": "51c032bc0fc1ce223cea653ebe8256",
  "Authors": ["Yuan-Fang Li", "Hongyu Zhang"],
  "Affiliations": ["Monash University, Australia", "Tsinghua University, China"],
  "Abstract": "A plethora of software engineering data have been produced by different organizations and tools over time. These data may come from different sources, and are often disparate and distributed. The integration of these data may open up the possibility of conducting systemic, holistic study of software projects in ways previously unexplored. Semantic Web technologies have been used successfully in a wide array of domains such as health care and life sciences as a platform for information integration and knowledge management. The success is largely due to the open and extensible nature of ontology languages as well as growing tool support. We believe that Semantic Web technologies represent an ideal platform for the integration of software engineering data in a semantic repository. By querying and analyzing such a repository, researchers and practitioners can better understand and control software engineering activities and processes. In this paper, we describe how we apply Semantic Web techniques to integrate object-oriented software engineering data from different sources. We also show how the integrated data can help us answer complex queries about large-scale software projects through a case study on the Eclipse system."
},
{
  "Title": "Improving Efficiency in Software Maintenance",
  "Type": "Paper",
  "Key": "31bd543e45b70020309102f1b556ba",
  "Authors": ["Sergey Zeltyn", "Peri Tarr", "Murray Cantor", "Robert Delmonico", "Sateesh Kannegala", "Mila Keren", "Ashok Pon Kumar", "Segev Wasserkrug"],
  "Affiliations": ["IBM Haifa Research Laboratory, Israel", "IBM Watson Research, USA", "IBM Rational Software, USA", "IBM India Software Laboratory, India"],
  "Abstract": "Efficiency is critical to the profitability of software maintenance and support organizations. Managing such organizations effectively requires suitable measures of efficiency that are sensitive enough to detect significant changes, and accurate and timely in detecting them. Mean time to close problem reports is the most commonly used efficiency measure, but its suitability has not been evaluated carefully. We performed such an evaluation by mining and analyzing many years of support data on multiple IBM products. Our preliminary results suggest that the mean is less sensitive and accurate than another measure, percentiles, in cases that are particularly important in the maintenance and support domain. Using percentiles, we also identified statistical techniques to detect efficiency trends and evaluated their accuracy. Although preliminary, these results may have significant ramifications for effectively measuring and improving software maintenance and support processes."
},
{
  "Title": "An Empirical Analysis of the FixCache Algorithm",
  "Type": "Paper",
  "Key": "e3a7aa57ace54a0badbcad812ff524",
  "Authors": ["Caitlin Sadowski", "Chris Lewis", "Zhongpeng Lin", "Xiaoyan Zhu", "Jim Whitehead"],
  "Affiliations": ["UC Santa Cruz, USA", "Xi’an Jiaotong University, China"],
  "Abstract": "The FixCache algorithm, introduced in 2007, effectively identifies files or methods which are likely to contain bugs by analyzing source control repository history. However, many open questions remain about the behaviour of this algorithm. What is the variation in the hit rate over time? How long do files stay in the cache? Do buggy files tend to stay buggy, or can they be redeemed? This paper analyzes the behaviour of the FixCache algorithm on four open source projects. FixCache hit rate is found to generally increase over time for three of the four projects; file duration in cache follows a Zipf distribution; and topmost bug-fixed files go through periods of greater and lesser stability over a project’s history."
},
{
  "Title": "Visualizing Collaboration and Influence in the Open-Source Software Community",
  "Type": "Paper",
  "Key": "d4225a710b8e40a05829db62db94c2",
  "Authors": ["Brandon Heller", "Eli Marschner", "Evan Rosenfeld", "Jeffrey Heer"],
  "Affiliations": ["Stanford University, USA"],
  "Abstract": "We apply visualization techniques to user profiles and repository metadata from the GitHub source code hosting service. Our motivation is to identify patterns within this development community that might otherwise remain obscured. Such patterns include the effect of geographic distance on developer relationships, social connectivity and influence among cities, and variation in project-specific contribution styles (e.g., centralized vs. distributed). Our analysis examines directed graphs in which nodes represent users' geographic locations and edges represent (a) follower relationships, (b) successive commits, or (c) contributions to the same project. We inspect this data using a set of visualization techniques: geo-scatter maps, small multiple displays, and matrix diagrams. Using these representations, and tools based on them, we develop hypotheses about the larger GitHub community that would be difficult to discern using traditional lists, tables, or descriptive statistics. These methods are not intended to provide conclusive answers; instead, they provide a way for researchers to explore the question space and communicate initial insights."
},
{
  "Title": "MSR Challenge 2011: Eclipse, Netbeans, Firefox, and Chrome",
  "Type": "Paper",
  "Key": "d2349f6eb56c8c70ee784ca38d292f",
  "Authors": ["Adrian Schröter"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "The MSR Challenge aims at offering researchers and practitioners in the area of Mining Software Repositories a shared set of software repositories, enabling them to compare their tools and approaches. This year, the main theme of the challenge was on the comparison of projects. We selected four open source projects, and challenged participants to use their brains, tools, computational power, and magic to compare them and uncover interesting similarities and differences. The projects were Eclipse and Netbeans, two popular IDEs written in Java (Group 1) and Firefox and Chrome, two web browsers written in C/C++ (Group 2). We encouraged participants to analyze more than one project, ideally in the same group but allowed them to analyze a single project."
},
{
  "Title": "Operating System Compatibility Analysis of Eclipse and Netbeans Based on Bug Data",
  "Type": "Paper",
  "Key": "0edb49f8a9e130d9d19fb38fb6873f",
  "Authors": ["Xinlei (Oscar) Wang", "Eilwoo Baik", "Premkumar Devanbu"],
  "Affiliations": ["UC Davis, USA"],
  "Abstract": "Eclipse and Netbeans are two top of the line Integrated Development Environments (IDEs) for Java development. Both of them provide support for a wide variety of development tasks and have a large user base. This paper provides an analysis and comparison for the compatibility and stability of Eclipse and Netbeans on the three most commonly used operating systems, Windows, Linux and Mac OS. Both IDEs are programmed in Java and use a Bugzilla issue tracker to track reported bugs and feature requests. We looked into the Bugzilla repository databases of these two IDEs, which contains the bug records and histories of these two IDEs. We used some basic data mining techniques to analyze some historical statistics of the bug data. Based on the analysis, we try to answer certain stability-comparison oriented questions in the paper, so that users can have a better idea which of these two IDEs is designed better to work on different platforms."
},
{
  "Title": "What Topics do Firefox and Chrome Contributors Discuss?",
  "Type": "Paper",
  "Key": "2dea5a798c4409f1734928f4cbcbd4",
  "Authors": ["Mario Luca Bernardi", "Carmine Sementa", "Quirino Zagarese", "Damiano Distante", "Massimiliano Di Penta"],
  "Affiliations": ["University of Sannio, Italy", "Unitelma Sapienza University, Italy"],
  "Abstract": "Firefox and Chrome are two very popular open source Web browsers, implemented in C/C++. This paper analyzes what topics were discussed in Firefox and Chrome bug reports over time. To this aim, we indexed the text contained in bug reports submitted each semester of the project history, and identified topics using Latent Dirichlet Allocation (LDA). Then, we investigated to what extent Firefox and Chrome developers/contributors discussed similar topics, either in different periods, or over the same period. Results indicate a non-negligible overlap of topics, mainly on issues related to page layouting, user interaction, and multimedia contents."
},
{
  "Title": "A Tale of Two Browsers",
  "Type": "Paper",
  "Key": "2db6ac202894c8e09f8aacc2c3c817",
  "Authors": ["Olga Baysal", "Ian Davis", "Michael W. Godfrey"],
  "Affiliations": ["University of Waterloo, Canada"],
  "Abstract": "We explore the space of open source systems and their user communities by examining the development artifact histories of two popular web browsers - Firefox and Chrome - as well as usage data. By examining the data and addressing a number of research questions, two very different profiles emerge: Firefox, as the older and established system, with long product version cycles but short bug fix cycles, and a user base that is slow to adopt newer versions; and Chrome, as the new and fast evolving system, with short version cycles, longer bug fix cycles, and a user base that very quickly adopts new versions as they become available (due largely to Chrome's mandatory automatic updates)."
},
{
  "Title": "Do Comments Explain Codes Adequately? Investigation by Text Filtering",
  "Type": "Paper",
  "Key": "40a185ef8a1264ee05fd846d48172a",
  "Authors": ["Yukinao Hirata", "Osamu Mizuno"],
  "Affiliations": ["Kyoto Institute of Technology, Japan"],
  "Abstract": "Comment lines in the software source code include descriptions of codes, usage of codes, copyrights, unused codes, comments, and so on. It is required for comments to explain the content of written code adequately, since the wrong description in the comment may causes further bug and confusion in maintenance. \nIn this paper, we try to clarify a research question: ``In which projects do comments describe the code adequately?'' To answer this question, we selected the group 1 of mining challenge and used data obtained from Eclipse and Netbeans. Since it is difficult to answer the above question directly, we define the distance between codes and comments. By utilizing the fault-prone module prediction technique, we can answer the alternative question from the data of two projects. The result shows that Eclipse project has relatively adequate comments."
},
{
  "Title": "Apples vs. Oranges? An Exploration of the Challenges of Comparing the Source Code of Two Software Systems",
  "Type": "Paper",
  "Key": "1606d2b9439a969883da0292c0ac64",
  "Authors": ["Daniel M. German", "Julius Davies"],
  "Affiliations": ["University of Victoria, Canada"],
  "Abstract": "We attempt to compare the source code of two Java IDE systems: Netbeans and Eclipse. The result of this experiment shows that many factors, if ignored, could risk a bias in the results, and we posit various observations that should be taken into consideration to minimize such risk."
},
{
  "Title": "The Grand Challenges of Software Engineering - A Perspective from the Trenches",
  "Type": "Talk",
  "Key": "infosys-supporter",
  "Authors" : [ "T.S. Mohan" ],
  "Affiliations" : [ "Infosys Technologies Ltd." ]
},
{
  "Title": "Connect and Collaborate",
  "Type": "Talk",
  "Key": "microsoft-supporter",
  "Authors" : [ "Judith Bishop" ],
  "Affiliations" : [ "Microsoft Research" ]
},
{
  "Title": "How Software is Engineered at Google",
  "Type": "Talk",
  "Key": "google-supporter",
  "Authors" : [ "Marija Mikic-Rakic" ],
  "Affiliations" : [ "Google" ]
},
{
  "Title": "Impact Panel",
  "Type": "Panel",
  "Key": "impact-panel-item",
  "Authors" : [ "Carlo Ghezzi", "Pete Rotella", "Richard N. Taylor"],
  "Affiliations" : [ "Politecnico di Milano", "Cisco Research", "University of California, Irvine" ],
  "Abstract" : "This Impact Panel Session is intended to be the focus of a discussion of the general questions surrounding impact: What do we think research impact is? What do other communities regard as impact? How should impact be determined and measured? How can we increase the impact made by our community's research? What new directions might the Software Engineering Community take to stimulate thinking and action in examining these issues? How do the funding bodies view and measure impact?"
},
{
  "Title": "SE Research Grants and NSF \"Broader Impacts\": NSF Changes and SE Researcher Strategies",
  "Type": "Panel",
  "Key": "nsf-strategies-panel-item",
  "Authors" : [ "Margaret Burnett", "Lori Clarke", "Sebastian Elbaum", "Bill Pugh" ],
  "Affiliations" : [ "Oregon State University", "University of Massachusetts Amherst", "University of Nebraska", "NSF" ],
  "Abstract" : "In January 2011, President Obama signed the America COMPETES Reauthorization Act into law. The Act asks the NSF to apply the Broader Impact review criterion to achieve a number of societal goals and to implement a plan for achieving this within six months of the Act becoming law. Thus, NSF has been mandated to take Broader Impacts even more seriously than before. The clock has been ticking since January, and by the time ICSE is held, NSF's procedures will be only two months away from changing. This session's goal would be to define the five different ways the criteria can be fulfilled (only one of which is outreach), with many examples and strategies that should be easily within the comfort zones and means of almost any US software engineering researcher."
},
{
  "Title": "What Industry Wants from Research",
  "Type": "Panel",
  "Key": "what-industry-wants-item",
  "Authors" : [ "Lionel C. Briand", "Tatsuhiro Nishioka", "John Penix", "Wolfram Schulte", "Peri Tarr", "David Weiss" ],
  "Affiliations" : [ "Simula Research Laboratory", "Corporate Software Engineering Center, Toshiba Corporation", "Google", "Microsoft Research", "IBM Thomas J. Watson Research Center", "Iowa State University" ],
  "Abstract" : "Half of the people who attended the first ICSE in 1975 came from industry, but by 2010, industry participation was less than 20%.  This lack of participation hurts both sides: researchers have fewer insights on the problems that are important to practitioners, while practitioners fail to learn what researchers have already discovered that might be useful to them.\n\nSince the publication of \"Making Software\" in 2010, editor Greg Wilson and the other organizers of this panel have sought to close this gap between industry and research.  We started by interviewing leading practitioners in industry to find out what questions they want answered, and what kinds of answers they will find compelling as evidence.  At this panel, representatives from the software industry and professionals that straddle the line between research and practice will use the findings from our interviews as a starting point for discussion and will explore how to bring theory and practice back together."
},
{
  "Title": "Interactivity, Continuity, Sketching & Experience",
  "Type": "Plenary",
  "Key": "keynote-1",
  "Authors": ["Kumiyo Nakakoji"],
  "Affiliations": ["Software Research Associates Inc."],
  "Abstract": "The design and development of a software system deals with both the world of making and that of using. The world of making is concerned with molding, constructing and building. The world of using is concerned with engaging, experiencing, and interacting-with. The two worlds are structurally, semantically, and temporally intertwined through software programs.\n\nDesigning the world of using requires painstaking efforts toward envisioning all the possible situations of use for all the possible types of users in all the possible contexts, in various temporal and situational levels of granularity, to create a coherent and convivial user experience of using the system. Identifying typical use scenarios or depicting snapshots of crucial usage situations does not suffice to frame the world of using. Thorough analyses of the possible flows of interactions over a long period of time through the dynamism of user engagement and experience are essential in framing the world of using. \n\nThrough the delineation of previous and current work on designing and developing research prototype tools for creative knowledge activities, observing and analyzing interaction design processes, and directing user experience design teams for consumer products, this talk will address the expression, representation, communication and assessment of the design of the world of using from the perspectives of interactivity, continuity, sketching and experience."
},
{
  "Title": "Exciting New Trends in Design Thinking",
  "Type": "Plenary",
  "Key": "keynote-2",
  "Authors": ["Bill Dresselhaus"],
  "Affiliations": ["DRESSELHAUSgroup"],
  "Abstract": "Design and design thinking are becoming the hot topics and new business processes around the world--yes, business processes! Business schools are adding design thinking courses to their curricula and business professors are writing books on design thinking. Countries like Korea and Singapore are vying to be the leading Asian Design Nations. New, socalled Convergent courses, programs and schools are emerging globally that combine engineering, business and design disciplines and departments into integrated efforts. The Do-It-Yourself (DIY) Design Movement is gaining momentum and the personal discipline of Making things is coming back. DIY Prototyping and Manufacturing are gaining ground and opportunities with new technologies and innovations. User- Generated Design is becoming a common corporate process. Design process and design thinking are being applied cross-functionally to such global issues as clean water and alternative energy. And the old traditional view of design as art and decoration and styling is giving way to a broader and more comprehensive way of thinking and solving human-centered problems by other than just a few elite professionals. \n\n In light of all this and more, Bill is excited about the ideas of ubiquitous design education for everyone and DIY design as a universal human experience. He is passionate about an idea in what Victor Papanek said 40 years ago in his seminal book, Design for the Real World, “All that we do, almost all the time, is design, for design is basic to all human activity”. Just as all humans are inherently businesspeople in many ways at many times, we are also all designers in many ways at many times—it is time to believe this and make the best of it."
}

	],
	"Sessions": [
{
  "Title": "Announcements",
  "Type": "Plenary",
  "ShortType": "Plenary",
  "Key": "announcements",
  "Day": "5-26-2011",
  "Time": "8:15 am - 8:30 am",
  "Location": "Coral Ballroom 4&5"
},
{
  "Title": "Closing Remarks, ICSE 2012 and 2013 Preview",
  "ShortTitle": "Closing, ICSE 2012 and 2013 Preview",
  "Type": "Plenary",
  "ShortType": "Plenary",
  "Key": "closing-remarks-icse-2012-and-2013-preview",
  "Day": "5-27-2011",
  "Time": "4:45 pm - 5:30 pm",
  "Location": "Coral Ballroom 4&5"
},
{
  "Title": "Comprehending the Drift I",
  "ShortTitle": "Comprehending the Drift I",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "comprehending-drift-i",
  "Day": "5-25-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["mining-message-sequence-graphs", "automatically-detecting-and-describing-high-level-actions-within-methods", "portfolio-finding-relevant-functions-and-their-usages"],
  "Chair": "Martin P. Robillard"
},
{
  "Title": "Comprehending the Drift II",
  "ShortTitle": "Comprehending the Drift II",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "comprehending-drift-ii",
  "Day": "5-26-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["non-essential-changes-version-histories", "aspect-recommendation-evolving-software", "identifying-program-test-and-environmental-changes-affect-behaviour"],
  "Chair": "Alessandro Garcia"
},
{
  "Title": "Comprehending the Drift III",
  "ShortTitle": "Comprehending the Drift III",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "comprehending-drift-iii",
  "Day": "5-27-2011",
  "Time": "10:30 am - 12:00 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["improving-requirements-quality-using-essential-use-case-interaction-patterns", "understanding-broadcast-based-peer-review-open-source-software-projects", "software-systems-cities-controlled-experiment"],
  "Chair": "Yun Yang"
},
{
  "Title": "Debugging the Surf",
  "ShortTitle": "Debugging the Surf",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "debugging-surf",
  "Day": "5-25-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["angelic-debugging", "static-extraction-program-configuration-options", "empirical-study-build-maintenance-effort"],
  "Chair": "Sebastian Elbaum"
},
{
  "Title": "DemoSailing: All Demonstrations",
  "ShortTitle": "DemoSailing: All Demonstrations",
  "Type": "Demonstrations Track",
  "ShortType": "Demo Track",
  "Key": "demosailing-all-demonstrations",
  "Day": "5-27-2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Coral Lounge",
  "Chair": "Margaret-Anne Storey"
},
{
  "Title": "DemoSand: Computer Supported Cooperative Work and Software Engineering",
  "ShortTitle": "Computer Supported Coop. Work",
  "Type": "Demonstrations Track",
  "ShortType": "Demo Track",
  "Key": "demosand-computer-supported-cooperative-work-and-software-engineering",
  "Day": "5-27-2011",
  "Time": "8:30 am - 10:00 am",
  "Location": "South Pacific 3&4",
  "Items": ["using-matcon-generate-case-tools-guide-deployment-pre-packaged-applications", "serebro-facilitating-student-project-team-collaboration", "stakesource20-using-social-networks-stakeholders-identify-and-prioritise-requirements", "miler-toolset-exploring-email-data", "demonstration-distributed-software-design-sketching-tool"],
  "Chair": "Andrew Begel"
},
{
  "Title": "DemoShore: Software Development and Maintenance",
  "ShortTitle": "Software Development & Maintenance",
  "Type": "Demonstrations Track",
  "ShortType": "Demo Track",
  "Key": "demoshore-software-development-and-maintenance",
  "Day": "5-27-2011",
  "Time": "10:30 am - 12:00 pm",
  "Location": "South Pacific 3&4",
  "Items": ["view-infinity-zoomable-interface-feature-oriented-software-development", "codetopics-which-topic-am-i-coding-now", "jdeodorant-identification-and-application-extract-class-refactorings", "evolve-tool-support-architecture-evolution", "portfolio-search-engine-finding-functions-and-their-usages"],
  "Chair": "Andrea Zisman"
},
{
  "Title": "DemoSky: Software Testing and Quality Assessment",
  "ShortTitle": "Software Testing and Quality",
  "Type": "Demonstrations Track",
  "ShortType": "Demo Track",
  "Key": "demosky-software-testing-and-quality-assessment",
  "Day": "5-26-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "South Pacific 3&4",
  "Items": ["bql-capturing-and-reusing-debugging-knowledge", "covana-precise-identification-problems-pex", "quamoco-tool-chain-quality-modeling-and-assessment", "reassert-tool-repairing-broken-unit-tests", "autoblacktest-tool-automatic-black-box-testing"],
  "Chair": "John Grundy"
},
{
  "Title": "DemoSun: Dynamic Software Updates and Analysis",
  "ShortTitle": "Software Updates and Analysis",
  "Type": "Demonstrations Track",
  "ShortType": "Demo Track",
  "Key": "demosun-dynamic-software-updates-and-analysis",
  "Day": "5-25-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "South Pacific 3&4",
  "Items": ["javadaptor-unrestricted-dynamic-software-updates-java", "dyta-dynamic-symbolic-execution-guided-static-verification-results", "identifying-opaque-behavioural-changes", "firedetective-understanding-ajax-clientserver-interactions"],
  "Chair": "George Spanoudakis"
},
{
  "Title": "DemoSurf: Software Analysis & Model Evolution",
  "ShortTitle": "Software Analysis & Model Evolution",
  "Type": "Demonstrations Track",
  "ShortType": "Demo Track",
  "Key": "demosurf-software-analysis-model-evolution",
  "Day": "5-25-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "South Pacific 3&4",
  "Items": ["mt-scribe-end-user-approach-automate-software-model-evolution", "inconsistent-path-detection-xml-ides", "automated-security-hardening-evolving-uml-models"],
  "Chair": "Andy Zaidman"
},
{
  "Title": "Developer Waves",
  "ShortTitle": "Developer Waves",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "developer-waves",
  "Day": "5-26-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["configuring-global-software-teams-multi-company-analysis-project-productivity-quality-and-pr", "does-initial-environment-impact-future-developers", "socio-technical-developer-networks-should-we-trust-our-measurements"],
  "Chair": "André van der Hoek"
},
{
  "Title": "Empirical Luau I",
  "ShortTitle": "Empirical Luau I",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "empirical-luau-i",
  "Day": "5-25-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "South Pacific 1&2",
  "Items": ["empirical-investigation-role-api-level-refactorings-during-software-evolution", "factors-leading-integration-failures-global-feature-oriented-development-empirical-analysis", "assessing-programming-language-impact-development-and-maintenance-study-c-and-c"],
  "Chair": "Prem Devanbu"
},
{
  "Title": "Empirical Luau II",
  "ShortTitle": "Empirical Luau II",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "empirical-luau-ii",
  "Day": "5-27-2011",
  "Time": "8:30 am - 10:00 am",
  "Location": "South Pacific 1&2",
  "Items": ["empirical-assessment-mde-industry", "dealing-noise-defect-prediction", "ownership-experience-and-defects-fine-grained-study-authorship"],
  "Chair": "Anita Sarma"
},
{
  "Title": "Erupting NIER",
  "ShortTitle": "Erupting NIER",
  "Type": "New Ideas and Emerging Results Track",
  "ShortType": "NIER Track",
  "Key": "erupting-nier",
  "Day": "5-27-2011",
  "Time": "8:30 am - 10:00 am",
  "Location": "Coral Ballroom 1",
  "Items": ["tracing-architectural-concerns-high-assurance-systems", "combination-approach-enhancing-automated-traceability", "capturing-tacit-architectural-knowledge-using-repertory-grid-technique", "flexible-generators-software-reuse-and-evolution", "lazy-initialization-multilayered-modeling-framework", "towards-architectural-information-implementation", "topic-based-defect-prediction", "automated-usability-evaluation-parallel-programming-constructs", "data-analytics-game-development"],
  "Chairs": ["Mary Shaw", "Anthony Finkelstein", "Alexander Wolf"]
},
{
  "Title": "Explosive NIER",
  "ShortTitle": "Explosive NIER",
  "Type": "New Ideas and Emerging Results Track",
  "ShortType": "NIER Track",
  "Key": "explosive-nier",
  "Day": "5-25-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 1",
  "Items": ["toward-better-understanding-tool-usage", "characterizing-process-variation", "blending-freeform-and-managed-information-tables", "design-and-implementation-data-analytics-infrastructure-support-crisis-informatics-research", "domain-specific-requirements-model-scientific-computing", "creww-collaborative-requirements-engineering-wii-remotes", "learning-adapt-requirements-specifications-evolving-systems", "towards-overcoming-human-analyst-fallibility-requirements-tracing-process", "positive-effects-utilizing-relationships-between-inconsistencies-more-effective-inconsistenc", "matching-logic-new-program-verification-approach"],
  "Chairs": ["Jeff Magee", "Marsha Chechik", "Sebastián Uchitel"]
},
{
  "Title": "Far-out Surfware Engineering",
  "ShortTitle": "Far-out Surfware Engineering",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "far-out-surfware-engineering",
  "Day": "5-25-2011",
  "Time": "4:00 pm - 4:45 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["demand-feature-recommendations-derived-mining-public-product-descriptions", "inferring-better-contracts"],
  "Chair": "Tao Xie"
},
{
  "Title": "Impact Project Session",
  "ShortTitle": "Impact Project Session",
  "Type": "Impact Project Focus Area",
  "ShortType": "Impact Project",
  "Key": "impact-project-session",
  "Day": "5-26-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 1",
  "Items": ["impact-process-simulation-software-practice-initial-report", "impact-software-resource-estimation-research-practice-achievements-synergies-and-challenges", "symbolic-execution-software-testing-practice--preliminary-assessment"],
  "Chair": "Leon J. Osterweil"
},
{
  "Title": "Incendiary NIER",
  "ShortTitle": "Incendiary NIER",
  "Type": "New Ideas and Emerging Results Track",
  "ShortType": "NIER Track",
  "Key": "incendiary-nier",
  "Day": "5-26-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 1",
  "Items": ["model-based-performance-testing", "tuple-density-new-metric-combinatorial-test-suites", "search-enhanced-testing", "fuzzy-set-based-automatic-bug-triaging", "exploiting-hardware-advances-software-testing-and-debugging", "better-testing-through-oracle-selection", "tracking-data-structures-postmortem-analysis", "iterative-context-aware-feature-location", "study-ripple-effects-software-ecosystems"],
  "Chairs": ["Matt Dwyer", "Antonia Bertolino", "Alessandro Orso"]
},
{
  "Title": "Outrigger Models and Clones",
  "ShortTitle": "Outrigger Models and Clones",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "outrigger-models-and-clones",
  "Day": "5-26-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["model-projection-simplifying-models-response-restricting-environment", "mecc-memory-comparison-based-clone-detector", "frequency-and-risks-changes-clones"],
  "Chair": "Michele Lanza"
},
{
  "Title": "Posters: NIER I and SEAMS",
  "Type": "Posters",
  "ShortType": "Posters",
  "Key": "posters-nier-i-and-seams",
  "Day": "5-26-2011",
  "Time": "10:00 am - 10:45 am",
  "Location": "Coral Lounge"
},
{
  "Title": "Posters: NIER II",
  "ShortTitle": "Posters: NIER II",
  "Type": "Posters",
  "ShortType": "Posters",
  "Key": "posters-nier-ii",
  "Day": "5-26-2011",
  "Time": "3:15 pm - 4:00 pm",
  "Location": "Coral Lounge"
},
{
  "Title": "Program Surfing I",
  "ShortTitle": "Program Surfing I",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "program-surfing-i",
  "Day": "5-25-2011",
  "Time": "4:00 pm - 4:45 pm",
  "Location": "South Pacific 1&2",
  "Items": ["inference-field-initialization", "taiming-reflection-aiding-static-analysis-presence-reflection-and-custom-class-loaders-0", "patching-vulnerabilities-sanitization-synthesis"],
  "Chair": "Wilhelm Schaefer"
},
{
  "Title": "Program Surfing II",
  "ShortTitle": "Program Surfing II",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "program-surfing-ii",
  "Day": "5-27-2011",
  "Time": "10:30 am - 12:00 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["interface-decomposition-service-compositions", "unifying-execution-imperative-and-declarative-code", "always-available-static-and-dynamic-feedback"],
  "Chair": "Harold Ossher"
},
{
  "Title": "Pyroclastic NIER",
  "ShortTitle": "Pyroclastic NIER",
  "Type": "New Ideas and Emerging Results Track",
  "ShortType": "NIER Track",
  "Key": "pyroclastic-nier",
  "Day": "5-27-2011",
  "Time": "10:30 am - 12:00 pm",
  "Location": "Coral Ballroom 1",
  "Items": ["mining-service-abstractions", "software-behaviour-analysis-framework-based-human-perception-systems", "dynamic-shape-analysis-program-heap-using-graph-spectra", "program-analysis-qualitative-analysis-quantitative-analysis", "diagnosing-new-faults-using-mutants-and-prior-faults", "empirical-results-study-software-vulnerabilities", "multifractal-aspects-software-development", "american-law-institutes-principles-software-contracts-and-their-ramifications-software-engi", "toward-sustainable-software-engineering"],
  "Chairs": ["Lionel C. Briand", "Lori Clarke", "Paola Inverardi"]
},
{
  "Title": "Refactoring Your Lei I",
  "ShortTitle": "Refactoring Your Lei I",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "refactoring-your-lei-i",
  "Day": "5-25-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "South Pacific 1&2",
  "Items": ["transformation-class-immutability", "refactoring-java-programs-flexible-locking", "refactoring-pipe-mashups-end-user-programmers"],
  "Chair": "Arie van Deursen"
},
{
  "Title": "Refactoring Your Lei II",
  "ShortTitle": "Refactoring Your Lei II",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "refactoring-your-lei-ii",
  "Day": "5-27-2011",
  "Time": "8:30 am - 10:00 am",
  "Location": "Coral Ballroom 5",
  "Items": ["refactoring-role-objects", "supporting-professional-spreadsheet-users-generating-leveled-dataflow-diagrams", "reverse-engineering-feature-models"],
  "Chair": "Frank Tip"
},
{
  "Title": "Riding the Design Wave  I",
  "ShortTitle": "Riding the Design Wave  I",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "riding-design-wave-i",
  "Day": "5-25-2011",
  "Time": "4:00 pm - 4:45 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["lime-framework-debugging-load-imbalance-multi-threaded-execution", "synthesis-live-behaviour-models-fallible-domains", "coverage-guided-systematic-concurrency-testing"],
  "Chair": "Mauro Pezzè"
},
{
  "Title": "Riding the Design Wave  II",
  "ShortTitle": "Riding the Design Wave  II",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "riding-design-wave-ii",
  "Day": "5-27-2011",
  "Time": "8:30 am - 10:00 am",
  "Location": "Coral Ballroom 4",
  "Items": ["detecting-software-modularity-violations", "feature-cohesion-software-product-lines-exploratory-study", "leveraging-software-architectures-guide-and-verify-development-sensecomputecontrol-applicati"],
  "Chair": "Sam Malek"
},
{
  "Title": "SEIP: Empirical Software Engineering",
  "ShortTitle": "Empirical Software Engineering",
  "Type": "Software Engineering in Practice Track",
  "ShortType": "SEIP Track",
  "Key": "seip-empirical-software-engineering",
  "Day": "5-25-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 2",
  "Items": ["case-study-measuring-process-risk-early-insights-software-safety", "model-driven-engineering-practices-industry", "sorascs-case-study-soa-based-platform-design-socio-cultural-analysis"],
  "Chair": "Walt Scacchi"
},
{
  "Title": "SEIP: Industry Software Architecture",
  "ShortTitle": "Industry Software Architecture",
  "Type": "Software Engineering in Practice Track",
  "ShortType": "SEIP Track",
  "Key": "seip-industry-software-architecture",
  "Day": "5-25-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 2",
  "Items": ["method-selecting-soa-pilot-projects-including-pilot-metrics-framework", "architecture-evaluation-without-architecture-experience-smart-grid", "bringing-domain-specific-languages-digital-forensics"],
  "Chair": "Kristina Winbladh"
},
{
  "Title": "SEIP: Software Engineering at Large",
  "ShortTitle": "Software Engineering at Large",
  "Type": "Software Engineering in Practice Track",
  "ShortType": "SEIP Track",
  "Key": "seip-software-engineering-large",
  "Day": "5-26-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 2",
  "Items": ["building-and-using-pluggable-type-checkers", "deploying-cogtool-integrating-quantitative-usability-assessment-real-world-software-developm", "experiences-text-mining-large-collections-unstructured-systems-development-artifacts-jpl"],
  "Chair": "Joel Ossher"
},
{
  "Title": "SEIP: Software Metrics",
  "ShortTitle": "Software Metrics",
  "Type": "Software Engineering in Practice Track",
  "ShortType": "SEIP Track",
  "Key": "seip-software-metrics",
  "Day": "5-26-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 2",
  "Items": ["evaluation-internal-quality-business-applications-does-size-matter", "characterizing-differences-between-pre-and-post-release-versions-software", "why-software-quality-improvement-fails-and-how-succeed-nevertheless"],
  "Chair": "TS Mohan"
},
{
  "Title": "SEIP: Software Testing and Analysis",
  "ShortTitle": "Software Testing and Analysis",
  "Type": "Software Engineering in Practice Track",
  "ShortType": "SEIP Track",
  "Key": "seip-software-testing-and-analysis",
  "Day": "5-27-2011",
  "Time": "8:30 am - 10:00 am",
  "Location": "Coral Ballroom 2",
  "Items": ["code-coverage-analysis-practice-large-systems", "practical-change-impact-analysis-based-static-program-slicing-industrial-software-systems", "value-based-program-characterization-and-its-application-software-plagiarism-detection"],
  "Chair": "Nicolas Lopez"
},
{
  "Title": "SEIP: Tools and Environments",
  "ShortTitle": "Tools and Environments",
  "Type": "Software Engineering in Practice Track",
  "ShortType": "SEIP Track",
  "Key": "seip-tools-and-environments",
  "Day": "5-27-2011",
  "Time": "10:30 am - 12:00 pm",
  "Location": "Coral Ballroom 2",
  "Items": ["comparison-model-based-and-judgment-based-release-planning-incremental-software-projects", "industrial-case-study-quality-impact-prediction-evolving-service-oriented-software", "enabling-runtime-assertion-checking-concurrent-contracts-java-modeling-language"],
  "Chair": "TS Mohan"
},
{
  "Title": "Surfer Model Checking",
  "ShortTitle": "Surfer Model Checking",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "surfer-model-checking",
  "Day": "5-26-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["symbolic-model-checking-software-product-lines", "verifying-multi-threaded-software-using-smt-based-context-bounded-model-checking", "run-time-efficient-probabilistic-model-checking"],
  "Chair": "Matt Dwyer"
},
{
  "Title": "Surfing the Dependability Wave",
  "ShortTitle": "Surfing the Dependability Wave",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "surfing-dependability-wave",
  "Day": "5-25-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["lightweight-code-analysis-and-its-role-evaluation-dependability-case", "towards-quantitative-software-reliability-assessment-incremental-development-processes", "impact-fault-models-software-robustness-evaluations"],
  "Chair": "Sebastián Uchitel"
},
{
  "Title": "Testing the Waters I",
  "ShortTitle": "Testing the Waters I",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "testing-waters-i",
  "Day": "5-25-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["practical-guide-using-statistical-tests-assess-randomized-algorithms-software-engineering", "acomment-mining-annotations-comments-and-code-detect-interrupt-related-concurrency-bugs", "camouflage-automated-anonymization-field-data"],
  "Chair": "Laurie Williams"
},
{
  "Title": "Testing the Waters II",
  "ShortTitle": "Testing the Waters II",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "testing-waters-ii",
  "Day": "5-26-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Location": "South Pacific 1&2",
  "Items": ["program-abstractions-behaviour-validation", "programs-tests-and-oracles-foundations-testing-revisited", "racez-lightweight-and-non-invasive-race-detection-tool-production-applications"],
  "Chair": "Lionel C. Briand"
},
{
  "Title": "Testing the Waters III",
  "ShortTitle": "Testing the Waters III",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "testing-waters-iii",
  "Day": "5-27-2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Coral Ballroom 5",
  "Items": ["mining-parametric-specifications", "estimating-footprints-model-operations", "precise-identification-problems-structural-test-generation"],
  "Chair": "Andreas Zeller"
},
{
  "Title": "Volcanic NIER",
  "ShortTitle": "Volcanic NIER",
  "Type": "New Ideas and Emerging Results Track",
  "ShortType": "NIER Track",
  "Key": "volcanic-nier",
  "Day": "5-25-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "Coral Ballroom 1",
  "Items": ["perspectives-delegation-team-based-distributed-software-development-over-geni-infrastructure", "hidden-experts-software-engineering-communication", "how-do-programmers-ask-and-answer-questions-web", "sketching-tools-ideation", "digitally-annexing-desk-space-software-development", "information-foraging-foundation-code-navigation", "identifying-method-friendships-remove-feature-envy-bad-smell", "code-orb-supporting-contextualized-coding-glance-views", "permission-based-programming-languages"],
  "Chairs": ["Judith Bishop", "Karin K. Breitman", "Michele Lanza"]
},
{
  "Title": "Web Surfing",
  "ShortTitle": "Web Surfing",
  "Type": "Technical/Research Track",
  "ShortType": "Research Track",
  "Key": "web-surfing",
  "Day": "5-27-2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Coral Ballroom 4",
  "Items": ["automated-cross-browser-compatibility-testing", "framework-automated-testing-javascript-web-applications", "coalescing-executions-fast-uncertainty-analysis"],
  "Chair": "Alessandro Orso"
},
{
  "Title": "Welcome",
  "Type": "Plenary",
  "ShortType": "Plenary",
  "Key": "welcome-0",
  "Day": "5-25-2011",
  "Time": "8:00 am - 8:30 am",
  "Location": "Coral Ballroom 4&5"
},
{
  "Title": "Sixth International Workshop on Automation of Software Test",
  "ShortTitle": "AST 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11AST",
  "Day": "5-23-2011",
  "Time": "8:45 am - 5:00 pm",
  "Location": "South Pacific 4",
  "Abstract": "The workshop on the Automation of Software Test (AST) seeks high quality research and industrial case study papers on the theory and practice of software test automation whilst encouraging discussions through Charette presentations. The focus of AST is aimed at providing researchers and practitioners with a forum for exchanging ideas and experiences, developing an understanding of the fundamental challenges, articulating a vision for the future, and finding promising solutions to pressing problems.",
  "Workshop": true,
  "Items": ["828f1725ac9c68fa9f3c9c1c90037d", "8d709409bce142ccb89bd044936fe2", "cd5c41bac90e175931b2a72868a9c1", "9a7f0e191abaf7a896825f7d572ec5", "4681cc1dc8426c2c606a33f8b2f9ad", "24b0f48ff7b50088c0d72527c8e72d", "fabc4e3026412281545a1027665ca3", "f79dd1883841659eda0235b00a4ea5", "05ce481140e4760cdf566e16246d64", "65f06da1fbf3796c5288697c3b82fc", "520843123f4c88272ad38ffadef552", "c65aebc32ad2498b49257db44204fe", "8c0802a992b909098bde351e898643", "1424c5abefad16a02e820470c7475c", "6945f3f16fed06d030141f48f3efe4", "a670e2439bec74c93310b27b8d67b8", "9729767f9f5bffb69c6df5e5698690", "634e1eeaf1908199776d90738a7873", "2bab594ee45a2cd550551456dad25c", "f9303c3ab4e69a6d1cc49ab71bd4cc"],
  "Chairs": ["Howard Foster", "Antonia Bertolino", "J. Jenny Li"]
},
{
  "Title": "Fourth International Workshop on Cooperative and Human Aspects of Software Engineering",
  "ShortTitle": "CHASE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11CHASE",
  "Day": "5-21-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Sea Pearl 1-2",
  "Abstract": "Software is created by people for people working in varied environments, under various conditions. Thus understanding cooperative and human aspects of software development is crucial to comprehend how methods and tools are used, and thereby improve the creation and maintenance of software. Over the years, both researchers and practitioners have recognized the need to study and understand these aspects. Despite recognizing this, researchers in cooperative and human aspects have no clear place to meet and are dispersed in different research conferences and areas. The goal of this workshop is to provide a forum for discussing high quality research on human and cooperative aspects of software engineering. We aim at providing both a meeting place for the growing community and the possibility for researchers interested in joining the field to present their work in progress and get an overview over the field.",
  "Workshop": true,
  "Items": ["90945d17f9e1bf3dfe1d7080f7033e", "fee728623696ee3be981f514b20409", "ce8f846a9f0cd75f65dd4e77db501b", "8266379404a94d50370ce184016cee", "846609acb5555fa22c69a1c8069500", "45d2ec5236275ecd117bcb8e4df7c7", "ae1de5cd7dc98d80415a91629c5ec0", "f84f1221df2fca511449fcf7b7d52a", "a0596ae1da4c090cf8010914f3ce08", "2f01cfceb7615307cc4fc5f920ee04", "ba1df3adbda4beb8476be9177674d0", "905b27aee54bd5f9f0f6292928ad38", "c44f602b798a598ef3d5d498be7e9b", "ea59b1a01a2dda1f361251807cdc9d", "13686072b6a501a85e28d945fc9d39", "f8b6d5464336a95ba2e63c8ab24070", "71dd2a3352efd8b43b78d1b2af7103", "e03c9dae6e07f88f7e05fa6a954db7", "5f4f908857204722a240ce7f4af474"],
  "Chairs": ["Marcelo Cataldo", "Cleidson de Souza", "Yvonne Dittrich", "Rashina Hoda", "Helen Sharp"]
},
{
  "Title": "Collaborative Teaching of Globally Distributed Software Development: Community Building Workshop",
  "ShortTitle": "CTGDSD 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11CTGDSD",
  "Day": "5-23-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Sea Pearl 5-6",
  "Abstract": "Software engineering project courses where student teams are geographically distributed can effectively simulate the problems of globally distributed software development (DSD). However, this pedagogical model has proven difficult to adopt or sustain. It requires significant pedagogical resources and collaboration infrastructure. Institutionalizing such courses also requires compatible and reliable teaching partners.\n\nThe purpose of this workshop is to foster a community of international faculty and institutions committed to developing, supporting, and teaching DSD. Foundational materials presented will include pedagogical materials and infrastructure developed and used in teaching DSD courses (by the organizers and participants) along with results and lessons learned. Long-range goals include: lowering adoption barriers by providing common pedagogical materials, validated collaboration infrastructure, and a pool of potential teaching partners from around the globe.",
  "Workshop": true,
  "Items": ["8d82fbf96cff50d6a2394bd2896794", "f0679012fba495f014180ca60d6020", "beeb4cc9f727b1dc2865ff54b61c28", "79ad65c910993d3ffeb67766baac07", "5d78236fe6930f08b0f84928543436", "c871e69807cf4bc359aeb265142079", "5aa93d09ace202e1200f72029c6b34", "3370a71eb746d26d53eb8c3c946578"],
  "Chairs": ["Stuart Faulk", "Michal Young", "David M. Weiss", "Lian Yu"]
},
{
  "Title": "Workshop on Games and Software Engineering",
  "ShortTitle": "GAS 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11GAS",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 3",
  "Abstract": "At the core of video games are complex interactions leading to emergent behaviors. This complexity creates difficulties architecting components, predicting their behaviors and testing the results. Software engineering hasn't yet been able to meet the demands of the games industry, an industry that works at the forefront of technology and creativity, where creating a fun experience is the most important metric of success. GAS 2011 will explore the demands of game creation and ascertain how the software engineering community can contribute to this important creative domain. Furthermore, GAS 2011 will investigate how games can help aid the software engineering process or improve software engineering education. Research in these areas has been exciting and interesting, and GAS 2011 will be the first time practitioners from all three fields to have the opportunity to come together at ICSE to investigate the possibilities of this innovative research area.",
  "Workshop": true,
  "Items": ["72d88d9b171f38b7b9f9e29961e663", "f58925fffe7582df1e0ef7b09788bd", "f5bb91d83b03bd637b77d4f3275de5", "a9bea8cdfe3ec16dab6eca28e7af13", "3a811da800f9e5dbdf707f9c9e9a9d", "dd1a48b519ff16423754db03506b9c", "07ba3e97e30453bcf41bc8eea60259", "41ec092921ab8911fc52f177c5aea2", "43c8409fdb7e5de5f5000f8998443b", "3d7a27aa7fe55a1d6a234367c85858", "f1b891b2f393ea651f9f2c90080449", "158203f9108e548756e8ff695e326a", "5c6b3edcd806e6d2dd7170f3f03db6", "d888eccfb8997fba034d3652dfe39a"],
  "Chairs": ["Jim Whitehead", "Chris Lewis"]
},
{
  "Title": "Fourth International Workshop on Multicore Software Engineering",
  "ShortTitle": "IWMSE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11IWMSE",
  "Day": "5-21-2011",
  "Time": "9:00 am - 6:30 pm",
  "Location": "Sea Pearl 3-4",
  "Abstract": "Multicore processors are ubiquitous and every new computer is a truly parallel machine. This is a fundamental change in the history of computing: parallelism is not confined to scientific applications any more but becomes available for everyone at low cost. Everyday applications and industry applications will need to be parallel in order to exploit the full hardware potential. As a consequence, software engineers now face the challenge of parallelizing applications of all sorts. Compared to sequential applications, our repertoire of tools and methods for cost-effectively developing reliable and robust parallel applications is spotty. The purpose of this workshop is to bring together researchers and practitioners with diverse backgrounds in order to advance the state of the art in multicore software engineering.",
  "Workshop": true,
  "Items": ["13ee4dc296f5acaa7663908ffbc842", "a5a8d51028d43c7b5c2a5207cec8ca", "c219e63c7cc665b8f1a107589de7c7", "b59e6ecfcdd47ec05aed56ca3b2c22", "faf08d7756e10976fc4e4afcbeae3e", "570af13048ba13ff7a6ada0effe02b", "5cd7d6c891b9733f28a2bbb7e2bc77"],
  "Chairs": ["Victor Pankratius", "Michael Philippsen"]
},
{
  "Title": "Fifth International Workshop on Software Clones",
  "ShortTitle": "IWSC 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11IWSC",
  "Day": "5-23-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 3",
  "Abstract": "Software clones are identical or similar pieces of code, design or other artifacts. Clones are known to be closely related to various issues in software engineering, such as software quality, complexity, architecture, refactoring, evolution, licensing, plagiarism, and so on. Various characteristics of software systems can be uncovered through clone analysis, and system restructuring can be performed by merging clones. The goals of this workshop are to bring together researchers and practitioners from around the world to evaluate the current state of research and applications, discuss common problems, discover new opportunities for collaboration, exchange ideas, envision new areas of research and applications, and explore synergies with similarity analysis in other areas and disciplines.",
  "Workshop": true,
  "Items": ["db391b9711f7fcd6e502160a8d456c", "e896b28039ae7060078d93922d8f66", "e84e84846dd15b9b06e0eb275af5a9", "1ace2b658c1f4592b1fb8e2b3d4d35", "af8c0a27a57dfeff4e79d64ac82645", "f49a48ee861f39b07396ea6af3a779", "7fb5b25896d9ea9b8b0862f54a4916", "0da010c8f46556c1d9a4cd5b0121b1", "d0e354d279f212f08832b294196534", "d85a83174fd804ee8ead125740e291", "1f21391255764f7cef9bb7465bc173", "eb0903cef2aa1aad500b13d6b5b649", "2e55d8fad3437a38e9db346530d583", "8d5849e382ed4edba00e295fabc2d7", "c994e646c70ee1dcf81ab46929c287", "821a844939c24ab8d266da02097a33", "866bd8cd58eb5680d621550fd00644", "014b34bdaf8ede00ee3aca438b4127", "3befc790cab8489f439f31ae0bb2ff", "237cd7d0433e6d2852c0963de08f88", "5e2baa2dc9fa4ad7da2dc3a4ad85bf"],
  "Chairs": ["James R. Cordy", "Katsuro Inoue", "Stanislaw Jarzabek", "Rainer Koschke"]
},
{
  "Title": "Second International Workshop on Managing Technical Debt",
  "ShortTitle": "MTD 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11MTD",
  "Day": "5-23-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Nautilus 1",
  "Abstract": "The technical debt metaphor is gaining significant traction in the software development community as a way to understand and communicate issues of intrinsic quality, value, and cost. The idea is that developers sometimes accept compromises in a system in one dimension (e.g., modularity) to meet an urgent demand in some other dimension (e.g., a deadline), and that such compromises incur a “debt”: on which “interest” has to be paid and which should be repaid at some point for the long-term health of the project. Little is known about technical debt, beyond feelings and opinions. The software engineering research community has an opportunity to study this phenomenon and improve the way it is handled. We can offer software engineers a foundation for managing such trade-offs based on models of their economic impacts. The goal of this second workshop is to discuss managing technical debt as a part of the research agenda for the software engineering field.",
  "Workshop": true,
  "Items": ["3d18b20e4d8d355bdd42e51c29cf74", "8ddabe363556cbd4a2e0e32a767a85", "60bfdcbeebf29be43c52c1e358350c", "450eabf26ed84cf1735ead4a05a314", "730fc9acce5bfd6fa6d8ae946f98e1", "5b3f1b6a189089a921cc15fcdc0116", "c2062db158a226591d0c6d6aeb5bd9", "488d431b023eeb11abbb1c84af4c8f", "bb5c99bd63693c2a6dff708c921107"],
  "Chairs": ["Ipek Ozkaya", "Philippe Kruchten", "Robert L. Nord", "Nanette Brown"]
},
{
  "Title": "Third International Workshop on Principles of Engineering Service-Oriented Systems",
  "ShortTitle": "PESOS 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11PESOS",
  "Day": "5-23-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Nautilus 2",
  "Abstract": "Service-oriented systems represent a new class of software system in which software is being used and integrated as external, loosely-coupled services rather than being physically integrated and owned permanently.  Service-oriented systems provide a more flexible approach to software development, provisioning and maintenance because services offer reusable functionality that can be combined to support business processes (or similar) that are dynamic by nature.  This enables the adaptation to changes in a system's environment as well as to the continuously evolving system requirements that are commonplace today.  However, service orientation poses challenges to more traditional approaches to software development, stemming from the lack of homogeneity of its basic components and from the requirement of being able to accommodate changes and dynamic evolution right from the beginning.  In this respect, the workshop aims at finding possibilities of synergy between Software Engineering (SE) technologies and Service-Oriented Computing (SOC) that are beneficial to both fields.",
  "Workshop": true,
  "Items": ["382014c1e7da7ba90d6b7bc36df071", "1bbfc0ae8dfa6d70cc30de9343b7a2", "3596837f7bd93c5244079ff56d13d7", "605e67badedd920b4eb32c46fd641c", "fa0050f0531354cef74ef893f43ee6", "b995ffd236259581086d2296f2539b", "0167b87f69f47c5a929c3aadcd7313", "9e3536e3c68c07d7644c65510e1720", "ff6f976815b2c3555ce4735dbb9946"],
  "Chairs": ["Manuel Carro", "Dimka Karastoyanova", "Grace A. Lewis", "Anna Liu"]
},
{
  "Title": "Second International Workshop on Product Line Approaches in Software Engineering",
  "ShortTitle": "PLEASE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11PLEASE",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 2",
  "Abstract": "By adopting Software Product Line Engineering (SPLE) practices, organizations can achieve significant improvement in time-to-market, engineering and maintenance costs, portfolio size, and quality. However, despite the known benefits of SPLE over traditional reuse approaches, SPLE is still in the early adopter stage.\n\nThe main goal of PLEASE is to bring together industrial practitioner and software product line researchers in order to couple real-life industrial problems with concrete solutions. We plan for a 2-day highly interactive event in which concrete solutions will be proposed for the identified industrial problems. Besides promoting SPLE adoption, this will also allow validating existing techniques developed by the research community and identifying unresolved problems in the SPLE area. As the result, we will be able to feed researchers with real-life industrial SPLE problems and to establish agenda for future research. These goals are in the spirit of moving researchers and industrial practitioners into Pasteur's Quadrant.\n\nThis event will also allow industrial practitioners to learn from each other's experience and to establish long lasting collaboration between the industry and research. Towards this end, we will also discuss and establish measures of progress and benefits from applying different solutions.",
  "Workshop": true,
  "Items": ["883f1bc423901e1683e5a9b93e5ae4", "c62633b1b27f050ec7da47dc480c26", "72a05c4161ee5895837181ba7ca242", "804eec556f8180d58cae78d0fb30aa", "285f9e978535a37159cf04ecc33a89", "fe06e32733d537652f2277b761dfe6", "695dde743d5b3d41660f8af4d18060", "632c4fa2f1b0a5a34589adfef38fe3", "3e42301008c1ad19df93d7cdc36700", "8f1989f84ef8b05ac647bcfcc71e35", "73509b2e3aec634379c0879cd765cf", "7eb619def0933f5cf5553fa1310095", "16390fbdce2284d0f389de382f2e02"],
  "Chairs": ["Julia Rubin", "Goetz Botterweck", "Andreas Pleuss", "David M. Weiss"]
},
{
  "Title": "Workshop on Software Engineering for Cloud Computing",
  "ShortTitle": "SECLOUD 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SECLOUD",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 4",
  "Abstract": "Cloud Computing has engendered a disruptive change in the requirements, architecture, implementation and evolution methodologies for software – in short, it has challenged our thinking as software engineering (SE) researchers and practitioners in ways that we as a community are still scrambling to find out.\n\nWhether you are interested in Infrastructure-as-a-Service (IaaS) such as Amazon Web Services, or AWS; platform-as-a-service (PaaS) including Google App Engine or Azure; software-as-a-service (SaaS) e.g., salesforce.com; or in frameworks that enable the above like Apache Hadoop or Microsoft’s Dryad, we feel that the time is ripe to rethink SE foundations in light of the cloud.\n\nThe SECLOUD workshop will focus on identifying the SE for cloud 'grand challenges' that lay before us. We will debate existing notions of SE for the construction of cloud services and for their deployment. We will evangelize success stories and how they were arrived at. We anticipate as outcomes the definition of a long-term concrete SE research agenda for cloud, and the sharing of existing SE for cloud “tribal knowledge” that can be applied in the short-term.",
  "Workshop": true,
  "Items": ["64e1c26a863e56a4ea165f0d4c456a", "df000fbe6bd8f903e76415248af76e", "958e2d3419ff9c4b1186deeb013467", "5aedfd98fa708d33e40b5769159f48", "d7a1d89085e26c593e1b61bcfdc86e", "5f30465c10e03d83038877bc0cbd7e", "7b4ab958d7d7c214e2cb170e6af5a3", "c6eca7bf04954798ae95d886e28121", "7da7d3af49b051e77c72a96680623a", "960b195911a31f2157cea0710d0921", "6219143c1961b54c20f889838a42e7", "f3749c9f96288883927409c514de1c", "43649b73e948c8ab5a242b53dc9f22", "fa97060e943e93308f2e73f661f488", "326f486842f29265ca56ad29ede21c"],
  "Chairs": ["Chris A. Mattmann", "Nenad Medvidovic", "T. S. Mohan", "Owen O'Malley"]
},
{
  "Title": "Fourth International Workshop on Software Engineering for Computational Science and Engineering",
  "ShortTitle": "SE-CSE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SECSE",
  "Day": "5-28-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 2",
  "Abstract": "Computational Science and Engineering (CSE) software supports a wide variety of domains including nuclear physics, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increase in the importance of CSE software motivates the need to identify and understand appropriate software engineering (SE) practices for CSE. Because of the uniqueness of CSE software development, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the CSE development environment. This situation creates an opportunity for members of the SE community to interact with members of the CSE community to address this need. This workshop facilitates that collaboration by bringing together members of the SE community and the CSE community to share perspectives and present findings from research and practice relevant to CSE software. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods for studying CSE software engineering.",
  "Workshop": true,
  "Items": ["fe044764b6f30b977205314141b987", "a6b0b7d762b5bc53d2cf751ea0c5f6", "e0be6bd69a8c0a0c3201d13181af95", "fdd6e2acfa11219b0dcd5c364a38f0", "59ba8fbb0273c42c929cd32bb99a15", "9190db92f6bcb5dcffadc91d77d674", "a92476630c29bd1a5fe50046390622", "bc22c88e53fe4c456bbd92a87e97be"],
  "Chairs": ["Jeffrey C. Carver", "Roscoe Bartlett", "Ian Gorton", "Lorin Hochstein", "Diane Kelly", "Judith Segal"]
},
{
  "Title": "Third International Workshop on Software Engineering in Healthcare",
  "ShortTitle": "SEHC 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SEHC",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Sea Pearl 1-2",
  "Abstract": "There are enormous opportunities for use-inspired, fundamental research in computer science and software and systems engineering research in the domain of health, healthcare and medical informatics. To date, however, the health and software engineering research communities have had very little interaction. The consequence is that important research problem formulations remain unrecognized and innovative approaches remain undeveloped. The goal of this workshop is to bridge this divide to help catalyze software engineering research and development activities in the demanding domains of health, healthcare and medical informatics. The workshop will thus bring together distinguished researchers from both of these fields, who are already active at the boundaries between the fields, to develop a vision and strategy for research community formation and activity in the coming years.",
  "Workshop": true,
  "Items": ["9488cf1669e1870ecb298cac9127ab", "3c2219002903356b6ce700c4eae048", "660898c77c0350c4ce9144aa653a57", "19f51d07ad22e4235eee4261f06cc9", "c2f4214ae2fe2887b476e679cb7a09", "fa02b61470373f1b9d720c1f0f2bc9", "7a36348a90df98997c0e472aef8562", "710082fef1e63703436a977f6b7843", "adb92ec3d1715caa69dd34ea320a80", "a9b7462dc8750a64a6651297f91973", "9f24908e8f4c395ce70e316f182e65", "c237d8179853105b6cec994309ca85"],
  "Chairs": ["Eleni Stroulia", "Kevin Sullivan"]
},
{
  "Title": "Second International Workshop on Software Engineering for Sensor Network Applications",
  "ShortTitle": "SESENA 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SESENA",
  "Day": "5-22-2011",
  "Time": "8:45 am - 5:45 pm",
  "Location": "Hibiscus 2",
  "Abstract": "Wireless sensor networks (WSNs) are a fundamental building-block of the upcoming Internet of Things, as they enable seamless integration of the digital and physical worlds. Despite the interest raised by this decade-old research topic, the development of WSN software is still carried out in a rather primitive fashion, by building software directly atop the operating system and by relying on the individual, hard-earned programming skills. Software engineering (SE) support is therefore sought, not only to ease the development task but also to make it more reliable, dependable, and repeatable.\n\nThe aim of SESENA11 is to attract researchers belonging to both the SE and WSN communities, not only to exchange their recent research results on the topic, but also to stimulate discussion about the core open problems and to define a shared research agenda. The workshop welcomes both research contributions and position statements. SESENA11 will also include a 'speakers corner' session composed by impromptu presentations where any of the attendees will be given a chance to present their own views in very short bouts.",
  "Workshop": true,
  "Items": ["58869c7ee74752419b2fdb5cb253ff", "f7881b8287e13ba00cfa44d81489f0", "2882a657d493cab2ad572c22c5bddb", "8cae3d724afe5aa03f7f122d971a85", "345280cd8bdae988e704a14795467e", "b931c14a2bf65c3f5da6fa9b9bc749", "558189bd75cf4c4f61d417687ada17", "e3a932610ec67d8786b2af0bed5421", "aee2783ecbd449b25261db6ebbb482", "3aba5a9273a1fecdad4836932ad7a9", "a36831037ac5fd39900f110a8850d9", "689bf6ea59394df65d1b3ac64e92f7", "8ca3cc9c6d29173b379d1079d5c0c1"],
  "Chairs": ["Kurt Geihs", "Luca Mottola", "Gian Pietro Picco", "Kay Römer"]
},
{
  "Title": "Seventh International Workshop on Software Engineering for Secure Systems",
  "ShortTitle": "SESS 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SESS",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Sea Pearl 5-6",
  "Abstract": "The 7th edition of the SESS workshop aims at providing a venue for software engineers and security researchers to exchange ideas and techniques. In fact, software is at core of most of the business transactions and its smart integration in an industrial setting may be the competitive advantage even when the core competence is outside the ICT field. As a result, the revenues of a firm depend directly on several complex software-based systems. Thus, stakeholders and users should be able to trust these systems to provide data and elaborations with a degree of confidentiality, integrity, and availability compatible with their needs. Moreover, the pervasiveness of software products in the creation of critical infrastructures has raised the value of trustworthiness and new efforts should be dedicated to achieve it. However, nowadays almost every application has some kind of security requirement even if its use is not to be considered critical.",
  "Workshop": true,
  "Items": ["e8b1602433a1a562c8532199790a39", "23e91a4adbc3d196c76e0a88debe47", "2d77a427ac03fc180ab86ed9745d93", "a01f54b621d17f2c72b724d4f4cb9b", "de5e2a14122966d344945f708af1ec", "ecd6791584724b6485aec35b5a6ae4", "a96b2d047ea8cbcd9f5c23787298e9", "f7380590a14088392eba164d08b4a2"],
  "Chairs": ["Seok-Won Lee", "Mattia Monga", "Jan Jürjens"]
},
{
  "Title": "Workshop on SHAring and Reusing architectural Knowledge",
  "ShortTitle": "SHARK 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SHARK",
  "Day": "5-24-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Kahili 1",
  "Abstract": "SHARK focuses on current and emerging methods, languages, notations, technologies and tools to extract, represent, share, use and reuse architectural knowledge. Architectural Knowledge (AK) is the integrated representation of the software architecture of a software-intensive system (or a family of systems), the architectural design decisions, and the external context/environment. It is recognized as the means for architecture governance; it facilitates and supports collaboration and the transfer of expertise.\n\nIn this sixth SHARK edition we will investigate the approaches for AK personalization, where knowledge is not codified through templates or annotations, but it is exchanged through the discussion between the different stakeholders. Therefore, the emphasis does not lie on resource-intensive documentation but on lightweight, just-in-time conversations facilitated by 'knowledge yellow pages' (who knows what). The AK community has not explored AK personalization in depth, even though it has acknowledged its value as a viable approach.\n\nIn good tradition, SHARK aims to bring together researchers and practitioners that are interested in sharing and reusing architectural knowledge. The workshop will kick-start with short position statements from the paper authors. The main focus will be on fostering creative discussion between the participants, on specific themes.",
  "Workshop": true,
  "Items": ["b5b287466fd8e704b6ced10a763d6f", "b065b6c910af50618c004522a72959", "5384bea40f7579ac60c6ca51944a2b", "c1514b9850ecd3ab7a35bbc780f412", "b750e68e9e0560a66d05084f33e894", "a15e3c4604de39c3fb7aca1070bb26", "c9ff829909e8e75c3073700cc0b860", "b52485d9fd19818a07a110aed8ec8c"],
  "Chairs": ["Paris Avgeriou", "Patricia Lago", "Philippe Kruchten"]
},
{
  "Title": "Third International Workshop on Search-Driven Development: Users, Infrastructure, Tools, and Evaluation",
  "ShortTitle": "SUITE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11SUITE",
  "Day": "5-28-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 1",
  "Abstract": "SUITE is a workshop that focuses on exploring the notion of search as a fundamental activity during software development. The first two editions of SUITE were held at ICSE 2009/2010, and they have focused on the building of a research community that brings researchers and practioners who are interested in the research areas that SUITE addresses. While this thrid workshop continues the effort of community building, it puts more focus on addressing directly some of the urgent issues identified by previous two workshops, encouraging researchers to contribute to and take advantage of common datasets that we have started assembling for SUITE research.",
  "Workshop": true,
  "Items": ["828c5cb2f666e0b8f54bca7be9e1d3", "ce55c281f9a6c156f526ef081ce238", "9b5bc245d60b74824edb3beafe8801", "db4ed4ed0c3aa0ff657e5d529e9fb1", "50fb974754f0918b1fe59ce35630b0", "e5c9a2efb28c09bf8f4dd11845a06e", "8de303a8d2e0c51b3043da0e3c9521", "942192c8e2603bb2b4b06fb418f642", "1f23e4502822a905aaafed365d647e", "5621f8140a80438e73f2fcf06de2b0", "623539daea8bba9a23fab57c7d47ba"],
  "Chairs": ["Sushil Bajracharya", "Adrian Kuhn", "Yunwen Ye"]
},
{
  "Title": "Sixth International Workshop on Traceability in Emerging Forms of Software Engineering",
  "ShortTitle": "TEFSE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11TEFSE",
  "Day": "5-23-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Sea Pearl 3-4",
  "Abstract": "The Sixth International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2011) will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering artifacts, ranging from user needs to models to source code. The objective of the 6th edition of TEFSE is to build on the work the traceability research community has completed in identifying the open traceability challenges. In particular, it is intended to be a working event focused on discussing the main problems related to software artifact traceability and propose possible solutions for such problems. Moreover, the workshop also aims at identifying key issues concerning the importance of maintaining the traceability information during software development, to further improve the cooperation between academia and industry and to facilitate technology transfer.",
  "Workshop": true,
  "Items": ["9a098ef49040837eeade3d9c4d2fcc", "5c60d51d948215b400d3ad90a7bd38", "a246a56fd24b431081f0d5bf88b67a", "6c0f3d9368d18d3a182a4218551585", "17084583edbe62efd8ad5637368307", "fc1dcffdcc513af8b7c91197b688e8", "0634917c79c03a437819dfc0dc2157", "18cb4a491f58ae514a6e3cd3d8459b", "92b602b22b1eaec6e1ada77013f1a8", "267bcf197f037655e602596746c8d3", "46c9e55bfd0ac6a7ccfcc509ddb48e", "a71df86368925972b370640e72483f", "746bf65c2ab3cfa4c4cf09be00fb7d", "b5c2406783f6368d66757e4c8ee7c8", "fc1efacd42774c4838aa3039db281a", "cf0dd743b243efa8aebcb3c1b09c03"],
  "Chairs": ["Denys Poshyvanyk", "Massimiliano Di Penta", "Huzefa Kagdi"]
},
{
  "Title": "First Workshop on Developing Tools as Plug-ins",
  "ShortTitle": "TOPI 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11TOPI",
  "Day": "5-28-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 3",
  "Abstract": "Our knowledge as to how to solve software engineering problems is increasingly being encapsulated in tools. These tools are at their strongest when they operate in a pre-existing development environment that can provide integration with existing elements such as compilers, debuggers, profilers and visualizers. Some also exist beyond development time and work with the runtime. A further challenge is to develop tools that can span different – and future - development environments and runtimes. This workshop should of interest to all those interested in developing tools as plug-ins for IDEs, runtimes and browsers. We will examine the categories of problems that are best solved in this way, and look at the future challenges. Attendees should have a working knowledge of an IDE such as Visual Studio 2010, Eclipse or MonoDevelop, and experience or interest in tool development.",
  "Workshop": true,
  "Items": ["0ab56946d6190b2b7dd760854bf8cf", "d5ce22521b5ad7b75ec5b133f8e978", "40a593638dc7047f8e7168d6d94e98", "aef68bf29fea4d8f4bddc216b4817e", "d581f019d28f702ba66dd9b8a9e074", "f736d9e45495c51a43172227d67c7a", "32721ff4880e376268150ecf596591", "1decf61e280dca2e78628ed3876b02", "cc6e7eb09ca67d77463951b02370a1", "f3723606f9b92d35e9f5cbd69a220d", "d424ec4401b1d20d9762ff21607e15", "95839523c7d24b99f9a53b03f0b297", "9cd1b11d509628952de96f26a06b3d", "3f074fcf75921d6ac27d6fe7ae28d8", "6edfc59ab349d9caa13fbaa7292111", "2c29c3592632fe8a06d8fd94b15205", "cd9a3a61436178cd4e809c86048b62", "f0b3e9b2ff2b369184048e6fa322fe", "9c54887964c1470d40aeb7886228d7", "d3dfb238b3f69950daacb07cac7791", "134bcb750d029655f95d2f2a060291"],
  "Chairs": ["Judith Bishop", "David Notkin", "Karin K. Breitman"]
},
{
  "Title": "Second International Workshop on Web 2.0 for Software Engineering",
  "ShortTitle": "Web2SE 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11WEBSE",
  "Day": "5-24-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Nautilus 1",
  "Abstract": "Social software is built around an 'architecture of participation' where user data is aggregated as a side-effect of using Web 2.0 applications. Web 2.0 implies that processes and tools are socially open, and that content can be used in several different contexts. Web 2.0 tools and technologies support interactive information sharing, data interoperability and user centered design. For instance, wikis, blogs, tags and feeds help us organize, manage and categorize content in an informal and collaborative way. Some of these technologies have made their way into collaborative software development processes and development platforms. These processes and environments are just scratching the surface of what can be done by incorporating Web 2.0 approaches and technologies into collaborative software development. Web 2.0 opens up new opportunities for developers to form teams and collaborate, but it also comes with challenges for developers and researchers. Web2SE aims to improve our understanding of how Web 2.0, manifested in technologies such as mashups or dashboards, can change the culture of collaborative software development.",
  "Workshop": true,
  "Items": ["e038297cec28170683ed26e2d82224", "9760073eeb538fb21e5df2e1e16b8b", "70cac7deff1dc7dda11a6c09cc1416", "c8444885d736d11d783ad475188fb8", "aaa8bd5709201e4aea516d4e0b34ff", "b3c3a7fe04b63d093b1d80078a10a5"],
  "Chairs": ["Christoph Treude", "Margaret-Anne Storey", "Arie van Deursen", "Andrew Begel", "Sue Black"]
},
{
  "Title": "Workshop on Emerging Trends in Software Metrics",
  "ShortTitle": "WETSoM 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11WETSOM",
  "Day": "5-24-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 3",
  "Abstract": "The Workshop on Emerging Trends in Software Metrics aims at bringing together researchers and practitioners to discuss the progress of software metrics. The motivation for this workshop is the low impact that software metrics has on current software development. The goals of this workshop are to critically examine the evidence for the effectiveness of existing metrics and to identify new directions for development of software metrics.",
  "Workshop": true,
  "Items": ["bd29abb14db917d99d7c4d745a5f0e", "042f7b532240444dffc309ddfdc629", "8f5ce25445ec0b8fb6d7ffa5d8f3f9", "51df3dee541b83001e0393fde8730f", "82b15988d17ffe72fe103d804b2818", "46263bf7316e8b5790a9617e98014a", "f4c3c31c8524cdb5f9b71d93d2065d", "3d99ece0a4db5c2ec73d6303ccad4f", "52b3b635ffc3b789e066a46a55655e", "31d5bcf61758556c9f4124969d5eb0", "a73b48c8a4360a3a3ea0b80022e2bc", "e05899fb27fd7a3d94aaa1014c5585", "6029e48dda9524e5fa79c7c2b4bce2", "f48649d2fed8a3256b2ae810721561", "07528ff991d55beccc1cea97215a46"],
  "Chairs": ["Giulio Concas", "Massimiliano Di Penta", "Ewan Tempero", "Hongyu Zhang"]
},
{
  "Title": "Fourth Workshop on Refactoring Tools",
  "ShortTitle": "WRT 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11WRT",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:30 pm",
  "Location": "Sea Pearl 3-4",
  "Abstract": "While there is a great deal of interest in developing tool support for refactoring, researchers and tool vendors rarely work together. This forum will enable the transfer of ideas and expertise both ways: researchers can show the state-of-the-art analyses they are using in developing tool support for refactoring, and tool vendors can offer valuable insights on the challenges of scaling such analyses to realistic applications. By bringing together researchers and tool vendors we can shorten the time to embody ideas into production systems. In addition, by making researchers aware of what others are working on, the potential for reinventing the wheel is reduced while the potential for creative collaboration is enhanced.This workshop is the next step in our ongoing effort to create such a community, building on our successful refactoring workshops at ECOOP 2007 and OOPSLA 2008.",
  "Workshop": true,
  "Items": ["d7abb9a050a53c85883d2c6c582f5f", "c55d38ca060d16949dcb69774aa6fc", "e51227a53d6fcd42a1f49036000d5e", "9dd1098af26a64c6c688cae76663ba", "65abc1f139ca0d363ab6cad2248f58", "cc9370e610270b0e7b31e249f7addb", "207b0d981debf9c709894ecd609e50", "aa7882b41075e36ef3a68b92e622d6"],
  "Chairs": ["Danny Dig", "Don Batory"]
},
{
  "Title": "8th Working Conference on Mining Software Repositories",
  "ShortTitle": "MSR 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "MSR2011",
  "Day": "5-21-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "South Pacific 1",
  "Abstract": "The Mining Software Repositories (MSR) field analyzes the rich data available in software repositories to uncover interesting and actionable information about software systems and projects. The goal of this two-day working conference is to advance the science and practice of MSR.",
  "Workshop": true,
  "Items": ["ea9b26e54a111369371853ee44e797", "f5181393804e53d4e86a95bea4c0bc", "f6d09aab78aedbfa7f19a562b7c302", "abadf565c709a63bc4eb5726b7f8da", "4451316fee89ab420446e477b981b8", "a0b6faccacc255ba1c12eb8676747e", "10790789b2ba4ae11d0c6a2df2fb79", "d2c86ee9c342c091604067a0fe96a3", "cb44a5df494900e50e37847e7340b7", "aad811fb5ef508a86a27d77693c545", "ded086338a3a0e20604d669b25467d", "a8a3d367d3d362055493f4254ca7a8", "37cf47ae19d2b85537af8ce0e249ab", "3880d0db5bbe27535e9a8e9ef2b674", "54413c4325db3bbff985025f083e0d", "c735abee252b205ec18d86c53d36ef", "85e0b3681470e06c9c27a6adac9210", "915cf7a613a6ce74c9b6fc8919aa7f", "02acda2f9c794fef376c3d9aca4176", "f47b2ea7cea93b3d17e13b7ed39dfc", "2beaf37e02a37242e52f852f825dc2", "2834c29fdb43d0ac2a18425b989df1", "65027048902f4bc025b5373bd34613", "caed91ab726d05539b6e83722e257c", "51c032bc0fc1ce223cea653ebe8256", "31bd543e45b70020309102f1b556ba", "e3a7aa57ace54a0badbcad812ff524", "d4225a710b8e40a05829db62db94c2", "d2349f6eb56c8c70ee784ca38d292f", "0edb49f8a9e130d9d19fb38fb6873f", "2dea5a798c4409f1734928f4cbcbd4", "2db6ac202894c8e09f8aacc2c3c817", "40a185ef8a1264ee05fd846d48172a", "1606d2b9439a969883da0292c0ac64"],
  "Chairs": ["Arie van Deursen", "Tao Xie", "Thomas Zimmermann", "Adrian Schröter"]
},
{
  "Title": "Keynote: Nakakoji",
  "Type": "Plenary",
  "Key": "keynote-session-1",
  "Day": "5-25-2011",
  "Time": "8:30 am - 10:00 am",
  "Items": ["keynote-1"],
  "Location": "Coral Ballroom 4&5"
},
{
  "Title": "Keynote: Dresselhaus",
  "Type": "Plenary",
  "Key": "keynote-session-2",
  "Day": "5-26-2011",
  "Time": "8:30 am - 10:00 am",
  "Items": ["keynote-2"],
  "Location": "Coral Ballroom 4&5"
},
{
  "Title": "Workshop on Flexible Modeling Tools",
  "ShortTitle": "FlexiTools 2011",
  "Type": "Co-located Event",
  "ShortType": "Co-located Event",
  "Key": "ICSEWS11FLEXI",
  "Day": "5-22-2011",
  "Time": "8:30 am - 5:00 pm",
  "Location": "Hibiscus 1",
  "Abstract": "During the exploratory phases of design, it is more common to use white boards, pen and paper or other informal mechanisms than modeling tools. During the early stages of requirements engineering, it is more common to use office tools. These informal approaches are easy to learn and give the user great freedom, among other advantages. Yet, as in many other tasks throughout the software lifecycle, the advantages of modeling tools would be valuable too: multiple views, providing domain-specific assistance, ensuring consistency, etc.\n\nFormal modeling tools and more informal but flexible, free-form approaches have complementary strengths and weaknesses. Whichever practitioners choose for a particular task, they lose the advantages of the other, with attendant frustration and loss of productivity. What can be done about this unfortunate dichotomy? Flexible modeling tools that blend the advantages of modeling tools and the more free-form approaches would allow users to make tradeoffs between flexibility and precision/formality and to move smoothly between them. They might be modeling tools with added flexibility, or office tools with added modeling support, or tools of a new kind.\n\nThe focus of the workshop will be on challenge problems in the area of flexible modeling. In burgeoning fields, it is valuable for the community to identify key, difficult problems that help to define the research area and serve as a means of evaluating the the success of proposed solutions in that area. The concrete goals of this workshop are to identify a foundational set of challenges and concerns for the field of flexible modeling, and promising directions for addressing each. To that end, it will bring together people who understand tool users' needs, usability, user interface design and tool infrastructure.",
  "Workshop": true,
  "Chairs": ["Harold Ossher", "André van der Hoek", "Margaret-Anne Storey", "John Grundy", "Rachel Bellamy", "Marian Petre"]
},
{
  "Title": "Impact Panel",
  "ShortTitle": "Impact Panel",
  "Type": "Panel",
  "ShortType": "Panel",
  "Key": "impact-panel",
  "Day": "5-27-2011",
  "Time": "10:30 am - 12:00 pm",
  "Location": "South Pacific 1&2",
  "Items" : ["impact-panel-item"],
  "Chair" : "Leon J. Osterweil"
},
{
  "Title": "SE Research Grants and NSF \"Broader Impacts\": NSF Changes and SE Researcher Strategies",
  "ShortTitle": "SE Research Grants and NSF Broader Impacts",
  "Type": "Panel",
  "ShortType": "Panel",
  "Key": "nsf-strategies-panel",
  "Day": "5-27-2011",
  "Time": "2:00 pm - 3:30 pm",
  "Location": "Coral Ballroom 2",
  "Items" : ["nsf-strategies-panel-item"],
  "Chair" : "Margaret Burnett"
},
{
  "Title": "What Industry Wants from Research",
  "ShortTitle": "What Industry Wants from Research",
  "Type": "Panel",
  "ShortType": "Panel",
  "Key": "what-industry-wants-research",
  "Day": "5-26-2011",
  "Time": "10:45 am - 12:15 pm",
  "Location": "South Pacific 1&2",
  "Items" : ["what-industry-wants-item"],
  "Chairs" : [ "Jorge Aranda", "Daniela Damian", "Marian Petre", "Margaret-Anne Storey", "Greg Wilson" ]
},
{
  "Title": "Luau Banquet",
  "ShortTitle": "Luau Banquet",
  "Type": "Social Event",
  "ShortType": "Social Event",
  "Key": "lua-banquet",
  "Day": "5-25-2011",
  "Time": "6:00 pm - 9:00 pm",
  "Location": "Hale Koa Hotel",
  "Abstract" : "Come celebrate ICSE 2011 at the luau banquet on Wednesday evening! The banquet will feature a traditional luau dinner and festive Polynesian entertainment provided by Tihati. The luau will be held outdoors on the grounds of the Hale Koa Hotel, next door to the conference hotel."
},
{
  "Title": "Student and ICSE Supporters Mixer",
  "ShortTitle": "Student and ICSE Supporters Mixer",
  "Type": "Social Event",
  "ShortType": "Social Event",
  "Key": "student-mixer",
  "Day": "5-26-2011",
  "Time": "6:00 pm - 6:30 pm",
  "Location": "Lagoon Green/Great Lawn"
},
{
  "Title": "ICSE Reception",
  "ShortTitle": "ICSE Reception",
  "Type": "Social Event",
  "ShortType": "Social Event",
  "Key": "icse-reception",
  "Day": "5-26-2011",
  "Time": "6:30 pm - 7:30 pm",
  "Location": "Lagoon Green/Great Lawn"
},
{
  "Title": "Awards I",
  "ShortTitle": "Awards I",
  "Type": "Award",
  "ShortType": "Award",
  "Key": "awards-i",
  "Day": "5-26-2011",
  "Time": "4:00 pm - 5:30 pm",
  "Location": "Coral Ballroom 4&5",
  "Abstract": "Distinguished Paper Awards\n(presented by ICSE 2011 PC Chairs: Harald Gall and Nenad Medvidović)\n\nMost Influential Paper of ICSE 2001\n(presented by MIP Award Chair: Dewayne Perry)\n\nNew IEEE Fellows\nIEEE TCSE Outstanding Service Award\nIEEE TCSE Outstanding Educator Award\n(presented by IEEE TCSE Chair: Hausi A. Müller)",
  "Chairs": [ "Harald Gall", "Nenad Medvidović", "Dewayne Perry", "Hausi A. Müller" ]
},
{
  "Title": "Awards II",
  "ShortTitle": "Awards II",
  "Type": "Award",
  "ShortType": "Award",
  "Key": "awards-ii",
  "Day": "5-27-2011",
  "Time": "4:00 pm - 4:45 pm",
  "Location": "Coral Ballroom 4&5",
  "Chairs": [ "David Rosenblum", "Hausi A. Müller", "Matteo Rossi", "Michal Young", "Thomas Zimmermann" ] , 
  "Abstract" : "ACM Student Research Competition Awards\n(presented by SRC Chair: Tom Zimmermann)\n\nStudent Contest on Software Engineering Awards\n(presented by SCORE Co-Chairs: Matteo Rossi and Michal Young)\n\nIEEE Computer Society Harlan D. Mills Award\n(presented by IEEE TCSE Chair: Hausi A. Müller)\n\nNew ACM Fellows, Distinguished ACM Members\nRecap of 2010 Impact Paper Awards\nACM SIGBED/SIGSOFT Frank Anger Memorial Award\nSIGSOFT Distinguished Service Award\nSIGSOFT Influential Educator Award\nSIGSOFT Outstanding Research Award\n(presented by ACM SIGSOFT Chair: David Rosenblum)"
},


{
  "Title": "Posters: Doctoral Symposium",
  "ShortTitle": "Posters: Doctoral Symposium",
  "Type": "Posters",
  "ShortType": "Posters",
  "Key": "posters-doctoral-symposium",
  "Day": "5-25-2011",
  "Time": "10:00 am - 10:45 am",
  "Location": "Coral Lounge",
  "Items" : ["pragmatic-reuse-web-application-development","reuse-vs-maintainability-revealing-impact-composition-code-properties","directed-test-suite-augmentation","specification-mining-concurrent-and-distributed-systems","gate-game-based-testing-environment","pragmatic-prioritization-software-quality-assurance-efforts","detecting-architecturally-relevant-code-smells-evolving-software-systems","1x-way-architecture-implementation-mapping","reengineering-legacy-software-products-software-product-line-based-automatic-variability-ana","inconsistency-management-framework-model-based-development","exploring-exposing-and-exploiting-emails-include-human-factors-software-engineering","mental-models-and-parallel-program-maintenance","using-software-evolution-history-facilitate-development-and-maintenance","searching-selecting-and-synthesizing-source-code","tracing-architecturally-significant-requirements-decision-centric-approach","predictable-dynamic-deployment-components-embedded-systems","declarative-approach-enable-flexible-and-dynamic-service-compositions","framework-integration-user-centered-design-and-agile-software-development-processes","improving-open-source-software-patch-contribution-process-methods-and-tools","systematizing-security-test-case-planning-using-functional-requirements-phrases","mining-software-repositories-using-topic-models"]
},
{
  "Title": "Posters: SCORE",
  "ShortTitle": "Posters: SCORE",
  "Type": "Posters",
  "ShortType": "Posters",
  "Key": "posters-score",
  "Day": "5-25-2011",
  "Time": "3:15 pm - 4:00 pm",
  "Location": "Coral Lounge"
},
{
  "Title": "Posters: Student Research Competition",
  "ShortTitle": "Posters: SRC",
  "Type": "Posters",
  "ShortType": "Posters",
  "Key": "posters-src",
  "Day": "5-25-2011",
  "Time": "3:15 pm - 4:00 pm",
  "Location": "Coral Lounge",
  "Items": [ "test-blueprint-effective-visual-support-test-coverage", "formal-approach-software-synthesis-architectural-platforms", "detecting-cross-browser-issues-web-applications", "measuring-subversions-security-and-legal-risk-reused-software-artifacts", "building-domain-specific-software-architectures-software-architectural-design-patterns", "using-impact-analysis-industry", "decision-support-system-classification-software-coding-faults-research-abstract", "specification-mining-concurrent-and-distributed-systems-0", "case-study-refactoring-haskell-programs", "build-system-maintenance", "finding-relevant-functions-millions-lines-code", "requirements-tracing-discovering-related-documents-through-artificial-pheromones-and-term-pr", "end-user-demonstration-approach-support-aspect-oriented-modeling", "problem-identification-structural-test-generation-first-step-towards-cooperative-developer-t", "palus-hybrid-automated-test-generation-tool-java", "scalable-automatic-linearizability-checking" ]
},
{
  "Title": "Student Contest on Software Engineering (SCORE) Demos",
  "ShortTitle": "SCORE Demos",
  "Type": "Student Competition",
  "ShortType": "Student Competition",
  "Key": "score",
  "Day": "5-26-2011",
  "Time": "12:15 pm - 1:45 pm",
  "Abstract": "The Student Contest on Software Engineering (SCORE) is a worldwide competition for student teams at the undergraduate and master’s level.  Student teams participating in the contest are able to choose from a number of project topics proposed by the SCORE Program Committee, which cover diverse application fields and types."
},
{
  "Title": "ACM Student Research Competition (SRC) Presentations",
  "ShortTitle": "ACM SRC Presentations",
  "Type": "Student Competition",
  "ShortType": "Student Competition",
  "Location": "South Pacific 3&4",
  "Key": "src-presentations",
  "Day": "5-26-2011",
  "Time": "1:45 pm - 3:15 pm",
  "Abstract": "ICSE 2011 will be hosting an ACM Student Research Competition (SRC), sponsored by Microsoft Research. This competition offers undergraduate and graduate students a unique forum to experience the research world, present their research results to conference attendees, and compete for prizes.  The third round of the competition will be held at the conference. Students will present their research to a panel of judges to determine who will go on to compete for the grand prize."
},
{
  "Title": "Investing in Software Engineering: A View from ICSE's Supporters",
  "ShortTitle": "Supporter Talks",
  "Type": "Other",
  "ShortType": "Other",
  "Key": "supporter-talks",
  "Day": "5-25-2011",
  "Time": "4:00 pm - 4:45 pm",
  "Location": "South Pacific 3&4",
  "Items" : ["infosys-supporter","microsoft-supporter","google-supporter"],
  "Chairs": ["Richard N. Taylor"]
}

	],
	"People": [
{
    "Name": "A. Azadmanesh"
},
{
    "Name": "Aaron Peeler"
},
{
    "Name": "Aaron Schram"
},
{
    "Name": "Aaron Wolfson"
},
{
    "Name": "Aarti Gupta"
},
{
    "Name": "Abayomi King"
},
{
    "Name": "Abbas Tahir"
},
{
    "Name": "Abdelwahab Hamou-Lhadj"
},
{
    "Name": "Abhik Roychoudhury"
},
{
    "Name": "Abram Hindle"
},
{
    "Name": "Adam Czauderna"
},
{
    "Name": "Adam Steele"
},
{
    "Name": "Adrian Jung"
},
{
    "Name": "Adrian Kuhn"
},
{
    "Name": "Adrian Schröter"
},
{
    "Name": "Aharon Abadi"
},
{
    "Name": "Ahmed E. Hassan"
},
{
    "Name": "Ahmed Tamrawi"
},
{
    "Name": "Ajit Singh"
},
{
    "Name": "Alain Mouttham"
},
{
    "Name": "Alan Fekete"
},
{
    "Name": "Alberto Bacchelli"
},
{
    "Name": "Alberto Sillitti"
},
{
    "Name": "Alejandro Russo"
},
{
    "Name": "Aleksandar Milicevic"
},
{
    "Name": "Alessandro Bozzon"
},
{
    "Name": "Alessandro Cimatti"
},
{
    "Name": "Alessandro Fantechi"
},
{
    "Name": "Alessandro Garcia"
},
{
    "Name": "Alessandro Gurgel"
},
{
    "Name": "Alessandro Marchetto"
},
{
    "Name": "Alessandro Orso"
},
{
    "Name": "Alex Dekhtyar"
},
{
    "Name": "Alex Teterev"
},
{
    "Name": "Alexander Boden"
},
{
    "Name": "Alexander Chatzigeorgiou"
},
{
    "Name": "Alexander Egyed"
},
{
    "Name": "Alexander Grebhahn"
},
{
    "Name": "Alexander Helleboogh"
},
{
    "Name": "Alexander Mera"
},
{
    "Name": "Alexander Nöhrer"
},
{
    "Name": "Alexander Reder"
},
{
    "Name": "Alexander Repenning"
},
{
    "Name": "Alexander Serebrenik"
},
{
    "Name": "Alexander W. J. Bradley"
},
{
    "Name": "Alexander Wise"
},
{
    "Name": "Alexander Wolf"
},
{
    "Name": "Alexander Zeier"
},
{
    "Name": "Alfredo Goldman"
},
{
    "Name": "Ali Mesbah"
},
{
    "Name": "Allen Nikora"
},
{
    "Name": "Allen R. Hanson"
},
{
    "Name": "Amal El Fallah Seghrouchni"
},
{
    "Name": "Americo Sampaio"
},
{
    "Name": "Amir Farrahi"
},
{
    "Name": "Amir Malik"
},
{
    "Name": "Ana Petričić"
},
{
    "Name": "Anas Mahmoud"
},
{
    "Name": "Anders Møller"
},
{
    "Name": "Andre L. M. Santos"
},
{
    "Name": "André R. G. do A. Leitão"
},
{
    "Name": "André Riboira"
},
{
    "Name": "André van der Hoek"
},
{
    "Name": "Andrea Arcuri"
},
{
    "Name": "Andrea De Lucia"
},
{
    "Name": "Andrea Micheli"
},
{
    "Name": "Andrea Nickel"
},
{
    "Name": "Andrea Segni"
},
{
    "Name": "Andrea Zisman"
},
{
    "Name": "Andreas Classen"
},
{
    "Name": "Andreas Foltinek"
},
{
    "Name": "Andreas Loukas"
},
{
    "Name": "Andreas Pleuss"
},
{
    "Name": "Andreas Sewe"
},
{
    "Name": "Andreas Ulrich"
},
{
    "Name": "Andreas Wicht"
},
{
    "Name": "Andreas Zeller"
},
{
    "Name": "Andrei Solomon"
},
{
    "Name": "Andrei Ştefănescu"
},
{
    "Name": "Andrejs Jermakovics"
},
{
    "Name": "Andrew Begel"
},
{
    "Name": "Andrew Brownsword"
},
{
    "Name": "Andrew F. Hart"
},
{
    "Name": "Andrew J. Ko"
},
{
    "Name": "Andrew Kerr"
},
{
    "Name": "Andrew King"
},
{
    "Name": "Andrew McVeigh"
},
{
    "Name": "Andrew Meneely"
},
{
    "Name": "Andrew W. Bingham"
},
{
    "Name": "Andrian Marcus"
},
{
    "Name": "Andriy V. Miranskyy"
},
{
    "Name": "Andrzej Wąsowski"
},
{
    "Name": "Andy Smith"
},
{
    "Name": "Andy Zaidman"
},
{
    "Name": "Angelo Susi"
},
{
    "Name": "Anh Nguyen-Tuong"
},
{
    "Name": "Anita Sarma"
},
{
    "Name": "Anna Liu"
},
{
    "Name": "Anne Koziolek"
},
{
    "Name": "Anthony Finkelstein"
},
{
    "Name": "Anton Morant"
},
{
    "Name": "Antonella Santone"
},
{
    "Name": "Antonia Bertolino"
},
{
    "Name": "Antonio Cavalcanti"
},
{
    "Name": "Antonio Filieri"
},
{
    "Name": "Antony Tang"
},
{
    "Name": "Apostolos V. Zarras"
},
{
    "Name": "Ariadi Nugroho"
},
{
    "Name": "Arie van Deursen"
},
{
    "Name": "Ariel S. Rabkin"
},
{
    "Name": "Armijn Hemel"
},
{
    "Name": "Armin Eberlein"
},
{
    "Name": "Artur D'Avila Garcez"
},
{
    "Name": "Ashok Pon Kumar"
},
{
    "Name": "Audris Mockus"
},
{
    "Name": "Aviad Zlotnick"
},
{
    "Name": "Avinash Kak"
},
{
    "Name": "Axel Legay"
},
{
    "Name": "Ayşe Başar Bener"
},
{
    "Name": "Ayse Tosun Mısırlı"
},
{
    "Name": "Bach Bui"
},
{
    "Name": "Baiqiang Chen"
},
{
    "Name": "Bamshad Mobasher"
},
{
    "Name": "Ban Al-Ani"
},
{
    "Name": "Barbara Paech"
},
{
    "Name": "Baris Aktemur"
},
{
    "Name": "Barry Boehm"
},
{
    "Name": "Barton Satchwill"
},
{
    "Name": "Bartosz Michalik"
},
{
    "Name": "Bashar Nuseibeh"
},
{
    "Name": "Bastian Schlich"
},
{
    "Name": "Ben Smith"
},
{
    "Name": "Benjamin Biegel"
},
{
    "Name": "Benjamin Hummel"
},
{
    "Name": "Benny Shimony"
},
{
    "Name": "Benoit Baudry"
},
{
    "Name": "Bernd Fischer"
},
{
    "Name": "Bertrand Meyer"
},
{
    "Name": "Bhuricha Sethanandha"
},
{
    "Name": "Bikram Sengupta"
},
{
    "Name": "Bill Curtis"
},
{
    "Name": "Bill Tomlinson"
},
{
    "Name": "Billy Kidwell"
},
{
    "Name": "Bin Chen"
},
{
    "Name": "Biplav Srivastava"
},
{
    "Name": "Bo Ma"
},
{
    "Name": "Bogdan Dit"
},
{
    "Name": "Bogdan Vasilescu"
},
{
    "Name": "Bojana Bislimovska"
},
{
    "Name": "Bonita Sharif"
},
{
    "Name": "Bonnie John"
},
{
    "Name": "Bora Çağlayan"
},
{
    "Name": "Borislava I. Simidchieva"
},
{
    "Name": "Bradley Blankenship"
},
{
    "Name": "Bradley Schmerl"
},
{
    "Name": "Bram Adams"
},
{
    "Name": "Brandon Heller"
},
{
    "Name": "Brendan Murphy"
},
{
    "Name": "Brett Daniel"
},
{
    "Name": "Brian Berenbach"
},
{
    "Name": "Brian Mastropietro"
},
{
    "Name": "Brian Robinson"
},
{
    "Name": "Bruno C. da Silva"
},
{
    "Name": "Brygg Ullmer"
},
{
    "Name": "C. Lee Giles"
},
{
    "Name": "C. Shaun Longstreet"
},
{
    "Name": "Caitlin Sadowski"
},
{
    "Name": "Cal Swart"
},
{
    "Name": "Cameron E. Goodale"
},
{
    "Name": "Carlo A. Furia"
},
{
    "Name": "Carlo Ghezzi"
},
{
    "Name": "Carlos Bilich"
},
{
    "Name": "Carlos Castro-Herrera"
},
{
    "Name": "Carmine Sementa"
},
{
    "Name": "Carolyn B. Seaman"
},
{
    "Name": "Carolyn Mair"
},
{
    "Name": "Casper Lassenius"
},
{
    "Name": "Cédric Jeanneret"
},
{
    "Name": "Celina Gibbs"
},
{
    "Name": "Cesare Pautasso"
},
{
    "Name": "Chanchal K. Roy"
},
{
    "Name": "Chandrika Sivaramakrishnan"
},
{
    "Name": "Changhai Nie"
},
{
    "Name": "Chao Wang"
},
{
    "Name": "Charles Consel"
},
{
    "Name": "Charles Zhang"
},
{
    "Name": "Charlie Tran"
},
{
    "Name": "Chen Fu"
},
{
    "Name": "Cheng Thao"
},
{
    "Name": "Chengyu Song"
},
{
    "Name": "Chenyang Lu"
},
{
    "Name": "Chien-Liang Fok"
},
{
    "Name": "Choonghwan Lee"
},
{
    "Name": "Chris A. Mattmann"
},
{
    "Name": "Chris Branton"
},
{
    "Name": "Chris Lewis"
},
{
    "Name": "Chris Parnin"
},
{
    "Name": "Chris Wilcox"
},
{
    "Name": "Christelle Scharff"
},
{
    "Name": "Christian Bird"
},
{
    "Name": "Christian Engwer"
},
{
    "Name": "Christian Kästner"
},
{
    "Name": "Christian Murphy"
},
{
    "Name": "Christian Scherling"
},
{
    "Name": "Christina Chavez"
},
{
    "Name": "Christine Julien"
},
{
    "Name": "Christine Miller"
},
{
    "Name": "Christoph Treude"
},
{
    "Name": "Christoph W. Kessler"
},
{
    "Name": "Christopher Bull"
},
{
    "Name": "Christopher Dragert"
},
{
    "Name": "Christopher Forbes"
},
{
    "Name": "Christopher Imbriano"
},
{
    "Name": "Christopher J. Hughes"
},
{
    "Name": "Christopher S. Corley"
},
{
    "Name": "Claire Ingram"
},
{
    "Name": "Clark Verbrugge"
},
{
    "Name": "Cláudio Sant'Anna"
},
{
    "Name": "Clauirton Siebra"
},
{
    "Name": "Clay Williams"
},
{
    "Name": "Cleidson de Souza"
},
{
    "Name": "Cleviton V. F. Monteiro"
},
{
    "Name": "Colin Atkinson"
},
{
    "Name": "Collin McMillan"
},
{
    "Name": "Constantin Sârbu"
},
{
    "Name": "Corina S. Păsăreanu"
},
{
    "Name": "Cornel Barna"
},
{
    "Name": "Craig E. Kuziemsky"
},
{
    "Name": "Cristian Cadar"
},
{
    "Name": "Cristina Marconcini"
},
{
    "Name": "Cuixiong Hu"
},
{
    "Name": "Da Young Lee"
},
{
    "Name": "Dacio N. M. Neto"
},
{
    "Name": "Damian W. I. Rouson"
},
{
    "Name": "Damiano Distante"
},
{
    "Name": "Damien Cassou"
},
{
    "Name": "Damion Mitchell"
},
{
    "Name": "Dan Crichton"
},
{
    "Name": "Dan Houston"
},
{
    "Name": "Dan Port"
},
{
    "Name": "Dan Tofan"
},
{
    "Name": "Dan Zhao"
},
{
    "Name": "Dana Freeborn"
},
{
    "Name": "Daniel Gorin"
},
{
    "Name": "Daniel Hoffman"
},
{
    "Name": "Daniel Jackson"
},
{
    "Name": "Daniel Kern"
},
{
    "Name": "Daniel M. German"
},
{
    "Name": "Daniel M. Zimmerman"
},
{
    "Name": "Daniel Schall"
},
{
    "Name": "Daniela Damian"
},
{
    "Name": "Daniela Steidl"
},
{
    "Name": "Danny Dig"
},
{
    "Name": "Danny Weyns"
},
{
    "Name": "Darko Marinov"
},
{
    "Name": "Darpan Saini"
},
{
    "Name": "Daryl Posnett"
},
{
    "Name": "David Ameller"
},
{
    "Name": "David Binkley"
},
{
    "Name": "David Bowes"
},
{
    "Name": "David Chodos"
},
{
    "Name": "David Clark"
},
{
    "Name": "David Cok"
},
{
    "Name": "David Cuddeback"
},
{
    "Name": "David F. Redmiles"
},
{
    "Name": "David Garlan"
},
{
    "Name": "David Hyde"
},
{
    "Name": "David Kawrykow"
},
{
    "Name": "David Lo"
},
{
    "Name": "David M. Weiss"
},
{
    "Name": "David McComas"
},
{
    "Name": "David Melski"
},
{
    "Name": "David Notkin"
},
{
    "Name": "David Röthlisberger"
},
{
    "Name": "David Socha"
},
{
    "Name": "David T. Cuddy"
},
{
    "Name": "Dawn Lawrie"
},
{
    "Name": "Dean Pucsek"
},
{
    "Name": "Debarshi Chatterji"
},
{
    "Name": "Debdoot Mukherjee"
},
{
    "Name": "Debra J. Richardson"
},
{
    "Name": "Dejan Milojicic"
},
{
    "Name": "Dejana Bajic"
},
{
    "Name": "Denis Gopan"
},
{
    "Name": "Dennis Pagano"
},
{
    "Name": "Denys Poshyvanyk"
},
{
    "Name": "Derek Rayside"
},
{
    "Name": "Detlef Schoder"
},
{
    "Name": "Dharmalingam Ganesan"
},
{
    "Name": "Diane Kelly"
},
{
    "Name": "Dianxiang Xu"
},
{
    "Name": "Didier Donsez"
},
{
    "Name": "Diego Garbervetsky"
},
{
    "Name": "Diego Salomone Bruno"
},
{
    "Name": "Dietmar Pfahl"
},
{
    "Name": "Dimitra Giannakopoulou"
},
{
    "Name": "Dimka Karastoyanova"
},
{
    "Name": "Dina Salah"
},
{
    "Name": "Dinghao Wu"
},
{
    "Name": "Dionysis Athanasopoulos"
},
{
    "Name": "Dirk Beyer"
},
{
    "Name": "Domenico Bianculli"
},
{
    "Name": "Don Batory"
},
{
    "Name": "Dong Li"
},
{
    "Name": "Dongmei Zhang"
},
{
    "Name": "Dongxiang Cai"
},
{
    "Name": "Donna Kaminskyj Long"
},
{
    "Name": "Doris Carver"
},
{
    "Name": "Dorothea Blostein"
},
{
    "Name": "Douglas Martin"
},
{
    "Name": "Duane Waliser"
},
{
    "Name": "Earl T. Barr"
},
{
    "Name": "Ed Keenan"
},
{
    "Name": "Edgard Marx"
},
{
    "Name": "Edgardo Zoppi"
},
{
    "Name": "Eduardo Figueiredo"
},
{
    "Name": "Eduardo Tovar"
},
{
    "Name": "Eelco Dolstra"
},
{
    "Name": "Eilwoo Baik"
},
{
    "Name": "Elad Fein"
},
{
    "Name": "Eleni Stroulia"
},
{
    "Name": "Eli Marschner"
},
{
    "Name": "Elisa S. F. Cardozo"
},
{
    "Name": "Elisabetta Di Nitto"
},
{
    "Name": "Elmar Juergens"
},
{
    "Name": "Emad Shihab"
},
{
    "Name": "Emanuel Giger"
},
{
    "Name": "Emerson Murphy-Hill"
},
{
    "Name": "Emilie Balland"
},
{
    "Name": "Emily Hill"
},
{
    "Name": "Emily Law"
},
{
    "Name": "Emina Torlak"
},
{
    "Name": "Eric Bodden"
},
{
    "Name": "Eric Helms"
},
{
    "Name": "Eric Schuh"
},
{
    "Name": "Éric Tanter"
},
{
    "Name": "Erik H. Trainer"
},
{
    "Name": "Erika Corona"
},
{
    "Name": "Ethan V. Munson"
},
{
    "Name": "Ettore Merlo"
},
{
    "Name": "Eunjong Choi"
},
{
    "Name": "Eunsuk Kang"
},
{
    "Name": "Evan Rosenfeld"
},
{
    "Name": "Evelyn Duesterwald"
},
{
    "Name": "Everett Toews"
},
{
    "Name": "Evie Powell"
},
{
    "Name": "Ewan Tempero"
},
{
    "Name": "Fabien Dagnat"
},
{
    "Name": "Fabio Kon"
},
{
    "Name": "Fabio Martinelli"
},
{
    "Name": "Fabio Q. B. da Silva"
},
{
    "Name": "Fabrizio Pastore"
},
{
    "Name": "Fahad R. Golra"
},
{
    "Name": "Fang Yu"
},
{
    "Name": "Fausto Spoto"
},
{
    "Name": "Fei Li"
},
{
    "Name": "Felienne Hermans"
},
{
    "Name": "Felipe Farias"
},
{
    "Name": "Felipe M. Besson"
},
{
    "Name": "Felix Bott"
},
{
    "Name": "Feng Chen"
},
{
    "Name": "Ferosh Jacob"
},
{
    "Name": "Filippo Ricca"
},
{
    "Name": "Florian Barth"
},
{
    "Name": "Florian Deissenboeck"
},
{
    "Name": "Forrest Shull"
},
{
    "Name": "Foyzur Rahman"
},
{
    "Name": "Francesca Arcelli Fontana"
},
{
    "Name": "Francisco Dantas"
},
{
    "Name": "Francisco Ortin"
},
{
    "Name": "Franco Fummi"
},
{
    "Name": "Franco Mazzanti"
},
{
    "Name": "Frank Tip"
},
{
    "Name": "Franz Schweiggert"
},
{
    "Name": "Franz Wotawa"
},
{
    "Name": "Fredrik Kjolstad"
},
{
    "Name": "Friedrich Steimann"
},
{
    "Name": "Fritz Stallinger"
},
{
    "Name": "Gabriel A. Moreno"
},
{
    "Name": "Gabriel Acevedo"
},
{
    "Name": "Gabriel L. Zenarosa"
},
{
    "Name": "Gabriel Pedraza"
},
{
    "Name": "Gabriele Bavota"
},
{
    "Name": "Gail C. Murphy"
},
{
    "Name": "Gail Kaiser"
},
{
    "Name": "Gargi Bougie"
},
{
    "Name": "Gary Black"
},
{
    "Name": "Gary Bradshaw"
},
{
    "Name": "Gary Chastek"
},
{
    "Name": "Gary Kaminski"
},
{
    "Name": "George Adams"
},
{
    "Name": "George Chang"
},
{
    "Name": "George Spanoudakis"
},
{
    "Name": "Gerald Bortis"
},
{
    "Name": "Gerald C. Gannod"
},
{
    "Name": "Gerald Kotonya"
},
{
    "Name": "Gerardo Canfora"
},
{
    "Name": "Gian Pietro Picco"
},
{
    "Name": "Giancarlo Succi"
},
{
    "Name": "Giordano Tamburrelli"
},
{
    "Name": "Giriprasad Sridhara"
},
{
    "Name": "Giuliano Antoniol"
},
{
    "Name": "Giulio Barabino"
},
{
    "Name": "Giulio Concas"
},
{
    "Name": "Giuseppe Di Guglielmo"
},
{
    "Name": "Goetz Botterweck"
},
{
    "Name": "Grace A. Lewis"
},
{
    "Name": "Graziano Pravadelli"
},
{
    "Name": "Graziela Tonin"
},
{
    "Name": "Greg Leach"
},
{
    "Name": "Greg Little"
},
{
    "Name": "Greg Wilson"
},
{
    "Name": "Gregory M. Kapfhammer"
},
{
    "Name": "Grigore Roşu"
},
{
    "Name": "Gruia-Catalin Roman"
},
{
    "Name": "Guenther Ruhe"
},
{
    "Name": "Guido de Caso"
},
{
    "Name": "Gunnar Stevens"
},
{
    "Name": "Gunther Heidemann"
},
{
    "Name": "Guru Venkataramani"
},
{
    "Name": "Guy Collins Ndem"
},
{
    "Name": "Ha Duy Trung"
},
{
    "Name": "Hadar Ziv"
},
{
    "Name": "Hadi Sharifi"
},
{
    "Name": "Hakim Sultanov"
},
{
    "Name": "Hamid Abdul Basit"
},
{
    "Name": "Hamid Bagheri"
},
{
    "Name": "Hamman W. Samuel"
},
{
    "Name": "Hamoun Ghanbari"
},
{
    "Name": "Hanna Remmel"
},
{
    "Name": "Hans Christian Benestad"
},
{
    "Name": "Hans Petter Langtangen"
},
{
    "Name": "Hans van Vliet"
},
{
    "Name": "Hans-Joachim Goltz"
},
{
    "Name": "Harald Gall"
},
{
    "Name": "Harold Ossher"
},
{
    "Name": "Harvey Siy"
},
{
    "Name": "Hasso Plattner"
},
{
    "Name": "Hazeline Asuncion"
},
{
    "Name": "Hazlifah Mohd Rusli"
},
{
    "Name": "He Zhang"
},
{
    "Name": "Heejung Kim"
},
{
    "Name": "Heidar Pirzadeh"
},
{
    "Name": "Heiko Koziolek"
},
{
    "Name": "Hela Oueslati"
},
{
    "Name": "Helen Sharp"
},
{
    "Name": "Helmut Goetz"
},
{
    "Name": "Hendrik Salomon"
},
{
    "Name": "Henrik Bærbak Christensen"
},
{
    "Name": "Hidehiko Masuhara"
},
{
    "Name": "Hoan A. Nguyen"
},
{
    "Name": "Holly L. Connor"
},
{
    "Name": "Hongyu Zhang"
},
{
    "Name": "HoonJae Lee"
},
{
    "Name": "Horatiu Dumitru"
},
{
    "Name": "Howard Foster"
},
{
    "Name": "Hung V. Nguyen"
},
{
    "Name": "Huzefa Kagdi"
},
{
    "Name": "Hyun Cho"
},
{
    "Name": "Ian Davis"
},
{
    "Name": "Ian Gorton"
},
{
    "Name": "Ian Simmonds"
},
{
    "Name": "Igor Čavrak"
},
{
    "Name": "Ilaria Matteucci"
},
{
    "Name": "Ilenia Fronza"
},
{
    "Name": "Iman Hemati Moghadam"
},
{
    "Name": "Iman Keivanloo"
},
{
    "Name": "Insup Lee"
},
{
    "Name": "Ioanis Nikolaidis"
},
{
    "Name": "Ioannis Parissis"
},
{
    "Name": "Ipek Ozkaya"
},
{
    "Name": "Irwin Kwan"
},
{
    "Name": "Isabella R. M. dos Santos"
},
{
    "Name": "Isabelle Comyn-Wattiau"
},
{
    "Name": "Isela Macia"
},
{
    "Name": "Israel Gat"
},
{
    "Name": "Iulian Neamtiu"
},
{
    "Name": "Ivana Bosnić"
},
{
    "Name": "Ivana Turnu"
},
{
    "Name": "Ivica Crnković"
},
{
    "Name": "J. Jenny Li"
},
{
    "Name": "Jack W. Davidson"
},
{
    "Name": "Jacky Akoka"
},
{
    "Name": "Jacky Keung"
},
{
    "Name": "Jacquelyn Martino"
},
{
    "Name": "JaeCheol Ha"
},
{
    "Name": "Jafar Al-Kofahi"
},
{
    "Name": "Jairo Aponte"
},
{
    "Name": "Jairus Hihn"
},
{
    "Name": "James B. Williams"
},
{
    "Name": "James Clause"
},
{
    "Name": "James D. Herbsleb"
},
{
    "Name": "James Gibson"
},
{
    "Name": "James Ivers"
},
{
    "Name": "James M. Bieman"
},
{
    "Name": "James R. Cordy"
},
{
    "Name": "James Williams"
},
{
    "Name": "Jamie Payton"
},
{
    "Name": "Jamie Starke"
},
{
    "Name": "Jamshaid G. Mohebzada"
},
{
    "Name": "Jan Harder"
},
{
    "Name": "Jan Jürjens"
},
{
    "Name": "Jan Sinschek"
},
{
    "Name": "Jane Cleland-Huang"
},
{
    "Name": "Jane Huffman Hayes"
},
{
    "Name": "Janet E. Burge"
},
{
    "Name": "Janet Feigenspan"
},
{
    "Name": "Jason D. Hiser"
},
{
    "Name": "Jason Mars"
},
{
    "Name": "Jaumir V. da Silveira"
},
{
    "Name": "Jay Sappidi"
},
{
    "Name": "Jean-Sylvain Bucumi"
},
{
    "Name": "Jeff Gray"
},
{
    "Name": "Jeff Holden"
},
{
    "Name": "Jeff Kramer"
},
{
    "Name": "Jeff Magee"
},
{
    "Name": "Jeff Offutt"
},
{
    "Name": "Jeffrey C. Carver"
},
{
    "Name": "Jeffrey Heer"
},
{
    "Name": "Jeffrey Koch"
},
{
    "Name": "Jelena Vlasenko"
},
{
    "Name": "Jennifer Baldwin"
},
{
    "Name": "Jens H. Weber"
},
{
    "Name": "Jens H. Weber-Jahnke"
},
{
    "Name": "Jens Krinke"
},
{
    "Name": "Jens Palsberg"
},
{
    "Name": "Jeremy Duvall"
},
{
    "Name": "Jeroen van den Bos"
},
{
    "Name": "Jian Zhang"
},
{
    "Name": "Jianguo Wang"
},
{
    "Name": "Jim Whitehead"
},
{
    "Name": "Jim Xia"
},
{
    "Name": "Jing Yang"
},
{
    "Name": "Jinguo Zhou"
},
{
    "Name": "Jinwon Kim"
},
{
    "Name": "Jitendra Subramanyam"
},
{
    "Name": "Jo E. Hannay"
},
{
    "Name": "Jo Van Eyck"
},
{
    "Name": "Joel Ossher"
},
{
    "Name": "Johan Enmyren"
},
{
    "Name": "Johannes Bohnet"
},
{
    "Name": "John C. Georgas"
},
{
    "Name": "John C. Knight"
},
{
    "Name": "John D. Heintz"
},
{
    "Name": "John D. McGregor"
},
{
    "Name": "John Grundy"
},
{
    "Name": "John Hardy"
},
{
    "Name": "John Hopson"
},
{
    "Name": "John Hosking"
},
{
    "Name": "John Hughes"
},
{
    "Name": "John Hutchinson"
},
{
    "Name": "John J. Tran"
},
{
    "Name": "John Marrero"
},
{
    "Name": "John Mylopoulos"
},
{
    "Name": "John Richards"
},
{
    "Name": "John Shillington"
},
{
    "Name": "Johnston Jiaa"
},
{
    "Name": "Jon Eyolfson"
},
{
    "Name": "Jon Gray"
},
{
    "Name": "Jon Whittle"
},
{
    "Name": "Jonah Wall"
},
{
    "Name": "Jonas Helming"
},
{
    "Name": "Jonathan Aldrich"
},
{
    "Name": "Jonathan Bell"
},
{
    "Name": "Jonathan Chow"
},
{
    "Name": "Jonathan Cook"
},
{
    "Name": "Jonathan de Halleux"
},
{
    "Name": "Jonathan Maletic"
},
{
    "Name": "Jonathan Sharp"
},
{
    "Name": "Jonathan Sillito"
},
{
    "Name": "Jonathan Streit"
},
{
    "Name": "Jonathan Tapicer"
},
{
    "Name": "Joost Visser"
},
{
    "Name": "Jörg Kienzle"
},
{
    "Name": "José Abdelnour-Nocera"
},
{
    "Name": "José Viterbo"
},
{
    "Name": "Joseph P. Near"
},
{
    "Name": "Joseph R. Kiniry"
},
{
    "Name": "Joshua Sunshine"
},
{
    "Name": "Josip Maras"
},
{
    "Name": "Jr."
},
{
    "Name": "Judith Bishop"
},
{
    "Name": "Judith Segal"
},
{
    "Name": "Juergen Rilling"
},
{
    "Name": "Juhnyoung Lee"
},
{
    "Name": "Jules White"
},
{
    "Name": "Julia Lawall"
},
{
    "Name": "Julia Rubin"
},
{
    "Name": "Julian Dolby"
},
{
    "Name": "Julian Tschannen"
},
{
    "Name": "Julie S. Fant"
},
{
    "Name": "Julio Cesar Sampaio do Prado Leite"
},
{
    "Name": "Julius Davies"
},
{
    "Name": "Jun Q. Lu"
},
{
    "Name": "Jungju Oh"
},
{
    "Name": "Junhua Ding"
},
{
    "Name": "Junsung Kim"
},
{
    "Name": "Jurand Nogiec"
},
{
    "Name": "Jürgen Dippon"
},
{
    "Name": "Jürgen Döllner"
},
{
    "Name": "K. Vijay-Shanker"
},
{
    "Name": "Kai Fischbach"
},
{
    "Name": "Kamin Whitehouse"
},
{
    "Name": "Karen L. Fisher"
},
{
    "Name": "Karen Reid"
},
{
    "Name": "Karen Schuchardt"
},
{
    "Name": "Karin K. Breitman"
},
{
    "Name": "Karl Beecher"
},
{
    "Name": "Karl Naden"
},
{
    "Name": "Karl Trygve Kalleberg"
},
{
    "Name": "Karla Morris"
},
{
    "Name": "Karthik Lakshmanan"
},
{
    "Name": "Katelyn Doran"
},
{
    "Name": "Katerina Goševa-Popstojanova"
},
{
    "Name": "Kathleen M. Carley"
},
{
    "Name": "Kathryn T. Stolee"
},
{
    "Name": "Katsuhisa Maruyama"
},
{
    "Name": "Katsuro Inoue"
},
{
    "Name": "Kay Römer"
},
{
    "Name": "Kelly Androutsopoulos"
},
{
    "Name": "Kelly Lyons"
},
{
    "Name": "Kelvin Sung"
},
{
    "Name": "Ken Bauer"
},
{
    "Name": "Kendra M. L. Cooper"
},
{
    "Name": "Kenji Tei"
},
{
    "Name": "Kenneth Hullett"
},
{
    "Name": "Kenneth M. Anderson"
},
{
    "Name": "Kevin A. Schneider"
},
{
    "Name": "Kevin Lano"
},
{
    "Name": "Kevin Sullivan"
},
{
    "Name": "Kevin Yoo"
},
{
    "Name": "Kiev Gama"
},
{
    "Name": "KiSeok Bae"
},
{
    "Name": "Kıvanç Muşlu"
},
{
    "Name": "Klaus Krogmann"
},
{
    "Name": "Klaus Lochmann"
},
{
    "Name": "Klaus Marius Hansen"
},
{
    "Name": "Koen Buyens"
},
{
    "Name": "Koen Claessen"
},
{
    "Name": "Koen Langendoen"
},
{
    "Name": "Kon S. Leung"
},
{
    "Name": "Kostadin Damevski"
},
{
    "Name": "Koushik Sen"
},
{
    "Name": "Kristen R. Walcott"
},
{
    "Name": "Kristina Winbladh"
},
{
    "Name": "Kristopher Welsh"
},
{
    "Name": "Krzysztof Czarnecki"
},
{
    "Name": "Krzysztof Piotrowski"
},
{
    "Name": "Kshirasagar Naik"
},
{
    "Name": "Kuat Yessenov"
},
{
    "Name": "Kunal Taneja"
},
{
    "Name": "Kurt Geihs"
},
{
    "Name": "Kurt Schneider"
},
{
    "Name": "Kwankeun Yi"
},
{
    "Name": "Laleh M. Eshkevari"
},
{
    "Name": "Lars Heinemann"
},
{
    "Name": "Laurie Williams"
},
{
    "Name": "Laya Madani"
},
{
    "Name": "Leandro Sales Pinto"
},
{
    "Name": "Leif Singer"
},
{
    "Name": "Len Bass"
},
{
    "Name": "Leon J. Osterweil"
},
{
    "Name": "Leonardo Mariani"
},
{
    "Name": "Letha H. Etzkorn"
},
{
    "Name": "Liam Kiemele"
},
{
    "Name": "Liam O'Brien"
},
{
    "Name": "Liam Peyton"
},
{
    "Name": "Lian Yu"
},
{
    "Name": "Liang Gong"
},
{
    "Name": "Liguo Huang"
},
{
    "Name": "Liming Zhu"
},
{
    "Name": "Lin Tan"
},
{
    "Name": "Lionel C. Briand"
},
{
    "Name": "Lori Clarke"
},
{
    "Name": "Lori Pollock"
},
{
    "Name": "Lorin Hochstein"
},
{
    "Name": "Luca Berardinelli"
},
{
    "Name": "Luca Cinquini"
},
{
    "Name": "Luca Mottola"
},
{
    "Name": "Lucas Cordeiro"
},
{
    "Name": "Lucas Layman"
},
{
    "Name": "Luigi Cerulo"
},
{
    "Name": "Luigi Di Guglielmo"
},
{
    "Name": "Luis C. Lamb"
},
{
    "Name": "Luis Miguel Pinho"
},
{
    "Name": "Luiz Fernando Gomes Soares"
},
{
    "Name": "Lutz Prechelt"
},
{
    "Name": "M. Rahmani"
},
{
    "Name": "M. S. Raunak"
},
{
    "Name": "M. Todd Gamble"
},
{
    "Name": "Madhan Muralimanohar"
},
{
    "Name": "Magnus Thorstein Sletholt"
},
{
    "Name": "Mahmoud Said"
},
{
    "Name": "MahnKi Ahn"
},
{
    "Name": "Malcom Gethers"
},
{
    "Name": "Malte Ressin"
},
{
    "Name": "Mandana Vaziri"
},
{
    "Name": "Manisha Bhandar"
},
{
    "Name": "Mansour Zand"
},
{
    "Name": "Manu Sridharan"
},
{
    "Name": "Manuel Carro"
},
{
    "Name": "Manuel Mohr"
},
{
    "Name": "Marc Snir"
},
{
    "Name": "Marcelo Cataldo"
},
{
    "Name": "Marcin Brzozowski"
},
{
    "Name": "Marcin Nowak"
},
{
    "Name": "Márcio De Oliveira Barros"
},
{
    "Name": "Marcio Ferreira Moreno"
},
{
    "Name": "Marco Brambilla"
},
{
    "Name": "Marco D'Ambros"
},
{
    "Name": "Marco Roveri"
},
{
    "Name": "Marco Torchiano"
},
{
    "Name": "Marek Gibiec"
},
{
    "Name": "Margaret-Anne Storey"
},
{
    "Name": "Maria Paasivaara"
},
{
    "Name": "Marin Litoiu"
},
{
    "Name": "Marin Orlić"
},
{
    "Name": "Mario Gleirscher"
},
{
    "Name": "Mario Luca Bernardi"
},
{
    "Name": "Mario Pukall"
},
{
    "Name": "Mario Žagar"
},
{
    "Name": "Marios Fokaefs"
},
{
    "Name": "Marius Gelhausen"
},
{
    "Name": "Mark Grechanik"
},
{
    "Name": "Mark Hahnenberg"
},
{
    "Name": "Mark Harman"
},
{
    "Name": "Mark Hofberg"
},
{
    "Name": "Mark Moir"
},
{
    "Name": "Mark Rouncefield"
},
{
    "Name": "Mark van den Brand"
},
{
    "Name": "Markus Herrmannsdoerfer"
},
{
    "Name": "Markus Pizka"
},
{
    "Name": "Marsha Chechik"
},
{
    "Name": "Marta Cimitile"
},
{
    "Name": "Martin Glinz"
},
{
    "Name": "Martin Kropp"
},
{
    "Name": "Martin Nordio"
},
{
    "Name": "Martin P. Robillard"
},
{
    "Name": "Martin Pinzger"
},
{
    "Name": "Martin Rouaux"
},
{
    "Name": "Martin Salois"
},
{
    "Name": "Martin Sandrieser"
},
{
    "Name": "Martin Shepperd"
},
{
    "Name": "Martin Treiber"
},
{
    "Name": "Marvin V. Zelkowitz"
},
{
    "Name": "Mary Lou Soffa"
},
{
    "Name": "Mary Shaw"
},
{
    "Name": "Masahiro Fujita"
},
{
    "Name": "Massila Kamalrudin"
},
{
    "Name": "Massimiliano Di Penta"
},
{
    "Name": "Mathias Frisch"
},
{
    "Name": "Mati Shomrat"
},
{
    "Name": "Mats P.E. Heimdahl"
},
{
    "Name": "Matt Dwyer"
},
{
    "Name": "Matt Smith"
},
{
    "Name": "Matt Staats"
},
{
    "Name": "Matthew Callery"
},
{
    "Name": "Matthew Hearn"
},
{
    "Name": "Matthew L. Hale"
},
{
    "Name": "Matthias Galster"
},
{
    "Name": "Matthias Kovatsch"
},
{
    "Name": "Matthias Woehrle"
},
{
    "Name": "Mattia Monga"
},
{
    "Name": "Maurice H. ter Beek"
},
{
    "Name": "Maurício Serrano"
},
{
    "Name": "Mauro Pezzè"
},
{
    "Name": "Mauro Santoro"
},
{
    "Name": "Max Goldman"
},
{
    "Name": "Max Schäfer"
},
{
    "Name": "Maximilian Koegel"
},
{
    "Name": "Mazidah Puteh"
},
{
    "Name": "Mechelle Gittens"
},
{
    "Name": "Mehdi Jazayeri"
},
{
    "Name": "Mehdi Mirakhorli"
},
{
    "Name": "Meiyappan Nagappan"
},
{
    "Name": "Mel Ó Cinnéide"
},
{
    "Name": "Michael A. Heroux"
},
{
    "Name": "Michael Bayne"
},
{
    "Name": "Michael Bigrigg"
},
{
    "Name": "Michael D. Ernst"
},
{
    "Name": "Michael Dalton"
},
{
    "Name": "Michael Desmond"
},
{
    "Name": "Michael E. Gangl"
},
{
    "Name": "Michael J. Lee"
},
{
    "Name": "Michael Madison"
},
{
    "Name": "Michael Philippsen"
},
{
    "Name": "Michael Stengel"
},
{
    "Name": "Michael W. Godfrey"
},
{
    "Name": "Michael W. Whalen"
},
{
    "Name": "Michał H. Pałka"
},
{
    "Name": "Michal Young"
},
{
    "Name": "Michel Dapiran"
},
{
    "Name": "Michele Co"
},
{
    "Name": "Michele L. Marchesi"
},
{
    "Name": "Michele Lanza"
},
{
    "Name": "Michele Shaw"
},
{
    "Name": "Michelle Craig"
},
{
    "Name": "Michelle Mills Strout"
},
{
    "Name": "Miguel J. A. Pernambuco Filho"
},
{
    "Name": "Mikael Lindvall"
},
{
    "Name": "Mila Keren"
},
{
    "Name": "Milos Prvulovic"
},
{
    "Name": "Milton Halem"
},
{
    "Name": "Minghui Zhou"
},
{
    "Name": "Mingyue Jiang"
},
{
    "Name": "Minhaz F. Zibran"
},
{
    "Name": "Mira Mezini"
},
{
    "Name": "Mircea Lungu"
},
{
    "Name": "Mircea Trifu"
},
{
    "Name": "Miryung Kim"
},
{
    "Name": "Mitch Dempsey"
},
{
    "Name": "Mithun Acharya"
},
{
    "Name": "Mladen Vouk"
},
{
    "Name": "Mojtaba Vahidi-Asl"
},
{
    "Name": "Muath Alkhalaf"
},
{
    "Name": "Muhammad Ali Babar"
},
{
    "Name": "Muhammad Asaduzzaman"
},
{
    "Name": "Muhammad Aufeef Chauhan"
},
{
    "Name": "Muhammad Zubair Malik"
},
{
    "Name": "Mukul R. Prasad"
},
{
    "Name": "Murray Cantor"
},
{
    "Name": "Nabil Layaida"
},
{
    "Name": "Nabor Mendonça"
},
{
    "Name": "Nachiappan Nagappan"
},
{
    "Name": "Nadav Steindler"
},
{
    "Name": "Nadine Amsel"
},
{
    "Name": "Nadira Lammari"
},
{
    "Name": "Nan Niu"
},
{
    "Name": "Nanette Brown"
},
{
    "Name": "Narayan Ramasubbu"
},
{
    "Name": "Natalia Razinkov"
},
{
    "Name": "Nate Kube"
},
{
    "Name": "Naved Ahmed"
},
{
    "Name": "Navid Ahmadi"
},
{
    "Name": "Nazareno Aguirre"
},
{
    "Name": "Nazim Madhavji"
},
{
    "Name": "Neeraj Suri"
},
{
    "Name": "Negar Hariri"
},
{
    "Name": "Neil A. Ernst"
},
{
    "Name": "Neil Vachharajani"
},
{
    "Name": "Nelis Boucké"
},
{
    "Name": "Nelly Bencomo"
},
{
    "Name": "Nenad Medvidovic"
},
{
    "Name": "Nguyen Xuan Thang"
},
{
    "Name": "Nicholas A. Kraft"
},
{
    "Name": "Nicholas Sawadsky"
},
{
    "Name": "Nick Matthijssen"
},
{
    "Name": "Nico Zazworka"
},
{
    "Name": "Nicolás D'Ippolito"
},
{
    "Name": "Nicolas Gold"
},
{
    "Name": "Nicolas Lopez"
},
{
    "Name": "Nicolas Mangano"
},
{
    "Name": "Nikolai Tillmann"
},
{
    "Name": "Nikolaos Tsantalis"
},
{
    "Name": "Nikolay Kazmin"
},
{
    "Name": "Nils Göde"
},
{
    "Name": "Ning Chen"
},
{
    "Name": "Nir Piterman"
},
{
    "Name": "Nitesh Narayan"
},
{
    "Name": "Noah M. Jorgenson"
},
{
    "Name": "Noam Behar"
},
{
    "Name": "Nobukazu Yoshioka"
},
{
    "Name": "Norihiro Yoshida"
},
{
    "Name": "Nuzio Ruffolo"
},
{
    "Name": "Ohad Barzilay"
},
{
    "Name": "Oh-Ig Kwoun"
},
{
    "Name": "Oleg Sokolsky"
},
{
    "Name": "Oleksandr Panchenko"
},
{
    "Name": "Olga Baysal"
},
{
    "Name": "Oliver Hummel"
},
{
    "Name": "Oliviero Riganelli"
},
{
    "Name": "Olly Gotel"
},
{
    "Name": "Onn Shehory"
},
{
    "Name": "Oriol Collell"
},
{
    "Name": "Orna Raz"
},
{
    "Name": "Osamu Mizuno"
},
{
    "Name": "Oscar Callaú"
},
{
    "Name": "Osmar R. Zaïane"
},
{
    "Name": "Owen O'Malley"
},
{
    "Name": "Pamela Bhattacharya"
},
{
    "Name": "Pankaj Dhoolia"
},
{
    "Name": "Panos Vassiliadis"
},
{
    "Name": "Paola Inverardi"
},
{
    "Name": "Paola Spoletini"
},
{
    "Name": "Paris Avgeriou"
},
{
    "Name": "Patrice Godefroid"
},
{
    "Name": "Patricia Lago"
},
{
    "Name": "Patrick Donohoe"
},
{
    "Name": "Patrick Heymans"
},
{
    "Name": "Patrick Lam"
},
{
    "Name": "Patrick Maeder"
},
{
    "Name": "Patrick Wagstrom"
},
{
    "Name": "Patrizia Asirelli"
},
{
    "Name": "Paul A. Zimdars"
},
{
    "Name": "Paul Ammann"
},
{
    "Name": "Paul Luo Li"
},
{
    "Name": "Paul Matchen"
},
{
    "Name": "Paulo José Azevedo Vianna Ferreira"
},
{
    "Name": "Pawel Gburzynski"
},
{
    "Name": "Pedro M. B. Leal"
},
{
    "Name": "Pekka Abrahamsson"
},
{
    "Name": "Peng Liu"
},
{
    "Name": "Percy E. Salas"
},
{
    "Name": "Peri Tarr"
},
{
    "Name": "Pete Rotella"
},
{
    "Name": "Peter Bastian"
},
{
    "Name": "Peter C Rigby"
},
{
    "Name": "Peter Fritz"
},
{
    "Name": "Peter Gloor"
},
{
    "Name": "Peter Johnson"
},
{
    "Name": "Peter Langendoerfer"
},
{
    "Name": "Peter Lean"
},
{
    "Name": "Peter Mülders"
},
{
    "Name": "Peter Sawyer"
},
{
    "Name": "Philipp Schugerl"
},
{
    "Name": "Philippe Charland"
},
{
    "Name": "Philippe Kruchten"
},
{
    "Name": "Phuong Nguyen"
},
{
    "Name": "Piero Fraternali"
},
{
    "Name": "Pierre F. Tiako"
},
{
    "Name": "Pierre Genevès"
},
{
    "Name": "Pierre-Yves Schobbens"
},
{
    "Name": "Pietro Mazzoleni"
},
{
    "Name": "Prasenjit Mitra"
},
{
    "Name": "Prem Devanbu"
},
{
    "Name": "Premkumar Devanbu"
},
{
    "Name": "Qing Xie"
},
{
    "Name": "Quinten David Soetens"
},
{
    "Name": "Quirino Zagarese"
},
{
    "Name": "Rachel Bellamy"
},
{
    "Name": "Rafael Lotufo"
},
{
    "Name": "Rafael Marques"
},
{
    "Name": "Rafael Prikladnicki"
},
{
    "Name": "Rafael Savignon Marinho"
},
{
    "Name": "Rafael V. Borges"
},
{
    "Name": "Raffaela Mirandola"
},
{
    "Name": "Raffaella Folgieri"
},
{
    "Name": "Ragunathan Rajkumar"
},
{
    "Name": "Raimund Dachselt"
},
{
    "Name": "Raimundas Matulevičius"
},
{
    "Name": "Rainer Koschke"
},
{
    "Name": "Rainer Lutz"
},
{
    "Name": "Rajesh Krishna Balan"
},
{
    "Name": "Rajesh Palit"
},
{
    "Name": "Rami Bahsoon"
},
{
    "Name": "Randy Katz"
},
{
    "Name": "Rashina Hoda"
},
{
    "Name": "Rastislav Bodik"
},
{
    "Name": "Ray Huang"
},
{
    "Name": "Rebeka Gomes"
},
{
    "Name": "Regis J. Leonard"
},
{
    "Name": "Reid Holmes"
},
{
    "Name": "Reimar Schröter"
},
{
    "Name": "René Just"
},
{
    "Name": "Renuka Arya"
},
{
    "Name": "Ricardo Contreras"
},
{
    "Name": "Ricardo Erikson V. S. Rosa"
},
{
    "Name": "Ricardo Valerdi"
},
{
    "Name": "Riccardo Scandariato"
},
{
    "Name": "Richard C. Holt"
},
{
    "Name": "Richard Cook"
},
{
    "Name": "Richard Goodwin"
},
{
    "Name": "Richard Kim"
},
{
    "Name": "Richard Wettel"
},
{
    "Name": "Rick Kazman"
},
{
    "Name": "Ripon K. Saha"
},
{
    "Name": "Rob Vermaas"
},
{
    "Name": "Rob Walker"
},
{
    "Name": "Robert C. Miller"
},
{
    "Name": "Robert Delmonico"
},
{
    "Name": "Robert Fuhrer"
},
{
    "Name": "Robert Goeritzer"
},
{
    "Name": "Robert Hilbrich"
},
{
    "Name": "Robert Hundt"
},
{
    "Name": "Robert J. Hall"
},
{
    "Name": "Robert L. Nord"
},
{
    "Name": "Robert Neumann"
},
{
    "Name": "Robert Rankin"
},
{
    "Name": "Robert Schossleitner"
},
{
    "Name": "Robert Tairas"
},
{
    "Name": "Roberta Arcoverde"
},
{
    "Name": "Roberto Cavada"
},
{
    "Name": "Roberto De Lorenzi"
},
{
    "Name": "Roberto Tonelli"
},
{
    "Name": "Robin Gandhi"
},
{
    "Name": "Robyn R. Lutz"
},
{
    "Name": "Rocco Oliveto"
},
{
    "Name": "Rodrick Borg"
},
{
    "Name": "Roger Wolff"
},
{
    "Name": "Rohit Goel"
},
{
    "Name": "Roland Weiss"
},
{
    "Name": "Romain Robbes"
},
{
    "Name": "Ronald Garcia"
},
{
    "Name": "Rongxin Wu"
},
{
    "Name": "Rosalva E. Gallardo-Valencia"
},
{
    "Name": "Roscoe Bartlett"
},
{
    "Name": "Rose F. Gamble"
},
{
    "Name": "Ross Jeffery"
},
{
    "Name": "Rui Abreu"
},
{
    "Name": "Rui Rodrigues"
},
{
    "Name": "Rumyana Proynova"
},
{
    "Name": "Ryan Kivett"
},
{
    "Name": "Ryo Shimizu"
},
{
    "Name": "Sabri Pllana"
},
{
    "Name": "Saeed Parsa"
},
{
    "Name": "Sai Zhang"
},
{
    "Name": "Sam Guinea"
},
{
    "Name": "Sam Malek"
},
{
    "Name": "Samuel Klock"
},
{
    "Name": "Sandeep Krishnan"
},
{
    "Name": "Sandeep Kumar"
},
{
    "Name": "Sandra Kogan"
},
{
    "Name": "SangJae Moon"
},
{
    "Name": "Sanjian Chen"
},
{
    "Name": "Sanjin Sehic"
},
{
    "Name": "Sarfraz Khurshid"
},
{
    "Name": "Sateesh Kannegala"
},
{
    "Name": "Satish Chandra"
},
{
    "Name": "Saurabh Sinha"
},
{
    "Name": "Sayed Gholam Hassan Tabatabaei"
},
{
    "Name": "Schahram Dustdar"
},
{
    "Name": "Scott Berfield"
},
{
    "Name": "Sebastian Draxler"
},
{
    "Name": "Sebastian Elbaum"
},
{
    "Name": "Sebastian Götz"
},
{
    "Name": "Sebastian Klenk"
},
{
    "Name": "Sebastián Uchitel"
},
{
    "Name": "Segev Wasserkrug"
},
{
    "Name": "Sencun Zhu"
},
{
    "Name": "Senthil Mani"
},
{
    "Name": "Seok-Won Lee"
},
{
    "Name": "Serge Demeyer"
},
{
    "Name": "Sergey Zeltyn"
},
{
    "Name": "Shahed Zaman"
},
{
    "Name": "Shan Malhotra"
},
{
    "Name": "Shane McIntosh"
},
{
    "Name": "Shao Jie Zhang"
},
{
    "Name": "Shaon Barman"
},
{
    "Name": "Shaoying Liu"
},
{
    "Name": "Sharla King"
},
{
    "Name": "Shaun Phillips"
},
{
    "Name": "Shauvik Roy Choudhary"
},
{
    "Name": "Shay Artzi"
},
{
    "Name": "Shehnila Zardari"
},
{
    "Name": "Sheng Liu"
},
{
    "Name": "Shigetoshi Yokoyama"
},
{
    "Name": "Shin Hwei Tan"
},
{
    "Name": "Shin Nakajima"
},
{
    "Name": "Shingo Takada"
},
{
    "Name": "Shinichi Honiden"
},
{
    "Name": "Shinji Kusumoto"
},
{
    "Name": "Shivani Rao"
},
{
    "Name": "Shlomit Shachor"
},
{
    "Name": "Shmuel Ur"
},
{
    "Name": "Shyh-Kwei Chen"
},
{
    "Name": "Siau Cheng Khoo"
},
{
    "Name": "Siegfried Benkner"
},
{
    "Name": "Signe White"
},
{
    "Name": "Siim Karus"
},
{
    "Name": "Simon Holm Jensen"
},
{
    "Name": "Sonali Pagade"
},
{
    "Name": "Song Ge"
},
{
    "Name": "Soo Ling Lim"
},
{
    "Name": "Sophia Krasikov"
},
{
    "Name": "Stacy K. Lukins"
},
{
    "Name": "Stan Jarzabek"
},
{
    "Name": "Stanislaw Jarzabek"
},
{
    "Name": "Stefan Gruner"
},
{
    "Name": "Stefan Wagner"
},
{
    "Name": "Stefan Winter"
},
{
    "Name": "Stefania Gnesi"
},
{
    "Name": "Stefano Pace"
},
{
    "Name": "Stefano Spinelli"
},
{
    "Name": "Stefano Tonetta"
},
{
    "Name": "Stefanos Zachariadis"
},
{
    "Name": "Steffen Becker"
},
{
    "Name": "Steinar Kristoffersen"
},
{
    "Name": "Stephan Diehl"
},
{
    "Name": "Stephan Kriener"
},
{
    "Name": "Stephane Eranian"
},
{
    "Name": "Stephanie Dietzel"
},
{
    "Name": "Stephanie Ludi"
},
{
    "Name": "Stephen W. Thomas"
},
{
    "Name": "Steve Livengood"
},
{
    "Name": "Steve Riddle"
},
{
    "Name": "Steven Ip"
},
{
    "Name": "Steven She"
},
{
    "Name": "Stuart Faulk"
},
{
    "Name": "Sue Black"
},
{
    "Name": "Suhaimi Ibrahim"
},
{
    "Name": "Sumit Bhatia"
},
{
    "Name": "Sumit Purohit"
},
{
    "Name": "Sung-eok Jeon"
},
{
    "Name": "Sunghun Kim"
},
{
    "Name": "Sunil Bohra"
},
{
    "Name": "Sunil Prabhakar"
},
{
    "Name": "Sunita Chulani"
},
{
    "Name": "Sunny Wong"
},
{
    "Name": "Suppawong Tuarob"
},
{
    "Name": "Susan Elliott Sim"
},
{
    "Name": "Sushil Bajracharya"
},
{
    "Name": "Sven Apel"
},
{
    "Name": "Sven H. Koch"
},
{
    "Name": "Sven Stork"
},
{
    "Name": "Swapneel Sheth"
},
{
    "Name": "Sweefen Goh"
},
{
    "Name": "Syed Shariyar Murtaza"
},
{
    "Name": "T. S. Mohan"
},
{
    "Name": "Taco de Boer"
},
{
    "Name": "Tadashi Dohi"
},
{
    "Name": "Takaji Fujiwara"
},
{
    "Name": "Takako Nakatani"
},
{
    "Name": "Takashi Hattori"
},
{
    "Name": "Takashi Ishio"
},
{
    "Name": "Takayuki Omori"
},
{
    "Name": "Tao Bao"
},
{
    "Name": "Tao Xie"
},
{
    "Name": "Tateki Sano"
},
{
    "Name": "Ted Theodoropoulos"
},
{
    "Name": "Tevfik Bultan"
},
{
    "Name": "Thanh H. D. Nguyen"
},
{
    "Name": "Thierry Lavoie"
},
{
    "Name": "Thomas Bracewell"
},
{
    "Name": "Thomas Huang"
},
{
    "Name": "Thomas Lévêque"
},
{
    "Name": "Thomas Wetter"
},
{
    "Name": "Thomas Zimmermann"
},
{
    "Name": "Thorsten Berger"
},
{
    "Name": "Tianwei Sheng"
},
{
    "Name": "Tien N. Nguyen"
},
{
    "Name": "Tiffany Barnes"
},
{
    "Name": "Tihomir Gvero"
},
{
    "Name": "Tijs van der Storm"
},
{
    "Name": "Tim Cianchi"
},
{
    "Name": "Tim Frey"
},
{
    "Name": "Tim Klinger"
},
{
    "Name": "Tobias Kuipers"
},
{
    "Name": "Todd King"
},
{
    "Name": "Todd W. Schiller"
},
{
    "Name": "Tom Holvoet"
},
{
    "Name": "Tomoko Kanemitsu"
},
{
    "Name": "Tong Wu"
},
{
    "Name": "Toshihiko Tsumaki"
},
{
    "Name": "Toshihiro Kamiya"
},
{
    "Name": "Toshiya Fujii"
},
{
    "Name": "Tracy Hall"
},
{
    "Name": "Trevor Savage"
},
{
    "Name": "TS Mohan"
},
{
    "Name": "Tu M. Phuong"
},
{
    "Name": "Tucker Smith"
},
{
    "Name": "Tung T. Nguyen"
},
{
    "Name": "Usman Ali"
},
{
    "Name": "Usman Dastgeer"
},
{
    "Name": "Valentin Chimisliu"
},
{
    "Name": "Valentina Ferrari"
},
{
    "Name": "Valerie Issarny"
},
{
    "Name": "Van Tran"
},
{
    "Name": "Vanessa Peña Araya"
},
{
    "Name": "Venera Arnaoudova"
},
{
    "Name": "Veselin Ganev"
},
{
    "Name": "Vibha Singhal Sinha"
},
{
    "Name": "Vibhu Saujanya Sharma"
},
{
    "Name": "Vicente F. Lucena"
},
{
    "Name": "Víctor Braberman"
},
{
    "Name": "Victor Pankratius"
},
{
    "Name": "Victor R. Basili"
},
{
    "Name": "Victoria Shipp"
},
{
    "Name": "Vidya Kulkarni"
},
{
    "Name": "Vikram Gupta"
},
{
    "Name": "Vikrant Kaulgud"
},
{
    "Name": "Vilas Jagannath"
},
{
    "Name": "Vinay Kulkarni"
},
{
    "Name": "Vishal Dwivedi"
},
{
    "Name": "Vittorio Cortellessa"
},
{
    "Name": "Waldemar Hummer"
},
{
    "Name": "Walid Maalej"
},
{
    "Name": "Walt Scacchi"
},
{
    "Name": "Walter Cazzola"
},
{
    "Name": "Wanda Gregory"
},
{
    "Name": "Waraporn Jirapanthong"
},
{
    "Name": "Watanabe Takuya"
},
{
    "Name": "Wei Fu"
},
{
    "Name": "Wei Jin"
},
{
    "Name": "Wei Le"
},
{
    "Name": "Wei-Keat Kong"
},
{
    "Name": "Weimin Zheng"
},
{
    "Name": "Wenbin Li"
},
{
    "Name": "Wenguang Chen"
},
{
    "Name": "Wenke Lee"
},
{
    "Name": "Wenyun Zhao"
},
{
    "Name": "Werner Dietl"
},
{
    "Name": "Werner Janjic"
},
{
    "Name": "Wilhelm Schaefer"
},
{
    "Name": "Willem Visser"
},
{
    "Name": "Willi Hornig"
},
{
    "Name": "William Sumner"
},
{
    "Name": "Wim Van Betsbrugge"
},
{
    "Name": "Winfried Dulz"
},
{
    "Name": "Wladimir Araujo"
},
{
    "Name": "Wouter Joosen"
},
{
    "Name": "Xavier Franch"
},
{
    "Name": "Xi Ge"
},
{
    "Name": "Xi Tan"
},
{
    "Name": "Xiangyu Zhang"
},
{
    "Name": "Xiao Xiao"
},
{
    "Name": "Xiaofan Chen"
},
{
    "Name": "Xiaoqi Jia"
},
{
    "Name": "Xiaoyan Zhu"
},
{
    "Name": "Xiaoying Bai"
},
{
    "Name": "Xin Peng"
},
{
    "Name": "Xin-Hua Hu"
},
{
    "Name": "Xinlei (Oscar) Wang"
},
{
    "Name": "Xinran Wang"
},
{
    "Name": "Xusheng Xiao"
},
{
    "Name": "Yan Wu"
},
{
    "Name": "Yang Li"
},
{
    "Name": "Yann-Gaël Guéhéneuc"
},
{
    "Name": "Yasutaka Kamei"
},
{
    "Name": "Yi Wei"
},
{
    "Name": "Yijun Yu"
},
{
    "Name": "Yingnong Dang"
},
{
    "Name": "Yingzhou Zhang"
},
{
    "Name": "Yinxing Xue"
},
{
    "Name": "Yishai A. Feldman"
},
{
    "Name": "Yoann Padioleau"
},
{
    "Name": "Yonghee Shin"
},
{
    "Name": "Yongjie Zheng"
},
{
    "Name": "Yoon-Chan Jhi"
},
{
    "Name": "Yoram Adler"
},
{
    "Name": "Yoshiaki Fukazawa"
},
{
    "Name": "Yoshiki Higo"
},
{
    "Name": "Yoshionori Tanabe"
},
{
    "Name": "Yossi Lev"
},
{
    "Name": "Yu David Liu"
},
{
    "Name": "Yu Sun"
},
{
    "Name": "Yuanfang Cai"
},
{
    "Name": "Yuan-Fang Li"
},
{
    "Name": "Yuanyuan Zhou"
},
{
    "Name": "Yubin Li"
},
{
    "Name": "Yuepu Guo"
},
{
    "Name": "Yukinao Hirata"
},
{
    "Name": "Yun Yang"
},
{
    "Name": "Yungbum Jung"
},
{
    "Name": "Yunwen Ye"
},
{
    "Name": "Yunzhan Gong"
},
{
    "Name": "Yvan Labiche"
},
{
    "Name": "Yvonne Coady"
},
{
    "Name": "Yvonne Dittrich"
},
{
    "Name": "Zaid Ibrahim"
},
{
    "Name": "Zaynab Mousavian"
},
{
    "Name": "Zhenchang Xing"
},
{
    "Name": "Zhendong Su"
},
{
    "Name": "Zheng Li"
},
{
    "Name": "Zhihong Xu"
},
{
    "Name": "Zhiyuan Zhan"
},
{
    "Name": "Zhongpeng Lin"
},
{
    "Name": "Zhongxian Gu"
},
{
    "Name": "Zongfang Lin"
},
{
    "Name": "Zude Li"
},
{
    "Name": "Zuohua Ding"
},
{
    "Name": "Bill Dresselhaus",
    "Bio": "Bill Dresselhaus is currently a full-time Joint Invited Professor of Design at Hongik University in Seoul, Korea, that country’s top art and design school. Bill teaches product design, design management and design innovation in three schools at Hongik: the Law School, the Mechanical Systems Design and Engineering Department, and the International Design School for Advanced Studies. He is also Founder and President of Dresselhaus Group, Inc., a design, innovation and education consultancy. Bill has over 40 years of broad and eclectic experience in many areas of business, design and technology, especially in product design and development. Bill’s career and background include biochemical research, petroleum refinery design, consumer and high-technology product design and development, college science and design teaching, mechanical design, industrial design and project management. Bill also consults and trains internationally to a variety of client organizations. His client list includes Apple Computer, Hewlett-Packard, EDS, Pantech Group, KIDP, LG Chemical, and InFocus, among many others. Bill was one of the first design innovators at Apple Computer and the Principal Product Designer of the Apple Lisa, the Mother of the Macintosh. He authored, designed and published his popular design book, ROI: Return On Innovation, which he is currently updating into an ebook. He was educated at Stanford University, Iowa State University and Art Center College of Design, obtaining two masters degrees in engineering and product design, and advanced executive graduate training in industrial design. His current passions are design thinking and process, ubiquitous design education for everyone, and the DIY design movement. Bill is a full professional member of IDSA, the Industrial Designers Society of America."
},
{
    "Name": "Kumiyo Nakakoji",
    "Bio": "Kumiyo Nakakoji, Research Director at Key Technology Laboratory, Software Research Associates Inc., Japan, received B.S. in computer science from Osaka University, Japan, in 1986, and M.S. in 1990 and Ph.D. in 1993, both in computer science from University of Colorado, Boulder, certified in Institute of Cognitive Science. She has been spending her research career both in industry and academia. While she has been working for Software Research Associates since 1986, she also held positions as Full Professor at Research Center for Advanced Science and Technology (RCAST), University of Tokyo, Japan, where she co-directed the Knowledge Interaction Design (KID) Laboratory (2002-2010), as Adjunct Associate Professor at Nara Institute of Science and Technology, Japan (1995-2002), and as Adjoint Assistant Professor at Institute of Cognitive Science, University of Colorado, Boulder, USA (1994-2002). She has served as chairs, editors, and members for numerous research committees, journals, conferences, and government funding agencies, in the fields of Human-Computer Interaction, Software Engineering, and Design and Creative Knowledge Work Support. She was awarded Distinguished Engineering Alumni Award from College of Engineering, University of Colorado, Boulder, in 2006."
}
	]
}